cls_labels_value_counts:
 0    1047
1    1035
Name: count, dtype: int64
bio_labels_value_counts:
  6      52764
 3       8717
 1       6948
-100     4164
 2       1320
 0       1179
 5        483
 4         79
Name: count, dtype: int64
rel_labels_value_counts:
 0    2887
1    1494
Name: count, dtype: int64
cls_weights: tensor([0.0015, 0.0016])
bio_weights: tensor([0.0014, 0.0010, 0.0014, 0.0010, 0.0132, 0.0026, 0.0010])
rel_weights: tensor([0.0011, 0.0013])
Epoch 1/2 [Training]:   0%|          | 0/2 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Epoch 1/2 [Training]:   0%|          | 0/2 [00:00<?, ?it/s, bio=2.4797, cls=1.0393, rel=1.1992, total_loss=12.1573]Epoch 1/2 [Training]:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.36it/s, bio=2.4797, cls=1.0393, rel=1.1992, total_loss=12.1573]Epoch 1/2 [Training]:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.36it/s, bio=1.7061, cls=1.0788, rel=0.8528, total_loss=8.7559]                                                                                                                           Epoch 1/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 1/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s, total_loss=2.7024]Epoch 1/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s, total_loss=2.9859]                                                                                Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]                                                 
================================================================================
Epoch 1/2 Summary
--------------------------------------------------------------------------------
  Average Training Loss:           10.4566
  Average Validation Loss:         2.8441
  Overall Validation Avg F1 (Macro): 0.3168
--------------------------------------------------------------------------------
Task-Specific Validation Performance:

  [Task 1: Sentence Classification]
    Macro F1-Score:  0.4048
    Macro Precision: 0.5000
    Macro Recall:    0.5000
    Accuracy:        0.5000
    Per-class details:
      non-causal  : F1=0.6429 (P=0.5000, R=0.9000, Support=10.0)
      causal      : F1=0.1667 (P=0.5000, R=0.1000, Support=10.0)

  [Task 2: BIO Prediction (Token-BIO)]
    Macro F1-Score:  0.1227
    Macro Precision: 0.1243
    Macro Recall:    0.1344
    Per-tag details (P=Precision, R=Recall, F1=F1-Score, S=Support):
      B-C       : F1=0.000 (P=0.000, R=0.000, S=12.0)
      I-C       : F1=0.000 (P=0.000, R=0.000, S=67.0)
      B-E       : F1=0.000 (P=0.000, R=0.000, S=11.0)
      I-E       : F1=0.087 (P=0.185, R=0.057, S=88.0)
      B-CE      : F1=0.000 (P=0.000, R=0.000, S=0.0)
      I-CE      : F1=0.000 (P=0.000, R=0.000, S=0.0)
      O         : F1=0.772 (P=0.685, R=0.884, S=413.0)

  [Task 3: Relation Prediction]
    Macro F1-Score:  0.4231
    Macro Precision: 0.4231
    Macro Recall:    0.4231
    Per-relation type details (P=Precision, R=Recall, F1=F1-Score, S=Support):
      Rel_None    : F1=0.615 (P=0.615, R=0.615, S=26.0)
      Rel_CE      : F1=0.231 (P=0.231, R=0.231, S=13.0)
--------------------------------------------------------------------------------
Status: ðŸŽ‰ New best model saved! Overall Avg F1: 0.3168
================================================================================

Epoch 2/2 [Training]:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 2/2 [Training]:   0%|          | 0/2 [00:00<?, ?it/s, bio=1.4700, cls=0.6911, rel=0.8394, total_loss=7.4103]Epoch 2/2 [Training]:   0%|          | 0/2 [00:00<?, ?it/s, bio=1.4150, cls=0.3702, rel=0.9124, total_loss=6.9427]Epoch 2/2 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 14.71it/s, bio=1.4150, cls=0.3702, rel=0.9124, total_loss=6.9427]                                                                                                                          Epoch 2/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s]Epoch 2/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s, total_loss=2.5058]Epoch 2/2 [Validation]:   0%|          | 0/2 [00:00<?, ?it/s, total_loss=3.0400]                                                                                Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]                                                 
================================================================================
Epoch 2/2 Summary
--------------------------------------------------------------------------------
  Average Training Loss:           7.1765
  Average Validation Loss:         2.7729
  Overall Validation Avg F1 (Macro): 0.3594
--------------------------------------------------------------------------------
Task-Specific Validation Performance:

  [Task 1: Sentence Classification]
    Macro F1-Score:  0.4505
    Macro Precision: 0.5000
    Macro Recall:    0.5000
    Accuracy:        0.5000
    Per-class details:
      non-causal  : F1=0.6154 (P=0.5000, R=0.8000, Support=10.0)
      causal      : F1=0.2857 (P=0.5000, R=0.2000, Support=10.0)

  [Task 2: BIO Prediction (Token-BIO)]
    Macro F1-Score:  0.1643
    Macro Precision: 0.1397
    Macro Recall:    0.1995
    Per-tag details (P=Precision, R=Recall, F1=F1-Score, S=Support):
      B-C       : F1=0.000 (P=0.000, R=0.000, S=12.0)
      I-C       : F1=0.000 (P=0.000, R=0.000, S=67.0)
      B-E       : F1=0.000 (P=0.000, R=0.000, S=11.0)
      I-E       : F1=0.000 (P=0.000, R=0.000, S=88.0)
      O         : F1=0.822 (P=0.698, R=0.998, S=413.0)

  [Task 3: Relation Prediction]
    Macro F1-Score:  0.4635
    Macro Precision: 0.5878
    Macro Recall:    0.5192
    Per-relation type details (P=Precision, R=Recall, F1=F1-Score, S=Support):
      Rel_None    : F1=0.794 (P=0.676, R=0.962, S=26.0)
      Rel_CE      : F1=0.133 (P=0.500, R=0.077, S=13.0)
--------------------------------------------------------------------------------
Status: ðŸŽ‰ New best model saved! Overall Avg F1: 0.3594
================================================================================

Loading best model state (in memory) with F1: 0.3594
Traceback (most recent call last):
  File "/home/rnorouzini/JointLearning/src/jointlearning/main.py", line 117, in <module>
    model.save_pretrained(model_save_path)
  File "/home/rnorouzini/JointLearning/myenv/lib/python3.10/site-packages/huggingface_hub/hub_mixin.py", line 412, in save_pretrained
    save_directory.mkdir(parents=True, exist_ok=True)
  File "/usr/lib/python3.10/pathlib.py", line 1175, in mkdir
    self._accessor.mkdir(self, mode)
FileExistsError: [Errno 17] File exists: 'src/jointlearning/roberta_softmax_model/best_softmax_model.pt'
