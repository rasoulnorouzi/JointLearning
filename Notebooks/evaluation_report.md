# JointCausalModel Evaluation Report

## Configuration

| Setting | Value |
|---------|-------|
| Tokenizer | `bert-base-uncased` |
| Batch size | 32 |
| Thresholds | 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95 |
| Classification modes | cls+span, span_only |
| Scenarios | all_documents, filtered_causal |
| Eval modes | coverage, discovery |


## bert-softmax

### Classification: `cls+span`

#### Threshold = 0.60

| Task1                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 166, 'FP': 28, 'FN': 55, 'TN': 203, 'Precision': 0.8556701030927835, 'Recall': 0.751131221719457, 'F1': 0.8, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7058823529411765, 'Recall': 0.631578947368421, 'F1': 0.6666666666666667, 'TP': 168, 'FP': 70, 'FN': 98}, 'effect': {'Precision': 0.7935943060498221, 'Recall': 0.7335526315789473, 'F1': 0.7623931623931623, 'TP': 223, 'FP': 58, 'FN': 81}}   | {'Precision': 0.7497383294954993, 'Recall': 0.6825657894736842, 'F1': 0.7145299145299145, 'TP': 391, 'FP': 128, 'FN': 179} | {'TP': 192, 'FP': 142, 'FN': 120, 'Accuracy': 0.42290748898678415, 'Precision': 0.5748502994011976, 'Recall': 0.6153846153846154, 'F1': 0.5944272445820433} | {'Precision': 0.7267529106631602, 'Recall': 0.6830272088592522, 'F1': 0.7029857197039858} | all_documents   | coverage    |
| {'TP': 166, 'FP': 28, 'FN': 55, 'TN': 203, 'Precision': 0.8556701030927835, 'Recall': 0.751131221719457, 'F1': 0.8, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.6929824561403509, 'Recall': 0.6053639846743295, 'F1': 0.6462167689161554, 'TP': 158, 'FP': 70, 'FN': 103}, 'effect': {'Precision': 0.7769230769230769, 'Recall': 0.7013888888888888, 'F1': 0.7372262773722628, 'TP': 202, 'FP': 58, 'FN': 86}} | {'Precision': 0.7349527665317139, 'Recall': 0.6533764367816092, 'F1': 0.6917215231442091, 'TP': 360, 'FP': 128, 'FN': 189} | {'TP': 163, 'FP': 142, 'FN': 127, 'Accuracy': 0.3773148148148148, 'Precision': 0.5344262295081967, 'Recall': 0.5620689655172414, 'F1': 0.5478991596638656}  | {'Precision': 0.7083496997108981, 'Recall': 0.6555255413394359, 'F1': 0.6798735609360249} | all_documents   | discovery   |
| {'TP': 166, 'FP': 28, 'FN': 55, 'TN': 203, 'Precision': 0.8556701030927835, 'Recall': 0.751131221719457, 'F1': 0.8, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8195121951219512, 'Recall': 0.8076923076923077, 'F1': 0.8135593220338982, 'TP': 168, 'FP': 37, 'FN': 40}, 'effect': {'Precision': 0.9065040650406504, 'Recall': 0.892, 'F1': 0.8991935483870968, 'TP': 223, 'FP': 23, 'FN': 27}}               | {'Precision': 0.8630081300813008, 'Recall': 0.8498461538461539, 'F1': 0.8563764352104974, 'TP': 391, 'FP': 60, 'FN': 67}   | {'TP': 192, 'FP': 101, 'FN': 65, 'Accuracy': 0.5363128491620112, 'Precision': 0.6552901023890785, 'Recall': 0.7470817120622568, 'F1': 0.6981818181818181}   | {'Precision': 0.7913227785210543, 'Recall': 0.7826863625426226, 'F1': 0.7848527511307718} | filtered_causal | coverage    |
| {'TP': 166, 'FP': 28, 'FN': 55, 'TN': 203, 'Precision': 0.8556701030927835, 'Recall': 0.751131221719457, 'F1': 0.8, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8102564102564103, 'Recall': 0.7783251231527094, 'F1': 0.7939698492462313, 'TP': 158, 'FP': 37, 'FN': 45}, 'effect': {'Precision': 0.8977777777777778, 'Recall': 0.8632478632478633, 'F1': 0.8801742919389979, 'TP': 202, 'FP': 23, 'FN': 32}}  | {'Precision': 0.854017094017094, 'Recall': 0.8207864932002864, 'F1': 0.8370720705926146, 'TP': 360, 'FP': 60, 'FN': 77}    | {'TP': 163, 'FP': 101, 'FN': 72, 'Accuracy': 0.4851190476190476, 'Precision': 0.6174242424242424, 'Recall': 0.6936170212765957, 'F1': 0.6533066132264529}   | {'Precision': 0.7757038131780399, 'Recall': 0.7551782453987798, 'F1': 0.7634595612730225} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7094017094017094, 'Recall': 0.6240601503759399, 'F1': 0.664, 'TP': 166, 'FP': 68, 'FN': 100}, 'effect': {'Precision': 0.7971014492753623, 'Recall': 0.7260726072607261, 'F1': 0.75993091537133, 'TP': 220, 'FP': 56, 'FN': 83}}               | {'Precision': 0.7532515793385359, 'Recall': 0.675066378818333, 'F1': 0.711965457685665, 'TP': 386, 'FP': 124, 'FN': 183}   | {'TP': 187, 'FP': 132, 'FN': 123, 'Accuracy': 0.4230769230769231, 'Precision': 0.5862068965517241, 'Recall': 0.603225806451613, 'F1': 0.5945945945945946} | {'Precision': 0.7314602518943354, 'Recall': 0.6749661733705249, 'F1': 0.7012205005185407} | all_documents   | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.6964285714285714, 'Recall': 0.5977011494252874, 'F1': 0.643298969072165, 'TP': 156, 'FP': 68, 'FN': 105}, 'effect': {'Precision': 0.78125, 'Recall': 0.6944444444444444, 'F1': 0.7352941176470588, 'TP': 200, 'FP': 56, 'FN': 88}}            | {'Precision': 0.7388392857142857, 'Recall': 0.6460727969348659, 'F1': 0.6892965433596119, 'TP': 356, 'FP': 124, 'FN': 193} | {'TP': 160, 'FP': 132, 'FN': 130, 'Accuracy': 0.3791469194312796, 'Precision': 0.547945205479452, 'Recall': 0.5517241379310345, 'F1': 0.5498281786941581} | {'Precision': 0.7139022569954946, 'Recall': 0.6481344232358431, 'F1': 0.6787420571097108} | all_documents   | discovery   |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8258706467661692, 'Recall': 0.8019323671497585, 'F1': 0.8137254901960784, 'TP': 166, 'FP': 35, 'FN': 41}, 'effect': {'Precision': 0.9128630705394191, 'Recall': 0.8870967741935484, 'F1': 0.8997955010224948, 'TP': 220, 'FP': 21, 'FN': 28}} | {'Precision': 0.8693668586527941, 'Recall': 0.8445145706716535, 'F1': 0.8567604956092867, 'TP': 386, 'FP': 56, 'FN': 69}   | {'TP': 187, 'FP': 91, 'FN': 67, 'Accuracy': 0.5420289855072464, 'Precision': 0.6726618705035972, 'Recall': 0.7362204724409449, 'F1': 0.7030075187969925}  | {'Precision': 0.7989836696497123, 'Recall': 0.7757804593180758, 'F1': 0.7856231545605472} | filtered_causal | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8167539267015707, 'Recall': 0.7722772277227723, 'F1': 0.7938931297709924, 'TP': 156, 'FP': 35, 'FN': 46}, 'effect': {'Precision': 0.9049773755656109, 'Recall': 0.8583690987124464, 'F1': 0.881057268722467, 'TP': 200, 'FP': 21, 'FN': 33}}  | {'Precision': 0.8608656511335908, 'Recall': 0.8153231632176093, 'F1': 0.8374751992467298, 'TP': 356, 'FP': 56, 'FN': 79}   | {'TP': 160, 'FP': 91, 'FN': 74, 'Accuracy': 0.49230769230769234, 'Precision': 0.6374501992031872, 'Recall': 0.6837606837606838, 'F1': 0.6597938144329896} | {'Precision': 0.7844127100431747, 'Recall': 0.7485633939399742, 'F1': 0.7647901543183605} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.717391304347826, 'Recall': 0.6226415094339622, 'F1': 0.6666666666666666, 'TP': 165, 'FP': 65, 'FN': 100}, 'effect': {'Precision': 0.7992700729927007, 'Recall': 0.7227722772277227, 'F1': 0.7590987868284229, 'TP': 219, 'FP': 55, 'FN': 84}}  | {'Precision': 0.7583306886702634, 'Recall': 0.6727068933308424, 'F1': 0.7128827267475448, 'TP': 384, 'FP': 120, 'FN': 184} | {'TP': 185, 'FP': 126, 'FN': 124, 'Accuracy': 0.42528735632183906, 'Precision': 0.594855305466238, 'Recall': 0.598705501618123, 'F1': 0.5967741935483872}   | {'Precision': 0.7360360913097491, 'Recall': 0.6726729099301981, 'F1': 0.7022527898570982} | all_documents   | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7058823529411765, 'Recall': 0.5977011494252874, 'F1': 0.6473029045643154, 'TP': 156, 'FP': 65, 'FN': 105}, 'effect': {'Precision': 0.7834645669291339, 'Recall': 0.6909722222222222, 'F1': 0.7343173431734318, 'TP': 199, 'FP': 55, 'FN': 89}} | {'Precision': 0.7446734599351552, 'Recall': 0.6443366858237548, 'F1': 0.6908101238688735, 'TP': 355, 'FP': 120, 'FN': 194} | {'TP': 159, 'FP': 126, 'FN': 131, 'Accuracy': 0.38221153846153844, 'Precision': 0.5578947368421052, 'Recall': 0.5482758620689655, 'F1': 0.5530434782608696} | {'Precision': 0.7191634921900022, 'Recall': 0.646406294244783, 'F1': 0.6803183504683684}  | all_documents   | discovery   |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8333333333333334, 'Recall': 0.8009708737864077, 'F1': 0.8168316831683169, 'TP': 165, 'FP': 33, 'FN': 41}, 'effect': {'Precision': 0.9163179916317992, 'Recall': 0.8830645161290323, 'F1': 0.8993839835728953, 'TP': 219, 'FP': 20, 'FN': 29}}  | {'Precision': 0.8748256624825663, 'Recall': 0.84201769495772, 'F1': 0.858107833370606, 'TP': 384, 'FP': 53, 'FN': 70}      | {'TP': 185, 'FP': 86, 'FN': 68, 'Accuracy': 0.5457227138643068, 'Precision': 0.6826568265682657, 'Recall': 0.7312252964426877, 'F1': 0.7061068702290075}    | {'Precision': 0.8041349229478594, 'Recall': 0.7732831087473455, 'F1': 0.7871053842916585} | filtered_causal | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8253968253968254, 'Recall': 0.7722772277227723, 'F1': 0.7979539641943734, 'TP': 156, 'FP': 33, 'FN': 46}, 'effect': {'Precision': 0.908675799086758, 'Recall': 0.8540772532188842, 'F1': 0.8805309734513276, 'TP': 199, 'FP': 20, 'FN': 34}}   | {'Precision': 0.8670363122417917, 'Recall': 0.8131772404708282, 'F1': 0.8392424688228505, 'TP': 355, 'FP': 53, 'FN': 80}   | {'TP': 159, 'FP': 86, 'FN': 75, 'Accuracy': 0.496875, 'Precision': 0.6489795918367347, 'Recall': 0.6794871794871795, 'F1': 0.6638830897703549}              | {'Precision': 0.7903127279570908, 'Recall': 0.7464235849332121, 'F1': 0.7667423359561892} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7161572052401747, 'Recall': 0.6188679245283019, 'F1': 0.6639676113360323, 'TP': 164, 'FP': 65, 'FN': 101}, 'effect': {'Precision': 0.8022388059701493, 'Recall': 0.7119205298013245, 'F1': 0.7543859649122807, 'TP': 215, 'FP': 53, 'FN': 87}} | {'Precision': 0.7591980056051619, 'Recall': 0.6653942271648132, 'F1': 0.7091767881241565, 'TP': 379, 'FP': 118, 'FN': 188} | {'TP': 178, 'FP': 122, 'FN': 128, 'Accuracy': 0.4158878504672897, 'Precision': 0.5933333333333334, 'Recall': 0.5816993464052288, 'F1': 0.5874587458745875} | {'Precision': 0.7358178729104138, 'Recall': 0.6645666361372237, 'F1': 0.6979123277580355} | all_documents   | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7045454545454546, 'Recall': 0.5938697318007663, 'F1': 0.6444906444906445, 'TP': 155, 'FP': 65, 'FN': 106}, 'effect': {'Precision': 0.7871485943775101, 'Recall': 0.6805555555555556, 'F1': 0.7299813780260708, 'TP': 196, 'FP': 53, 'FN': 92}} | {'Precision': 0.7458470244614823, 'Recall': 0.6372126436781609, 'F1': 0.6872360112583576, 'TP': 351, 'FP': 118, 'FN': 198} | {'TP': 155, 'FP': 122, 'FN': 135, 'Accuracy': 0.3762135922330097, 'Precision': 0.5595667870036101, 'Recall': 0.5344827586206896, 'F1': 0.5467372134038802} | {'Precision': 0.7201120304192795, 'Recall': 0.6394339123801598, 'F1': 0.6770248913125334} | all_documents   | discovery   |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8324873096446701, 'Recall': 0.7961165048543689, 'F1': 0.8138957816377173, 'TP': 164, 'FP': 33, 'FN': 42}, 'effect': {'Precision': 0.9227467811158798, 'Recall': 0.8704453441295547, 'F1': 0.8958333333333334, 'TP': 215, 'FP': 18, 'FN': 32}}  | {'Precision': 0.877617045380275, 'Recall': 0.8332809244919618, 'F1': 0.8548645574855254, 'TP': 379, 'FP': 51, 'FN': 74}    | {'TP': 178, 'FP': 83, 'FN': 72, 'Accuracy': 0.5345345345345346, 'Precision': 0.6819923371647509, 'Recall': 0.712, 'F1': 0.6966731898238747}                | {'Precision': 0.804843887445924, 'Recall': 0.7639624197778637, 'F1': 0.7828797321949207}  | filtered_causal | coverage    |
| {'TP': 165, 'FP': 28, 'FN': 56, 'TN': 203, 'Precision': 0.8549222797927462, 'Recall': 0.746606334841629, 'F1': 0.7971014492753623, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.824468085106383, 'Recall': 0.7673267326732673, 'F1': 0.7948717948717949, 'TP': 155, 'FP': 33, 'FN': 47}, 'effect': {'Precision': 0.9158878504672897, 'Recall': 0.8412017167381974, 'F1': 0.8769574944071589, 'TP': 196, 'FP': 18, 'FN': 37}}   | {'Precision': 0.8701779677868364, 'Recall': 0.8042642247057323, 'F1': 0.835914644639477, 'TP': 351, 'FP': 51, 'FN': 84}    | {'TP': 155, 'FP': 83, 'FN': 79, 'Accuracy': 0.4889589905362776, 'Precision': 0.6512605042016807, 'Recall': 0.6623931623931624, 'F1': 0.6567796610169492}   | {'Precision': 0.7921202505937543, 'Recall': 0.7377545739801746, 'F1': 0.7632652516439294} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 163, 'FP': 28, 'FN': 58, 'TN': 203, 'Precision': 0.8534031413612565, 'Recall': 0.7375565610859729, 'F1': 0.7912621359223301, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7168949771689498, 'Recall': 0.5924528301886792, 'F1': 0.6487603305785125, 'TP': 157, 'FP': 62, 'FN': 108}, 'effect': {'Precision': 0.8038461538461539, 'Recall': 0.6920529801324503, 'F1': 0.7437722419928825, 'TP': 209, 'FP': 51, 'FN': 93}} | {'Precision': 0.7603705655075519, 'Recall': 0.6422529051605648, 'F1': 0.6962662862856974, 'TP': 366, 'FP': 113, 'FN': 201} | {'TP': 171, 'FP': 117, 'FN': 134, 'Accuracy': 0.4052132701421801, 'Precision': 0.59375, 'Recall': 0.5606557377049181, 'F1': 0.5767284991568297}             | {'Precision': 0.7358412356229361, 'Recall': 0.6468217346504853, 'F1': 0.6880856404549524} | all_documents   | coverage    |
| {'TP': 163, 'FP': 28, 'FN': 58, 'TN': 203, 'Precision': 0.8534031413612565, 'Recall': 0.7375565610859729, 'F1': 0.7912621359223301, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7047619047619048, 'Recall': 0.5670498084291188, 'F1': 0.6284501061571125, 'TP': 148, 'FP': 62, 'FN': 113}, 'effect': {'Precision': 0.7892561983471075, 'Recall': 0.6631944444444444, 'F1': 0.720754716981132, 'TP': 191, 'FP': 51, 'FN': 97}}  | {'Precision': 0.7470090515545061, 'Recall': 0.6151221264367817, 'F1': 0.6746024115691223, 'TP': 339, 'FP': 113, 'FN': 210} | {'TP': 149, 'FP': 117, 'FN': 141, 'Accuracy': 0.36609336609336607, 'Precision': 0.5601503759398496, 'Recall': 0.5137931034482759, 'F1': 0.5359712230215827} | {'Precision': 0.7201875229518707, 'Recall': 0.6221572636570102, 'F1': 0.6672785901710118} | all_documents   | discovery   |
| {'TP': 163, 'FP': 28, 'FN': 58, 'TN': 203, 'Precision': 0.8534031413612565, 'Recall': 0.7375565610859729, 'F1': 0.7912621359223301, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.839572192513369, 'Recall': 0.7733990147783252, 'F1': 0.8051282051282052, 'TP': 157, 'FP': 30, 'FN': 46}, 'effect': {'Precision': 0.9288888888888889, 'Recall': 0.8565573770491803, 'F1': 0.8912579957356077, 'TP': 209, 'FP': 16, 'FN': 35}}   | {'Precision': 0.8842305407011289, 'Recall': 0.8149781959137528, 'F1': 0.8481931004319064, 'TP': 366, 'FP': 46, 'FN': 81}   | {'TP': 171, 'FP': 78, 'FN': 75, 'Accuracy': 0.5277777777777778, 'Precision': 0.6867469879518072, 'Recall': 0.6951219512195121, 'F1': 0.6909090909090909}    | {'Precision': 0.8081268900047309, 'Recall': 0.749218902739746, 'F1': 0.7767881090877758}  | filtered_causal | coverage    |
| {'TP': 163, 'FP': 28, 'FN': 58, 'TN': 203, 'Precision': 0.8534031413612565, 'Recall': 0.7375565610859729, 'F1': 0.7912621359223301, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8314606741573034, 'Recall': 0.7437185929648241, 'F1': 0.7851458885941645, 'TP': 148, 'FP': 30, 'FN': 51}, 'effect': {'Precision': 0.9227053140096618, 'Recall': 0.8304347826086956, 'F1': 0.8741418764302059, 'TP': 191, 'FP': 16, 'FN': 39}}  | {'Precision': 0.8770829940834826, 'Recall': 0.7870766877867599, 'F1': 0.8296438825121852, 'TP': 339, 'FP': 46, 'FN': 90}   | {'TP': 149, 'FP': 78, 'FN': 82, 'Accuracy': 0.48220064724919093, 'Precision': 0.6563876651982379, 'Recall': 0.645021645021645, 'F1': 0.6506550218340611}    | {'Precision': 0.7956246002143256, 'Recall': 0.7232182979647925, 'F1': 0.7571870134228589} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                               | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 162, 'FP': 28, 'FN': 59, 'TN': 203, 'Precision': 0.8526315789473684, 'Recall': 0.7330316742081447, 'F1': 0.7883211678832117, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7216981132075472, 'Recall': 0.583969465648855, 'F1': 0.6455696202531647, 'TP': 153, 'FP': 59, 'FN': 109}, 'effect': {'Precision': 0.8072289156626506, 'Recall': 0.674496644295302, 'F1': 0.7349177330895794, 'TP': 201, 'FP': 48, 'FN': 97}}    | {'Precision': 0.7644635144350989, 'Recall': 0.6292330549720785, 'F1': 0.690243676671372, 'TP': 354, 'FP': 107, 'FN': 206} | {'TP': 164, 'FP': 103, 'FN': 137, 'Accuracy': 0.40594059405940597, 'Precision': 0.6142322097378277, 'Recall': 0.5448504983388704, 'F1': 0.5774647887323944} | {'Precision': 0.743775767706765, 'Recall': 0.6357050758396979, 'F1': 0.6853432110956593}  | all_documents   | coverage    |
| {'TP': 162, 'FP': 28, 'FN': 59, 'TN': 203, 'Precision': 0.8526315789473684, 'Recall': 0.7330316742081447, 'F1': 0.7883211678832117, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7135922330097088, 'Recall': 0.5632183908045977, 'F1': 0.6295503211991434, 'TP': 147, 'FP': 59, 'FN': 114}, 'effect': {'Precision': 0.7957446808510639, 'Recall': 0.6493055555555556, 'F1': 0.7151051625239006, 'TP': 187, 'FP': 48, 'FN': 101}} | {'Precision': 0.7546684569303863, 'Recall': 0.6062619731800767, 'F1': 0.672327741861522, 'TP': 334, 'FP': 107, 'FN': 215} | {'TP': 146, 'FP': 103, 'FN': 144, 'Accuracy': 0.37150127226463103, 'Precision': 0.5863453815261044, 'Recall': 0.503448275862069, 'F1': 0.5417439703153989}  | {'Precision': 0.7312151391346197, 'Recall': 0.6142473077500967, 'F1': 0.6674642933533775} | all_documents   | discovery   |
| {'TP': 162, 'FP': 28, 'FN': 59, 'TN': 203, 'Precision': 0.8526315789473684, 'Recall': 0.7330316742081447, 'F1': 0.7883211678832117, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.8453038674033149, 'Recall': 0.7688442211055276, 'F1': 0.8052631578947368, 'TP': 153, 'FP': 28, 'FN': 46}, 'effect': {'Precision': 0.9348837209302325, 'Recall': 0.8410041841004184, 'F1': 0.8854625550660793, 'TP': 201, 'FP': 14, 'FN': 38}}   | {'Precision': 0.8900937941667737, 'Recall': 0.804924202602973, 'F1': 0.8453628564804081, 'TP': 354, 'FP': 42, 'FN': 84}   | {'TP': 164, 'FP': 66, 'FN': 77, 'Accuracy': 0.5342019543973942, 'Precision': 0.7130434782608696, 'Recall': 0.6804979253112033, 'F1': 0.6963906581740976}    | {'Precision': 0.8185896171250039, 'Recall': 0.7394846007074403, 'F1': 0.7766915608459057} | filtered_causal | coverage    |
| {'TP': 162, 'FP': 28, 'FN': 59, 'TN': 203, 'Precision': 0.8526315789473684, 'Recall': 0.7330316742081447, 'F1': 0.7883211678832117, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.84, 'Recall': 0.7424242424242424, 'F1': 0.7882037533512064, 'TP': 147, 'FP': 28, 'FN': 51}, 'effect': {'Precision': 0.9303482587064676, 'Recall': 0.8165938864628821, 'F1': 0.8697674418604652, 'TP': 187, 'FP': 14, 'FN': 42}}                 | {'Precision': 0.8851741293532338, 'Recall': 0.7795090644435623, 'F1': 0.8289855976058358, 'TP': 334, 'FP': 42, 'FN': 93}  | {'TP': 146, 'FP': 66, 'FN': 84, 'Accuracy': 0.49324324324324326, 'Precision': 0.6886792452830188, 'Recall': 0.6347826086956522, 'F1': 0.660633484162896}    | {'Precision': 0.8088283178612071, 'Recall': 0.7157744491157864, 'F1': 0.7593134165506479} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                               | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 152, 'FP': 26, 'FN': 69, 'TN': 205, 'Precision': 0.8539325842696629, 'Recall': 0.6877828054298643, 'F1': 0.7619047619047619, 'Accuracy': 0.7898230088495575, 'N': 452} | {'cause': {'Precision': 0.7473684210526316, 'Recall': 0.5461538461538461, 'F1': 0.6311111111111111, 'TP': 142, 'FP': 48, 'FN': 118}, 'effect': {'Precision': 0.8136363636363636, 'Recall': 0.6006711409395973, 'F1': 0.6911196911196911, 'TP': 179, 'FP': 41, 'FN': 119}} | {'Precision': 0.7805023923444976, 'Recall': 0.5734124935467217, 'F1': 0.661115401115401, 'TP': 321, 'FP': 89, 'FN': 237}  | {'TP': 147, 'FP': 82, 'FN': 151, 'Accuracy': 0.3868421052631579, 'Precision': 0.6419213973799127, 'Recall': 0.49328859060402686, 'F1': 0.5578747628083491} | {'Precision': 0.7587854579980243, 'Recall': 0.5848279631935376, 'F1': 0.660298308609504}  | all_documents   | coverage    |
| {'TP': 152, 'FP': 26, 'FN': 69, 'TN': 205, 'Precision': 0.8539325842696629, 'Recall': 0.6877828054298643, 'F1': 0.7619047619047619, 'Accuracy': 0.7898230088495575, 'N': 452} | {'cause': {'Precision': 0.7419354838709677, 'Recall': 0.5287356321839081, 'F1': 0.6174496644295302, 'TP': 138, 'FP': 48, 'FN': 123}, 'effect': {'Precision': 0.8028846153846154, 'Recall': 0.5798611111111112, 'F1': 0.6733870967741936, 'TP': 167, 'FP': 41, 'FN': 121}} | {'Precision': 0.7724100496277916, 'Recall': 0.5542983716475096, 'F1': 0.6454183806018619, 'TP': 305, 'FP': 89, 'FN': 244} | {'TP': 132, 'FP': 82, 'FN': 158, 'Accuracy': 0.3548387096774194, 'Precision': 0.616822429906542, 'Recall': 0.45517241379310347, 'F1': 0.5238095238095238}  | {'Precision': 0.7477216879346655, 'Recall': 0.5657511969568257, 'F1': 0.6437108887720492} | all_documents   | discovery   |
| {'TP': 152, 'FP': 26, 'FN': 69, 'TN': 205, 'Precision': 0.8539325842696629, 'Recall': 0.6877828054298643, 'F1': 0.7619047619047619, 'Accuracy': 0.7898230088495575, 'N': 452} | {'cause': {'Precision': 0.8819875776397516, 'Recall': 0.7593582887700535, 'F1': 0.8160919540229885, 'TP': 142, 'FP': 19, 'FN': 45}, 'effect': {'Precision': 0.9521276595744681, 'Recall': 0.8099547511312217, 'F1': 0.8753056234718826, 'TP': 179, 'FP': 9, 'FN': 42}}    | {'Precision': 0.9170576186071098, 'Recall': 0.7846565199506377, 'F1': 0.8456987887474355, 'TP': 321, 'FP': 28, 'FN': 87}  | {'TP': 147, 'FP': 47, 'FN': 78, 'Accuracy': 0.5404411764705882, 'Precision': 0.7577319587628866, 'Recall': 0.6533333333333333, 'F1': 0.701670644391408}    | {'Precision': 0.8429073872132197, 'Recall': 0.708590886237945, 'F1': 0.7697580650145351}  | filtered_causal | coverage    |
| {'TP': 152, 'FP': 26, 'FN': 69, 'TN': 205, 'Precision': 0.8539325842696629, 'Recall': 0.6877828054298643, 'F1': 0.7619047619047619, 'Accuracy': 0.7898230088495575, 'N': 452} | {'cause': {'Precision': 0.8789808917197452, 'Recall': 0.7340425531914894, 'F1': 0.8, 'TP': 138, 'FP': 19, 'FN': 50}, 'effect': {'Precision': 0.9488636363636364, 'Recall': 0.7914691943127962, 'F1': 0.8630490956072351, 'TP': 167, 'FP': 9, 'FN': 44}}                   | {'Precision': 0.9139222640416909, 'Recall': 0.7627558737521427, 'F1': 0.8315245478036175, 'TP': 305, 'FP': 28, 'FN': 94}  | {'TP': 132, 'FP': 47, 'FN': 85, 'Accuracy': 0.5, 'Precision': 0.7374301675977654, 'Recall': 0.6082949308755761, 'F1': 0.6666666666666667}                  | {'Precision': 0.8350950053030397, 'Recall': 0.6862778700191944, 'F1': 0.7533653254583488} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__cls+span__thr0.95.csv)

### Classification: `span_only`

#### Threshold = 0.60

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                               | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 172, 'FP': 33, 'FN': 49, 'TN': 198, 'Precision': 0.8390243902439024, 'Recall': 0.7782805429864253, 'F1': 0.8075117370892019, 'Accuracy': 0.8185840707964602, 'N': 452} | {'cause': {'Precision': 0.7016129032258065, 'Recall': 0.6541353383458647, 'F1': 0.6770428015564202, 'TP': 174, 'FP': 74, 'FN': 92}, 'effect': {'Precision': 0.7721088435374149, 'Recall': 0.7467105263157895, 'F1': 0.7591973244147158, 'TP': 227, 'FP': 67, 'FN': 77}} | {'Precision': 0.7368608733816107, 'Recall': 0.7004229323308271, 'F1': 0.718120062985568, 'TP': 401, 'FP': 141, 'FN': 169} | {'TP': 196, 'FP': 150, 'FN': 116, 'Accuracy': 0.42424242424242425, 'Precision': 0.5664739884393064, 'Recall': 0.6282051282051282, 'F1': 0.5957446808510638} | {'Precision': 0.7141197506882732, 'Recall': 0.7023028678407935, 'F1': 0.7071254936419445} | all_documents   | coverage    |
| {'TP': 172, 'FP': 33, 'FN': 49, 'TN': 198, 'Precision': 0.8390243902439024, 'Recall': 0.7782805429864253, 'F1': 0.8075117370892019, 'Accuracy': 0.8185840707964602, 'N': 452} | {'cause': {'Precision': 0.6890756302521008, 'Recall': 0.6283524904214559, 'F1': 0.657314629258517, 'TP': 164, 'FP': 74, 'FN': 97}, 'effect': {'Precision': 0.7545787545787546, 'Recall': 0.7152777777777778, 'F1': 0.734402852049911, 'TP': 206, 'FP': 67, 'FN': 82}}   | {'Precision': 0.7218271924154277, 'Recall': 0.6718151340996168, 'F1': 0.695858740654214, 'TP': 370, 'FP': 141, 'FN': 179} | {'TP': 167, 'FP': 150, 'FN': 123, 'Accuracy': 0.3795454545454545, 'Precision': 0.526813880126183, 'Recall': 0.5758620689655173, 'F1': 0.5502471169686985}   | {'Precision': 0.6958884875951711, 'Recall': 0.6753192486838531, 'F1': 0.6845391982373714} | all_documents   | discovery   |
| {'TP': 172, 'FP': 33, 'FN': 49, 'TN': 198, 'Precision': 0.8390243902439024, 'Recall': 0.7782805429864253, 'F1': 0.8075117370892019, 'Accuracy': 0.8185840707964602, 'N': 452} | {'cause': {'Precision': 0.8246445497630331, 'Recall': 0.8093023255813954, 'F1': 0.8169014084507042, 'TP': 174, 'FP': 37, 'FN': 41}, 'effect': {'Precision': 0.9007936507936508, 'Recall': 0.8832684824902723, 'F1': 0.8919449901768173, 'TP': 227, 'FP': 25, 'FN': 30}} | {'Precision': 0.862719100278342, 'Recall': 0.8462854040358339, 'F1': 0.8544231993137608, 'TP': 401, 'FP': 62, 'FN': 71}   | {'TP': 196, 'FP': 103, 'FN': 68, 'Accuracy': 0.5340599455040872, 'Precision': 0.6555183946488294, 'Recall': 0.7424242424242424, 'F1': 0.6962699822380106}   | {'Precision': 0.7857539617236912, 'Recall': 0.7889967298155005, 'F1': 0.7860683062136578} | filtered_causal | coverage    |
| {'TP': 172, 'FP': 33, 'FN': 49, 'TN': 198, 'Precision': 0.8390243902439024, 'Recall': 0.7782805429864253, 'F1': 0.8075117370892019, 'Accuracy': 0.8185840707964602, 'N': 452} | {'cause': {'Precision': 0.8159203980099502, 'Recall': 0.780952380952381, 'F1': 0.7980535279805353, 'TP': 164, 'FP': 37, 'FN': 46}, 'effect': {'Precision': 0.8917748917748918, 'Recall': 0.8547717842323651, 'F1': 0.8728813559322034, 'TP': 206, 'FP': 25, 'FN': 35}}  | {'Precision': 0.853847644892421, 'Recall': 0.817862082592373, 'F1': 0.8354674419563693, 'TP': 370, 'FP': 62, 'FN': 81}    | {'TP': 167, 'FP': 103, 'FN': 75, 'Accuracy': 0.48405797101449277, 'Precision': 0.6185185185185185, 'Recall': 0.6900826446280992, 'F1': 0.65234375}          | {'Precision': 0.7704635178849474, 'Recall': 0.7620750900689659, 'F1': 0.7651076430151904} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7049180327868853, 'Recall': 0.6466165413533834, 'F1': 0.6745098039215686, 'TP': 172, 'FP': 72, 'FN': 94}, 'effect': {'Precision': 0.7750865051903114, 'Recall': 0.7392739273927392, 'F1': 0.7567567567567568, 'TP': 224, 'FP': 65, 'FN': 79}} | {'Precision': 0.7400022689885983, 'Recall': 0.6929452343730613, 'F1': 0.7156332803391627, 'TP': 396, 'FP': 137, 'FN': 173} | {'TP': 191, 'FP': 140, 'FN': 119, 'Accuracy': 0.42444444444444446, 'Precision': 0.5770392749244713, 'Recall': 0.6161290322580645, 'F1': 0.5959438377535101} | {'Precision': 0.7184256126769055, 'Recall': 0.694276640913241, 'F1': 0.7054276668152046}  | all_documents   | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.6923076923076923, 'Recall': 0.6206896551724138, 'F1': 0.6545454545454545, 'TP': 162, 'FP': 72, 'FN': 99}, 'effect': {'Precision': 0.758364312267658, 'Recall': 0.7083333333333334, 'F1': 0.732495511669659, 'TP': 204, 'FP': 65, 'FN': 84}}   | {'Precision': 0.7253360022876751, 'Recall': 0.6645114942528736, 'F1': 0.6935204831075568, 'TP': 366, 'FP': 137, 'FN': 183} | {'TP': 164, 'FP': 140, 'FN': 126, 'Accuracy': 0.3813953488372093, 'Precision': 0.5394736842105263, 'Recall': 0.5655172413793104, 'F1': 0.5521885521885521}  | {'Precision': 0.7010149935386162, 'Recall': 0.6679281305802606, 'F1': 0.68347163921635}   | all_documents   | discovery   |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8309178743961353, 'Recall': 0.8037383177570093, 'F1': 0.8171021377672208, 'TP': 172, 'FP': 35, 'FN': 42}, 'effect': {'Precision': 0.9068825910931174, 'Recall': 0.8784313725490196, 'F1': 0.8924302788844621, 'TP': 224, 'FP': 23, 'FN': 31}} | {'Precision': 0.8689002327446264, 'Recall': 0.8410848451530144, 'F1': 0.8547662083258415, 'TP': 396, 'FP': 58, 'FN': 73}   | {'TP': 191, 'FP': 93, 'FN': 70, 'Accuracy': 0.53954802259887, 'Precision': 0.6725352112676056, 'Recall': 0.7318007662835249, 'F1': 0.7009174311926606}      | {'Precision': 0.7932235793766264, 'Recall': 0.7822137558483789, 'F1': 0.7867965072904811} | filtered_causal | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8223350253807107, 'Recall': 0.7751196172248804, 'F1': 0.7980295566502464, 'TP': 162, 'FP': 35, 'FN': 47}, 'effect': {'Precision': 0.8986784140969163, 'Recall': 0.85, 'F1': 0.8736616702355461, 'TP': 204, 'FP': 23, 'FN': 36}}               | {'Precision': 0.8605067197388134, 'Recall': 0.8125598086124401, 'F1': 0.8358456134428962, 'TP': 366, 'FP': 58, 'FN': 83}   | {'TP': 164, 'FP': 93, 'FN': 77, 'Accuracy': 0.49101796407185627, 'Precision': 0.6381322957198443, 'Recall': 0.6804979253112033, 'F1': 0.6586345381526104}   | {'Precision': 0.7789581031921017, 'Recall': 0.7556044633440804, 'F1': 0.7663953446494826} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7125, 'Recall': 0.6452830188679245, 'F1': 0.6772277227722773, 'TP': 171, 'FP': 69, 'FN': 94}, 'effect': {'Precision': 0.7770034843205574, 'Recall': 0.735973597359736, 'F1': 0.7559322033898306, 'TP': 223, 'FP': 64, 'FN': 80}}              | {'Precision': 0.7447517421602787, 'Recall': 0.6906283081138302, 'F1': 0.7165799630810539, 'TP': 394, 'FP': 133, 'FN': 174} | {'TP': 189, 'FP': 134, 'FN': 120, 'Accuracy': 0.42663656884875845, 'Precision': 0.5851393188854489, 'Recall': 0.6116504854368932, 'F1': 0.5981012658227848} | {'Precision': 0.7227087850544582, 'Recall': 0.6920114832197735, 'F1': 0.7064623704189267} | all_documents   | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7012987012987013, 'Recall': 0.6206896551724138, 'F1': 0.6585365853658537, 'TP': 162, 'FP': 69, 'FN': 99}, 'effect': {'Precision': 0.7602996254681648, 'Recall': 0.7048611111111112, 'F1': 0.7315315315315316, 'TP': 203, 'FP': 64, 'FN': 85}} | {'Precision': 0.730799163383433, 'Recall': 0.6627753831417624, 'F1': 0.6950340584486927, 'TP': 365, 'FP': 133, 'FN': 184}  | {'TP': 163, 'FP': 134, 'FN': 127, 'Accuracy': 0.38443396226415094, 'Precision': 0.5488215488215489, 'Recall': 0.5620689655172414, 'F1': 0.5553662691652471} | {'Precision': 0.7059520021075429, 'Recall': 0.6662000015892003, 'F1': 0.6850354033222937} | all_documents   | discovery   |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8382352941176471, 'Recall': 0.8028169014084507, 'F1': 0.8201438848920864, 'TP': 171, 'FP': 33, 'FN': 42}, 'effect': {'Precision': 0.9102040816326531, 'Recall': 0.8745098039215686, 'F1': 0.892, 'TP': 223, 'FP': 22, 'FN': 32}}              | {'Precision': 0.87421968787515, 'Recall': 0.8386633526650097, 'F1': 0.8560719424460432, 'TP': 394, 'FP': 55, 'FN': 74}     | {'TP': 189, 'FP': 88, 'FN': 71, 'Accuracy': 0.5431034482758621, 'Precision': 0.6823104693140795, 'Recall': 0.7269230769230769, 'F1': 0.7039106145251397}    | {'Precision': 0.7982551504356254, 'Recall': 0.7797806952322279, 'F1': 0.7882294797747079} | filtered_causal | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8307692307692308, 'Recall': 0.7751196172248804, 'F1': 0.8019801980198021, 'TP': 162, 'FP': 33, 'FN': 47}, 'effect': {'Precision': 0.9022222222222223, 'Recall': 0.8458333333333333, 'F1': 0.8731182795698924, 'TP': 203, 'FP': 22, 'FN': 37}} | {'Precision': 0.8664957264957265, 'Recall': 0.8104764752791069, 'F1': 0.8375492387948473, 'TP': 365, 'FP': 55, 'FN': 84}   | {'TP': 163, 'FP': 88, 'FN': 78, 'Accuracy': 0.49544072948328266, 'Precision': 0.649402390438247, 'Recall': 0.6763485477178424, 'F1': 0.66260162601626}      | {'Precision': 0.7847111370172067, 'Recall': 0.7535268930351823, 'F1': 0.7682855823880161} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7112970711297071, 'Recall': 0.6415094339622641, 'F1': 0.6746031746031746, 'TP': 170, 'FP': 69, 'FN': 95}, 'effect': {'Precision': 0.7793594306049823, 'Recall': 0.7251655629139073, 'F1': 0.751286449399657, 'TP': 219, 'FP': 62, 'FN': 83}}  | {'Precision': 0.7453282508673447, 'Recall': 0.6833374984380858, 'F1': 0.7129448120014159, 'TP': 389, 'FP': 131, 'FN': 178} | {'TP': 182, 'FP': 130, 'FN': 124, 'Accuracy': 0.41743119266055045, 'Precision': 0.5833333333333334, 'Recall': 0.5947712418300654, 'F1': 0.5889967637540453} | {'Precision': 0.7222989594394417, 'Recall': 0.6839547987922495, 'F1': 0.7022158193694675} | all_documents   | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7, 'Recall': 0.6168582375478927, 'F1': 0.6558044806517311, 'TP': 161, 'FP': 69, 'FN': 100}, 'effect': {'Precision': 0.7633587786259542, 'Recall': 0.6944444444444444, 'F1': 0.7272727272727272, 'TP': 200, 'FP': 62, 'FN': 88}}               | {'Precision': 0.7316793893129772, 'Recall': 0.6556513409961686, 'F1': 0.6915386039622291, 'TP': 361, 'FP': 131, 'FN': 188} | {'TP': 159, 'FP': 130, 'FN': 131, 'Accuracy': 0.37857142857142856, 'Precision': 0.5501730103806228, 'Recall': 0.5482758620689655, 'F1': 0.5492227979274611} | {'Precision': 0.7066958979370824, 'Recall': 0.6592276197245771, 'F1': 0.6818224280808772} | all_documents   | discovery   |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8374384236453202, 'Recall': 0.7981220657276995, 'F1': 0.8173076923076922, 'TP': 170, 'FP': 33, 'FN': 43}, 'effect': {'Precision': 0.9163179916317992, 'Recall': 0.8622047244094488, 'F1': 0.8884381338742393, 'TP': 219, 'FP': 20, 'FN': 35}} | {'Precision': 0.8768782076385597, 'Recall': 0.8301633950685742, 'F1': 0.8528729130909658, 'TP': 389, 'FP': 53, 'FN': 78}   | {'TP': 182, 'FP': 85, 'FN': 75, 'Accuracy': 0.5321637426900585, 'Precision': 0.6816479400749064, 'Recall': 0.708171206225681, 'F1': 0.6946564885496184}     | {'Precision': 0.798920480610371, 'Recall': 0.7706967524676175, 'F1': 0.7840784279978418}  | filtered_causal | coverage    |
| {'TP': 171, 'FP': 33, 'FN': 50, 'TN': 198, 'Precision': 0.8382352941176471, 'Recall': 0.7737556561085973, 'F1': 0.8047058823529412, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8298969072164949, 'Recall': 0.7703349282296651, 'F1': 0.7990074441687345, 'TP': 161, 'FP': 33, 'FN': 48}, 'effect': {'Precision': 0.9090909090909091, 'Recall': 0.8333333333333334, 'F1': 0.8695652173913043, 'TP': 200, 'FP': 20, 'FN': 40}} | {'Precision': 0.869493908153702, 'Recall': 0.8018341307814992, 'F1': 0.8342863307800195, 'TP': 361, 'FP': 53, 'FN': 88}    | {'TP': 159, 'FP': 85, 'FN': 82, 'Accuracy': 0.48773006134969327, 'Precision': 0.6516393442622951, 'Recall': 0.6597510373443983, 'F1': 0.6556701030927835}   | {'Precision': 0.7864561821778814, 'Recall': 0.745113608078165, 'F1': 0.7648874387419147}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 169, 'FP': 33, 'FN': 52, 'TN': 198, 'Precision': 0.8366336633663366, 'Recall': 0.7647058823529411, 'F1': 0.7990543735224587, 'Accuracy': 0.8119469026548672, 'N': 452} | {'cause': {'Precision': 0.7117903930131004, 'Recall': 0.6150943396226415, 'F1': 0.6599190283400809, 'TP': 163, 'FP': 66, 'FN': 102}, 'effect': {'Precision': 0.7802197802197802, 'Recall': 0.7052980132450332, 'F1': 0.7408695652173913, 'TP': 213, 'FP': 60, 'FN': 89}} | {'Precision': 0.7460050866164403, 'Recall': 0.6601961764338373, 'F1': 0.7003942967787361, 'TP': 376, 'FP': 126, 'FN': 191} | {'TP': 175, 'FP': 125, 'FN': 130, 'Accuracy': 0.4069767441860465, 'Precision': 0.5833333333333334, 'Recall': 0.5737704918032787, 'F1': 0.5785123966942148} | {'Precision': 0.7219906944387035, 'Recall': 0.666224183530019, 'F1': 0.6926536889984698}  | all_documents   | coverage    |
| {'TP': 169, 'FP': 33, 'FN': 52, 'TN': 198, 'Precision': 0.8366336633663366, 'Recall': 0.7647058823529411, 'F1': 0.7990543735224587, 'Accuracy': 0.8119469026548672, 'N': 452} | {'cause': {'Precision': 0.7, 'Recall': 0.5900383141762452, 'F1': 0.6403326403326404, 'TP': 154, 'FP': 66, 'FN': 107}, 'effect': {'Precision': 0.7647058823529411, 'Recall': 0.6770833333333334, 'F1': 0.7182320441988951, 'TP': 195, 'FP': 60, 'FN': 93}}                | {'Precision': 0.7323529411764705, 'Recall': 0.6335608237547893, 'F1': 0.6792823422657677, 'TP': 349, 'FP': 126, 'FN': 200} | {'TP': 153, 'FP': 125, 'FN': 137, 'Accuracy': 0.3686746987951807, 'Precision': 0.5503597122302158, 'Recall': 0.5275862068965518, 'F1': 0.5387323943661971} | {'Precision': 0.7064487722576743, 'Recall': 0.6419509710014274, 'F1': 0.6723563700514745} | all_documents   | discovery   |
| {'TP': 169, 'FP': 33, 'FN': 52, 'TN': 198, 'Precision': 0.8366336633663366, 'Recall': 0.7647058823529411, 'F1': 0.7990543735224587, 'Accuracy': 0.8119469026548672, 'N': 452} | {'cause': {'Precision': 0.844559585492228, 'Recall': 0.7761904761904762, 'F1': 0.8089330024813896, 'TP': 163, 'FP': 30, 'FN': 47}, 'effect': {'Precision': 0.922077922077922, 'Recall': 0.848605577689243, 'F1': 0.8838174273858922, 'TP': 213, 'FP': 18, 'FN': 38}}     | {'Precision': 0.8833187537850751, 'Recall': 0.8123980269398596, 'F1': 0.8463752149336409, 'TP': 376, 'FP': 48, 'FN': 85}   | {'TP': 175, 'FP': 80, 'FN': 78, 'Accuracy': 0.5255255255255256, 'Precision': 0.6862745098039216, 'Recall': 0.691699604743083, 'F1': 0.6889763779527559}    | {'Precision': 0.8020756423184444, 'Recall': 0.7562678380119613, 'F1': 0.7781353221362851} | filtered_causal | coverage    |
| {'TP': 169, 'FP': 33, 'FN': 52, 'TN': 198, 'Precision': 0.8366336633663366, 'Recall': 0.7647058823529411, 'F1': 0.7990543735224587, 'Accuracy': 0.8119469026548672, 'N': 452} | {'cause': {'Precision': 0.8369565217391305, 'Recall': 0.7475728155339806, 'F1': 0.7897435897435897, 'TP': 154, 'FP': 30, 'FN': 52}, 'effect': {'Precision': 0.9154929577464789, 'Recall': 0.8227848101265823, 'F1': 0.8666666666666667, 'TP': 195, 'FP': 18, 'FN': 42}}  | {'Precision': 0.8762247397428047, 'Recall': 0.7851788128302815, 'F1': 0.8282051282051281, 'TP': 349, 'FP': 48, 'FN': 94}   | {'TP': 153, 'FP': 80, 'FN': 85, 'Accuracy': 0.4811320754716981, 'Precision': 0.6566523605150214, 'Recall': 0.6428571428571429, 'F1': 0.6496815286624203}   | {'Precision': 0.7898369212080544, 'Recall': 0.7309139460134552, 'F1': 0.7589803434633358} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 168, 'FP': 33, 'FN': 53, 'TN': 198, 'Precision': 0.835820895522388, 'Recall': 0.7601809954751131, 'F1': 0.7962085308056872, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7162162162162162, 'Recall': 0.6068702290076335, 'F1': 0.6570247933884297, 'TP': 159, 'FP': 63, 'FN': 103}, 'effect': {'Precision': 0.7824427480916031, 'Recall': 0.6879194630872483, 'F1': 0.7321428571428571, 'TP': 205, 'FP': 57, 'FN': 93}} | {'Precision': 0.7493294821539096, 'Recall': 0.6473948460474409, 'F1': 0.6945838252656433, 'TP': 364, 'FP': 120, 'FN': 196} | {'TP': 168, 'FP': 111, 'FN': 133, 'Accuracy': 0.4077669902912621, 'Precision': 0.6021505376344086, 'Recall': 0.5581395348837209, 'F1': 0.5793103448275861} | {'Precision': 0.7291003051035688, 'Recall': 0.6552384588020916, 'F1': 0.6900342336329722} | all_documents   | coverage    |
| {'TP': 168, 'FP': 33, 'FN': 53, 'TN': 198, 'Precision': 0.835820895522388, 'Recall': 0.7601809954751131, 'F1': 0.7962085308056872, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7083333333333334, 'Recall': 0.5862068965517241, 'F1': 0.6415094339622641, 'TP': 153, 'FP': 63, 'FN': 108}, 'effect': {'Precision': 0.7701612903225806, 'Recall': 0.6631944444444444, 'F1': 0.712686567164179, 'TP': 191, 'FP': 57, 'FN': 97}}  | {'Precision': 0.739247311827957, 'Recall': 0.6247006704980842, 'F1': 0.6770980005632216, 'TP': 344, 'FP': 120, 'FN': 205}  | {'TP': 150, 'FP': 111, 'FN': 140, 'Accuracy': 0.3740648379052369, 'Precision': 0.5747126436781609, 'Recall': 0.5172413793103449, 'F1': 0.544464609800363}  | {'Precision': 0.7165936170095021, 'Recall': 0.634041015094514, 'F1': 0.6725903803897574}  | all_documents   | discovery   |
| {'TP': 168, 'FP': 33, 'FN': 53, 'TN': 198, 'Precision': 0.835820895522388, 'Recall': 0.7601809954751131, 'F1': 0.7962085308056872, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8502673796791443, 'Recall': 0.7718446601941747, 'F1': 0.8091603053435115, 'TP': 159, 'FP': 28, 'FN': 47}, 'effect': {'Precision': 0.9276018099547512, 'Recall': 0.8333333333333334, 'F1': 0.8779443254817988, 'TP': 205, 'FP': 16, 'FN': 41}}  | {'Precision': 0.8889345948169478, 'Recall': 0.8025889967637541, 'F1': 0.8435523154126552, 'TP': 364, 'FP': 44, 'FN': 88}   | {'TP': 168, 'FP': 68, 'FN': 80, 'Accuracy': 0.5316455696202531, 'Precision': 0.711864406779661, 'Recall': 0.6774193548387096, 'F1': 0.6942148760330579}    | {'Precision': 0.812206632372999, 'Recall': 0.7467297823591923, 'F1': 0.7779919074171334}  | filtered_causal | coverage    |
| {'TP': 168, 'FP': 33, 'FN': 53, 'TN': 198, 'Precision': 0.835820895522388, 'Recall': 0.7601809954751131, 'F1': 0.7962085308056872, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8453038674033149, 'Recall': 0.7463414634146341, 'F1': 0.7927461139896373, 'TP': 153, 'FP': 28, 'FN': 52}, 'effect': {'Precision': 0.9227053140096618, 'Recall': 0.809322033898305, 'F1': 0.8623024830699774, 'TP': 191, 'FP': 16, 'FN': 45}}   | {'Precision': 0.8840045907064884, 'Recall': 0.7778317486564696, 'F1': 0.8275242985298074, 'TP': 344, 'FP': 44, 'FN': 97}   | {'TP': 150, 'FP': 68, 'FN': 87, 'Accuracy': 0.4918032786885246, 'Precision': 0.6880733944954128, 'Recall': 0.6329113924050633, 'F1': 0.6593406593406594}   | {'Precision': 0.8026329602414298, 'Recall': 0.7236413788455488, 'F1': 0.7610244962253846} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 158, 'FP': 31, 'FN': 63, 'TN': 200, 'Precision': 0.8359788359788359, 'Recall': 0.7149321266968326, 'F1': 0.7707317073170732, 'Accuracy': 0.7920353982300885, 'N': 452} | {'cause': {'Precision': 0.74, 'Recall': 0.5692307692307692, 'F1': 0.6434782608695652, 'TP': 148, 'FP': 52, 'FN': 112}, 'effect': {'Precision': 0.7854077253218884, 'Recall': 0.6140939597315436, 'F1': 0.6892655367231637, 'TP': 183, 'FP': 50, 'FN': 115}}             | {'Precision': 0.7627038626609441, 'Recall': 0.5916623644811564, 'F1': 0.6663718987963645, 'TP': 331, 'FP': 102, 'FN': 227} | {'TP': 151, 'FP': 90, 'FN': 147, 'Accuracy': 0.38917525773195877, 'Precision': 0.6265560165975104, 'Recall': 0.5067114093959731, 'F1': 0.5602968460111317} | {'Precision': 0.7417462384124301, 'Recall': 0.6044353001913207, 'F1': 0.6658001507081898} | all_documents   | coverage    |
| {'TP': 158, 'FP': 31, 'FN': 63, 'TN': 200, 'Precision': 0.8359788359788359, 'Recall': 0.7149321266968326, 'F1': 0.7707317073170732, 'Accuracy': 0.7920353982300885, 'N': 452} | {'cause': {'Precision': 0.7346938775510204, 'Recall': 0.5517241379310345, 'F1': 0.6301969365426695, 'TP': 144, 'FP': 52, 'FN': 117}, 'effect': {'Precision': 0.7737556561085973, 'Recall': 0.59375, 'F1': 0.6719056974459726, 'TP': 171, 'FP': 50, 'FN': 117}}          | {'Precision': 0.7542247668298089, 'Recall': 0.5727370689655172, 'F1': 0.6510513169943211, 'TP': 315, 'FP': 102, 'FN': 234} | {'TP': 136, 'FP': 90, 'FN': 154, 'Accuracy': 0.35789473684210527, 'Precision': 0.6017699115044248, 'Recall': 0.4689655172413793, 'F1': 0.5271317829457364} | {'Precision': 0.7306578381043565, 'Recall': 0.5855449043012431, 'F1': 0.6496382690857102} | all_documents   | discovery   |
| {'TP': 158, 'FP': 31, 'FN': 63, 'TN': 200, 'Precision': 0.8359788359788359, 'Recall': 0.7149321266968326, 'F1': 0.7707317073170732, 'Accuracy': 0.7920353982300885, 'N': 452} | {'cause': {'Precision': 0.8862275449101796, 'Recall': 0.7628865979381443, 'F1': 0.8199445983379502, 'TP': 148, 'FP': 19, 'FN': 46}, 'effect': {'Precision': 0.9432989690721649, 'Recall': 0.8026315789473685, 'F1': 0.8672985781990522, 'TP': 183, 'FP': 11, 'FN': 45}} | {'Precision': 0.9147632569911723, 'Recall': 0.7827590884427564, 'F1': 0.8436215882685012, 'TP': 331, 'FP': 30, 'FN': 91}   | {'TP': 151, 'FP': 49, 'FN': 81, 'Accuracy': 0.5373665480427047, 'Precision': 0.755, 'Recall': 0.6508620689655172, 'F1': 0.6990740740740741}                | {'Precision': 0.8352473643233361, 'Recall': 0.7161844280350355, 'F1': 0.7711424565532162} | filtered_causal | coverage    |
| {'TP': 158, 'FP': 31, 'FN': 63, 'TN': 200, 'Precision': 0.8359788359788359, 'Recall': 0.7149321266968326, 'F1': 0.7707317073170732, 'Accuracy': 0.7920353982300885, 'N': 452} | {'cause': {'Precision': 0.8834355828220859, 'Recall': 0.7384615384615385, 'F1': 0.8044692737430168, 'TP': 144, 'FP': 19, 'FN': 51}, 'effect': {'Precision': 0.9395604395604396, 'Recall': 0.7844036697247706, 'F1': 0.8549999999999999, 'TP': 171, 'FP': 11, 'FN': 47}} | {'Precision': 0.9114980111912627, 'Recall': 0.7614326040931545, 'F1': 0.8297346368715084, 'TP': 315, 'FP': 30, 'FN': 98}   | {'TP': 136, 'FP': 49, 'FN': 88, 'Accuracy': 0.4981684981684982, 'Precision': 0.7351351351351352, 'Recall': 0.6071428571428571, 'F1': 0.6650366748166259}   | {'Precision': 0.827537327435078, 'Recall': 0.694502529310948, 'F1': 0.7551676730017358}   | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-softmax__span_only__thr0.95.csv)

## bert-gce

### Classification: `cls+span`

#### Threshold = 0.60

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.47509578544061304, 'F1': 0.5548098434004475, 'TP': 124, 'FP': 62, 'FN': 137}, 'effect': {'Precision': 0.75, 'Recall': 0.5733788395904437, 'F1': 0.6499032882011604, 'TP': 168, 'FP': 56, 'FN': 125}}             | {'Precision': 0.7083333333333333, 'Recall': 0.5242373125155284, 'F1': 0.6023565658008039, 'TP': 292, 'FP': 118, 'FN': 262} | {'TP': 119, 'FP': 93, 'FN': 172, 'Accuracy': 0.3098958333333333, 'Precision': 0.5613207547169812, 'Recall': 0.40893470790378006, 'F1': 0.4731610337972167}  | {'Precision': 0.7077190909212301, 'Recall': 0.5131689540160892, 'F1': 0.5948374361975765} | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6630434782608695, 'Recall': 0.4674329501915709, 'F1': 0.548314606741573, 'TP': 122, 'FP': 62, 'FN': 139}, 'effect': {'Precision': 0.7383177570093458, 'Recall': 0.5486111111111112, 'F1': 0.6294820717131475, 'TP': 158, 'FP': 56, 'FN': 130}} | {'Precision': 0.7006806176351077, 'Recall': 0.508022030651341, 'F1': 0.5888983392273602, 'TP': 280, 'FP': 118, 'FN': 269}  | {'TP': 114, 'FP': 93, 'FN': 176, 'Accuracy': 0.29765013054830286, 'Precision': 0.5507246376811594, 'Recall': 0.3931034482758621, 'F1': 0.45875251509054327} | {'Precision': 0.7016361466765476, 'Recall': 0.5024867735187208, 'F1': 0.5855485211042041} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.775, 'Recall': 0.7654320987654321, 'F1': 0.7701863354037267, 'TP': 124, 'FP': 36, 'FN': 38}, 'effect': {'Precision': 0.8615384615384616, 'Recall': 0.8527918781725888, 'F1': 0.8571428571428572, 'TP': 168, 'FP': 27, 'FN': 29}}               | {'Precision': 0.8182692307692307, 'Recall': 0.8091119884690104, 'F1': 0.813664596273292, 'TP': 292, 'FP': 63, 'FN': 67}    | {'TP': 119, 'FP': 63, 'FN': 70, 'Accuracy': 0.4722222222222222, 'Precision': 0.6538461538461539, 'Recall': 0.6296296296296297, 'F1': 0.6415094339622641}    | {'Precision': 0.7752061897762536, 'Recall': 0.6816921532425332, 'F1': 0.7213895797434217} | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7721518987341772, 'Recall': 0.7530864197530864, 'F1': 0.7625, 'TP': 122, 'FP': 36, 'FN': 40}, 'effect': {'Precision': 0.8540540540540541, 'Recall': 0.8229166666666666, 'F1': 0.8381962864721485, 'TP': 158, 'FP': 27, 'FN': 34}}              | {'Precision': 0.8131029763941157, 'Recall': 0.7880015432098766, 'F1': 0.8003481432360742, 'TP': 280, 'FP': 63, 'FN': 74}   | {'TP': 114, 'FP': 63, 'FN': 74, 'Accuracy': 0.4541832669322709, 'Precision': 0.6440677966101694, 'Recall': 0.6063829787234043, 'F1': 0.6246575342465754}    | {'Precision': 0.7702246525725537, 'Recall': 0.6669064545207467, 'F1': 0.7113334621591195} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.47509578544061304, 'F1': 0.5548098434004475, 'TP': 124, 'FP': 62, 'FN': 137}, 'effect': {'Precision': 0.7488789237668162, 'Recall': 0.571917808219178, 'F1': 0.6485436893203884, 'TP': 167, 'FP': 56, 'FN': 125}} | {'Precision': 0.7077727952167414, 'Recall': 0.5235067968298955, 'F1': 0.6016767663604179, 'TP': 291, 'FP': 118, 'FN': 262} | {'TP': 119, 'FP': 93, 'FN': 172, 'Accuracy': 0.3098958333333333, 'Precision': 0.5613207547169812, 'Recall': 0.40893470790378006, 'F1': 0.4731610337972167}  | {'Precision': 0.7075322448823661, 'Recall': 0.512925448787545, 'F1': 0.5946108363841146}  | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6630434782608695, 'Recall': 0.4674329501915709, 'F1': 0.548314606741573, 'TP': 122, 'FP': 62, 'FN': 139}, 'effect': {'Precision': 0.7383177570093458, 'Recall': 0.5486111111111112, 'F1': 0.6294820717131475, 'TP': 158, 'FP': 56, 'FN': 130}}  | {'Precision': 0.7006806176351077, 'Recall': 0.508022030651341, 'F1': 0.5888983392273602, 'TP': 280, 'FP': 118, 'FN': 269}  | {'TP': 114, 'FP': 93, 'FN': 176, 'Accuracy': 0.29765013054830286, 'Precision': 0.5507246376811594, 'Recall': 0.3931034482758621, 'F1': 0.45875251509054327} | {'Precision': 0.7016361466765476, 'Recall': 0.5024867735187208, 'F1': 0.5855485211042041} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.775, 'Recall': 0.7654320987654321, 'F1': 0.7701863354037267, 'TP': 124, 'FP': 36, 'FN': 38}, 'effect': {'Precision': 0.8608247422680413, 'Recall': 0.8520408163265306, 'F1': 0.8564102564102565, 'TP': 167, 'FP': 27, 'FN': 29}}                | {'Precision': 0.8179123711340206, 'Recall': 0.8087364575459813, 'F1': 0.8132982959069917, 'TP': 291, 'FP': 63, 'FN': 67}   | {'TP': 119, 'FP': 63, 'FN': 70, 'Accuracy': 0.4722222222222222, 'Precision': 0.6538461538461539, 'Recall': 0.6296296296296297, 'F1': 0.6415094339622641}    | {'Precision': 0.7750872365645168, 'Recall': 0.6815669762681901, 'F1': 0.7212674796213215} | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7721518987341772, 'Recall': 0.7530864197530864, 'F1': 0.7625, 'TP': 122, 'FP': 36, 'FN': 40}, 'effect': {'Precision': 0.8540540540540541, 'Recall': 0.8229166666666666, 'F1': 0.8381962864721485, 'TP': 158, 'FP': 27, 'FN': 34}}               | {'Precision': 0.8131029763941157, 'Recall': 0.7880015432098766, 'F1': 0.8003481432360742, 'TP': 280, 'FP': 63, 'FN': 74}   | {'TP': 114, 'FP': 63, 'FN': 74, 'Accuracy': 0.4541832669322709, 'Precision': 0.6440677966101694, 'Recall': 0.6063829787234043, 'F1': 0.6246575342465754}    | {'Precision': 0.7702246525725537, 'Recall': 0.6669064545207467, 'F1': 0.7113334621591195} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.47509578544061304, 'F1': 0.5548098434004475, 'TP': 124, 'FP': 62, 'FN': 137}, 'effect': {'Precision': 0.7488789237668162, 'Recall': 0.571917808219178, 'F1': 0.6485436893203884, 'TP': 167, 'FP': 56, 'FN': 125}} | {'Precision': 0.7077727952167414, 'Recall': 0.5235067968298955, 'F1': 0.6016767663604179, 'TP': 291, 'FP': 118, 'FN': 262} | {'TP': 119, 'FP': 93, 'FN': 172, 'Accuracy': 0.3098958333333333, 'Precision': 0.5613207547169812, 'Recall': 0.40893470790378006, 'F1': 0.4731610337972167}  | {'Precision': 0.7075322448823661, 'Recall': 0.512925448787545, 'F1': 0.5946108363841146}  | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6630434782608695, 'Recall': 0.4674329501915709, 'F1': 0.548314606741573, 'TP': 122, 'FP': 62, 'FN': 139}, 'effect': {'Precision': 0.7383177570093458, 'Recall': 0.5486111111111112, 'F1': 0.6294820717131475, 'TP': 158, 'FP': 56, 'FN': 130}}  | {'Precision': 0.7006806176351077, 'Recall': 0.508022030651341, 'F1': 0.5888983392273602, 'TP': 280, 'FP': 118, 'FN': 269}  | {'TP': 114, 'FP': 93, 'FN': 176, 'Accuracy': 0.29765013054830286, 'Precision': 0.5507246376811594, 'Recall': 0.3931034482758621, 'F1': 0.45875251509054327} | {'Precision': 0.7016361466765476, 'Recall': 0.5024867735187208, 'F1': 0.5855485211042041} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.775, 'Recall': 0.7654320987654321, 'F1': 0.7701863354037267, 'TP': 124, 'FP': 36, 'FN': 38}, 'effect': {'Precision': 0.8608247422680413, 'Recall': 0.8520408163265306, 'F1': 0.8564102564102565, 'TP': 167, 'FP': 27, 'FN': 29}}                | {'Precision': 0.8179123711340206, 'Recall': 0.8087364575459813, 'F1': 0.8132982959069917, 'TP': 291, 'FP': 63, 'FN': 67}   | {'TP': 119, 'FP': 63, 'FN': 70, 'Accuracy': 0.4722222222222222, 'Precision': 0.6538461538461539, 'Recall': 0.6296296296296297, 'F1': 0.6415094339622641}    | {'Precision': 0.7750872365645168, 'Recall': 0.6815669762681901, 'F1': 0.7212674796213215} | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7721518987341772, 'Recall': 0.7530864197530864, 'F1': 0.7625, 'TP': 122, 'FP': 36, 'FN': 40}, 'effect': {'Precision': 0.8540540540540541, 'Recall': 0.8229166666666666, 'F1': 0.8381962864721485, 'TP': 158, 'FP': 27, 'FN': 34}}               | {'Precision': 0.8131029763941157, 'Recall': 0.7880015432098766, 'F1': 0.8003481432360742, 'TP': 280, 'FP': 63, 'FN': 74}   | {'TP': 114, 'FP': 63, 'FN': 74, 'Accuracy': 0.4541832669322709, 'Precision': 0.6440677966101694, 'Recall': 0.6063829787234043, 'F1': 0.6246575342465754}    | {'Precision': 0.7702246525725537, 'Recall': 0.6669064545207467, 'F1': 0.7113334621591195} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6684782608695652, 'Recall': 0.47126436781609193, 'F1': 0.5528089887640449, 'TP': 123, 'FP': 61, 'FN': 138}, 'effect': {'Precision': 0.7488789237668162, 'Recall': 0.571917808219178, 'F1': 0.6485436893203884, 'TP': 167, 'FP': 56, 'FN': 125}} | {'Precision': 0.7086785923181906, 'Recall': 0.521591088017635, 'F1': 0.6006763390422167, 'TP': 290, 'FP': 117, 'FN': 263}  | {'TP': 118, 'FP': 92, 'FN': 173, 'Accuracy': 0.30809399477806787, 'Precision': 0.5619047619047619, 'Recall': 0.4054982817869416, 'F1': 0.47105788423153694} | {'Precision': 0.7080288463121095, 'Recall': 0.5111414038111786, 'F1': 0.5935763107561542} | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6648351648351648, 'Recall': 0.46360153256704983, 'F1': 0.54627539503386, 'TP': 121, 'FP': 61, 'FN': 140}, 'effect': {'Precision': 0.7383177570093458, 'Recall': 0.5486111111111112, 'F1': 0.6294820717131475, 'TP': 158, 'FP': 56, 'FN': 130}}  | {'Precision': 0.7015764609222552, 'Recall': 0.5061063218390804, 'F1': 0.5878787333735038, 'TP': 279, 'FP': 117, 'FN': 270} | {'TP': 113, 'FP': 92, 'FN': 177, 'Accuracy': 0.29581151832460734, 'Precision': 0.551219512195122, 'Recall': 0.3896551724137931, 'F1': 0.45656565656565656}  | {'Precision': 0.7020997192769176, 'Recall': 0.5006987786272775, 'F1': 0.5844796996446232} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7784810126582279, 'Recall': 0.7592592592592593, 'F1': 0.76875, 'TP': 123, 'FP': 35, 'FN': 39}, 'effect': {'Precision': 0.8608247422680413, 'Recall': 0.8520408163265306, 'F1': 0.8564102564102565, 'TP': 167, 'FP': 27, 'FN': 29}}              | {'Precision': 0.8196528774631346, 'Recall': 0.805650037792895, 'F1': 0.8125801282051283, 'TP': 290, 'FP': 62, 'FN': 68}    | {'TP': 118, 'FP': 62, 'FN': 71, 'Accuracy': 0.4701195219123506, 'Precision': 0.6555555555555556, 'Recall': 0.6243386243386243, 'F1': 0.6395663956639566}    | {'Precision': 0.7762372059106887, 'Recall': 0.6787745012534928, 'F1': 0.7203804109545979} | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7756410256410257, 'Recall': 0.7469135802469136, 'F1': 0.761006289308176, 'TP': 121, 'FP': 35, 'FN': 41}, 'effect': {'Precision': 0.8540540540540541, 'Recall': 0.8229166666666666, 'F1': 0.8381962864721485, 'TP': 158, 'FP': 27, 'FN': 34}}    | {'Precision': 0.8148475398475399, 'Recall': 0.7849151234567902, 'F1': 0.7996012878901623, 'TP': 279, 'FP': 62, 'FN': 75}   | {'TP': 113, 'FP': 62, 'FN': 75, 'Accuracy': 0.452, 'Precision': 0.6457142857142857, 'Recall': 0.601063829787234, 'F1': 0.6225895316804407}                  | {'Precision': 0.7713550034250671, 'Recall': 0.6641045982909944, 'F1': 0.7103951761884373} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.4674329501915709, 'F1': 0.5495495495495495, 'TP': 122, 'FP': 61, 'FN': 139}, 'effect': {'Precision': 0.7522522522522522, 'Recall': 0.571917808219178, 'F1': 0.6498054474708171, 'TP': 167, 'FP': 55, 'FN': 125}}   | {'Precision': 0.7094594594594594, 'Recall': 0.5196753792053744, 'F1': 0.5996774985101834, 'TP': 289, 'FP': 116, 'FN': 264} | {'TP': 117, 'FP': 91, 'FN': 174, 'Accuracy': 0.306282722513089, 'Precision': 0.5625, 'Recall': 0.4020618556701031, 'F1': 0.468937875751503}                 | {'Precision': 0.7084875480576117, 'Recall': 0.5093573588348123, 'F1': 0.5925366944187985} | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6629834254143646, 'Recall': 0.45977011494252873, 'F1': 0.5429864253393665, 'TP': 120, 'FP': 61, 'FN': 141}, 'effect': {'Precision': 0.7417840375586855, 'Recall': 0.5486111111111112, 'F1': 0.6307385229540918, 'TP': 158, 'FP': 55, 'FN': 130}} | {'Precision': 0.7023837314865251, 'Recall': 0.5041906130268199, 'F1': 0.5868624741467292, 'TP': 278, 'FP': 116, 'FN': 271} | {'TP': 112, 'FP': 91, 'FN': 178, 'Accuracy': 0.29396325459317585, 'Precision': 0.5517241379310345, 'Recall': 0.38620689655172413, 'F1': 0.4543610547667343} | {'Precision': 0.7025370180436452, 'Recall': 0.4989107837358344, 'F1': 0.5834060793027241} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7770700636942676, 'Recall': 0.7530864197530864, 'F1': 0.7648902821316615, 'TP': 122, 'FP': 35, 'FN': 40}, 'effect': {'Precision': 0.8652849740932642, 'Recall': 0.8520408163265306, 'F1': 0.8586118251928021, 'TP': 167, 'FP': 26, 'FN': 29}}    | {'Precision': 0.8211775188937659, 'Recall': 0.8025636180398086, 'F1': 0.8117510536622319, 'TP': 289, 'FP': 61, 'FN': 69}   | {'TP': 117, 'FP': 61, 'FN': 72, 'Accuracy': 0.468, 'Precision': 0.6573033707865169, 'Recall': 0.6190476190476191, 'F1': 0.6376021798365124}                 | {'Precision': 0.7773280247978862, 'Recall': 0.6759820262387956, 'F1': 0.7194493141644843} | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7741935483870968, 'Recall': 0.7407407407407407, 'F1': 0.7570977917981072, 'TP': 120, 'FP': 35, 'FN': 42}, 'effect': {'Precision': 0.8586956521739131, 'Recall': 0.8229166666666666, 'F1': 0.8404255319148937, 'TP': 158, 'FP': 26, 'FN': 34}}    | {'Precision': 0.8164446002805049, 'Recall': 0.7818287037037037, 'F1': 0.7987616618565004, 'TP': 278, 'FP': 61, 'FN': 76}   | {'TP': 112, 'FP': 61, 'FN': 76, 'Accuracy': 0.4497991967871486, 'Precision': 0.6473988439306358, 'Recall': 0.5957446808510638, 'F1': 0.6204986149584487}    | {'Precision': 0.7724488763081722, 'Recall': 0.6613027420612423, 'F1': 0.7094183286032193} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                                | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6648351648351648, 'Recall': 0.46360153256704983, 'F1': 0.54627539503386, 'TP': 121, 'FP': 61, 'FN': 140}, 'effect': {'Precision': 0.7522522522522522, 'Recall': 0.571917808219178, 'F1': 0.6498054474708171, 'TP': 167, 'FP': 55, 'FN': 125}}   | {'Precision': 0.7085437085437085, 'Recall': 0.517759670393114, 'F1': 0.5980404212523386, 'TP': 288, 'FP': 116, 'FN': 265}  | {'TP': 117, 'FP': 89, 'FN': 174, 'Accuracy': 0.3078947368421053, 'Precision': 0.5679611650485437, 'Recall': 0.4020618556701031, 'F1': 0.47082494969818917} | {'Precision': 0.710002686101876, 'Recall': 0.5087187892307253, 'F1': 0.5926200266484122}   | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6611111111111111, 'Recall': 0.4559386973180077, 'F1': 0.5396825396825397, 'TP': 119, 'FP': 61, 'FN': 142}, 'effect': {'Precision': 0.7417840375586855, 'Recall': 0.5486111111111112, 'F1': 0.6307385229540918, 'TP': 158, 'FP': 55, 'FN': 130}} | {'Precision': 0.7014475743348982, 'Recall': 0.5022749042145594, 'F1': 0.5852105313183158, 'TP': 277, 'FP': 116, 'FN': 272} | {'TP': 112, 'FP': 89, 'FN': 178, 'Accuracy': 0.2955145118733509, 'Precision': 0.5572139303482587, 'Recall': 0.38620689655172413, 'F1': 0.4562118126272912} | {'Precision': 0.7040548964655109, 'Recall': 0.49827221413174766, 'F1': 0.5834723509801053} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7756410256410257, 'Recall': 0.7469135802469136, 'F1': 0.761006289308176, 'TP': 121, 'FP': 35, 'FN': 41}, 'effect': {'Precision': 0.8652849740932642, 'Recall': 0.8520408163265306, 'F1': 0.8586118251928021, 'TP': 167, 'FP': 26, 'FN': 29}}    | {'Precision': 0.8204629998671449, 'Recall': 0.7994771982867221, 'F1': 0.809809057250489, 'TP': 288, 'FP': 61, 'FN': 70}    | {'TP': 117, 'FP': 59, 'FN': 72, 'Accuracy': 0.4717741935483871, 'Precision': 0.6647727272727273, 'Recall': 0.6190476190476191, 'F1': 0.6410958904109589}   | {'Precision': 0.779579637284416, 'Recall': 0.6749532196544336, 'F1': 0.7199665522187191}   | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7727272727272727, 'Recall': 0.7345679012345679, 'F1': 0.7531645569620253, 'TP': 119, 'FP': 35, 'FN': 43}, 'effect': {'Precision': 0.8586956521739131, 'Recall': 0.8229166666666666, 'F1': 0.8404255319148937, 'TP': 158, 'FP': 26, 'FN': 34}}   | {'Precision': 0.8157114624505929, 'Recall': 0.7787422839506173, 'F1': 0.7967950444384595, 'TP': 277, 'FP': 61, 'FN': 77}   | {'TP': 112, 'FP': 59, 'FN': 76, 'Accuracy': 0.4534412955465587, 'Precision': 0.6549707602339181, 'Recall': 0.5957446808510638, 'F1': 0.6239554317548747}   | {'Precision': 0.7747284691326289, 'Recall': 0.6602739354768801, 'F1': 0.7099150617293476}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                                | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6722222222222223, 'Recall': 0.46360153256704983, 'F1': 0.5487528344671202, 'TP': 121, 'FP': 59, 'FN': 140}, 'effect': {'Precision': 0.7545454545454545, 'Recall': 0.5684931506849316, 'F1': 0.6484375, 'TP': 166, 'FP': 54, 'FN': 126}}         | {'Precision': 0.7133838383838385, 'Recall': 0.5160473416259908, 'F1': 0.5985951672335601, 'TP': 287, 'FP': 113, 'FN': 266} | {'TP': 117, 'FP': 84, 'FN': 174, 'Accuracy': 0.312, 'Precision': 0.582089552238806, 'Recall': 0.4020618556701031, 'F1': 0.475609756097561}                 | {'Precision': 0.7163255251120068, 'Recall': 0.5081480129750177, 'F1': 0.5943998774419433}  | all_documents   | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.6685393258426966, 'Recall': 0.4559386973180077, 'F1': 0.5421412300683371, 'TP': 119, 'FP': 59, 'FN': 142}, 'effect': {'Precision': 0.7440758293838863, 'Recall': 0.5451388888888888, 'F1': 0.6292585170340681, 'TP': 157, 'FP': 54, 'FN': 131}} | {'Precision': 0.7063075776132914, 'Recall': 0.5005387931034483, 'F1': 0.5856998735512027, 'TP': 276, 'FP': 113, 'FN': 273} | {'TP': 112, 'FP': 84, 'FN': 178, 'Accuracy': 0.2994652406417112, 'Precision': 0.5714285714285714, 'Recall': 0.38620689655172413, 'F1': 0.4609053497942387} | {'Precision': 0.7104131112517461, 'Recall': 0.49769351042804394, 'F1': 0.5851999774467168} | all_documents   | discovery   |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7857142857142857, 'Recall': 0.7469135802469136, 'F1': 0.7658227848101264, 'TP': 121, 'FP': 33, 'FN': 41}, 'effect': {'Precision': 0.8691099476439791, 'Recall': 0.8469387755102041, 'F1': 0.8578811369509044, 'TP': 166, 'FP': 25, 'FN': 30}}   | {'Precision': 0.8274121166791324, 'Recall': 0.7969261778785588, 'F1': 0.8118519608805155, 'TP': 287, 'FP': 58, 'FN': 71}   | {'TP': 117, 'FP': 56, 'FN': 72, 'Accuracy': 0.4775510204081633, 'Precision': 0.6763005780346821, 'Recall': 0.6190476190476191, 'F1': 0.6464088397790054}   | {'Precision': 0.7857386264757301, 'Recall': 0.674102879518379, 'F1': 0.7224185032180767}   | filtered_causal | coverage    |
| {'TP': 134, 'FP': 23, 'FN': 87, 'TN': 208, 'Precision': 0.8535031847133758, 'Recall': 0.6063348416289592, 'F1': 0.708994708994709, 'Accuracy': 0.7566371681415929, 'N': 452} | {'cause': {'Precision': 0.7828947368421053, 'Recall': 0.7345679012345679, 'F1': 0.7579617834394904, 'TP': 119, 'FP': 33, 'FN': 43}, 'effect': {'Precision': 0.8626373626373627, 'Recall': 0.8177083333333334, 'F1': 0.839572192513369, 'TP': 157, 'FP': 25, 'FN': 35}}    | {'Precision': 0.822766049739734, 'Recall': 0.7761381172839507, 'F1': 0.7987669879764296, 'TP': 276, 'FP': 58, 'FN': 78}    | {'TP': 112, 'FP': 56, 'FN': 76, 'Accuracy': 0.45901639344262296, 'Precision': 0.6666666666666666, 'Recall': 0.5957446808510638, 'F1': 0.6292134831460674}  | {'Precision': 0.7809786337065922, 'Recall': 0.6594058799213246, 'F1': 0.7123250600390687}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__cls+span__thr0.95.csv)

### Classification: `span_only`

#### Threshold = 0.60

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.4827586206896552, 'F1': 0.56, 'TP': 126, 'FP': 63, 'FN': 135}, 'effect': {'Precision': 0.7489177489177489, 'Recall': 0.590443686006826, 'F1': 0.6603053435114504, 'TP': 173, 'FP': 58, 'FN': 120}}                 | {'Precision': 0.7077922077922078, 'Recall': 0.5366011533482405, 'F1': 0.6101526717557253, 'TP': 299, 'FP': 121, 'FN': 255} | {'TP': 123, 'FP': 94, 'FN': 168, 'Accuracy': 0.3194805194805195, 'Precision': 0.5668202764976958, 'Recall': 0.422680412371134, 'F1': 0.48425196850393704} | {'Precision': 0.7067638322118608, 'Recall': 0.526397022660606, 'F1': 0.6032697799995219}  | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6631016042780749, 'Recall': 0.47509578544061304, 'F1': 0.5535714285714286, 'TP': 124, 'FP': 63, 'FN': 137}, 'effect': {'Precision': 0.7375565610859729, 'Recall': 0.5659722222222222, 'F1': 0.6404715127701375, 'TP': 163, 'FP': 58, 'FN': 125}} | {'Precision': 0.7003290826820239, 'Recall': 0.5205340038314177, 'F1': 0.597021470670783, 'TP': 287, 'FP': 121, 'FN': 262}  | {'TP': 118, 'FP': 94, 'FN': 172, 'Accuracy': 0.3072916666666667, 'Precision': 0.5566037735849056, 'Recall': 0.4068965517241379, 'F1': 0.4701195219123506} | {'Precision': 0.7008706228708695, 'Recall': 0.5157800192726664, 'F1': 0.5941818974406791} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7730061349693251, 'Recall': 0.7590361445783133, 'F1': 0.7659574468085106, 'TP': 126, 'FP': 37, 'FN': 40}, 'effect': {'Precision': 0.865, 'Recall': 0.8480392156862745, 'F1': 0.8564356435643564, 'TP': 173, 'FP': 27, 'FN': 31}}                 | {'Precision': 0.8190030674846626, 'Recall': 0.8035376801322939, 'F1': 0.8111965451864336, 'TP': 299, 'FP': 64, 'FN': 71}   | {'TP': 123, 'FP': 64, 'FN': 73, 'Accuracy': 0.47307692307692306, 'Precision': 0.6577540106951871, 'Recall': 0.6275510204081632, 'F1': 0.6422976501305484} | {'Precision': 0.7741453635085095, 'Recall': 0.6836660676009668, 'F1': 0.7229662983519618} | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7701863354037267, 'Recall': 0.7469879518072289, 'F1': 0.7584097859327217, 'TP': 124, 'FP': 37, 'FN': 42}, 'effect': {'Precision': 0.8578947368421053, 'Recall': 0.8190954773869347, 'F1': 0.8380462724935733, 'TP': 163, 'FP': 27, 'FN': 36}}    | {'Precision': 0.8140405361229159, 'Recall': 0.7830417145970818, 'F1': 0.7982280292131475, 'TP': 287, 'FP': 64, 'FN': 78}   | {'TP': 118, 'FP': 64, 'FN': 77, 'Accuracy': 0.4555984555984556, 'Precision': 0.6483516483516484, 'Recall': 0.6051282051282051, 'F1': 0.6259946949602122}  | {'Precision': 0.7693570656067478, 'Recall': 0.6693598073292435, 'F1': 0.7132091413040879} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.4827586206896552, 'F1': 0.56, 'TP': 126, 'FP': 63, 'FN': 135}, 'effect': {'Precision': 0.7478260869565218, 'Recall': 0.589041095890411, 'F1': 0.6590038314176245, 'TP': 172, 'FP': 58, 'FN': 120}}                 | {'Precision': 0.7072463768115942, 'Recall': 0.5358998582900331, 'F1': 0.6095019157088123, 'TP': 298, 'FP': 121, 'FN': 255} | {'TP': 123, 'FP': 94, 'FN': 168, 'Accuracy': 0.3194805194805195, 'Precision': 0.5668202764976958, 'Recall': 0.422680412371134, 'F1': 0.48425196850393704} | {'Precision': 0.7065818885516563, 'Recall': 0.5261632576412035, 'F1': 0.6030528613172176} | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6631016042780749, 'Recall': 0.47509578544061304, 'F1': 0.5535714285714286, 'TP': 124, 'FP': 63, 'FN': 137}, 'effect': {'Precision': 0.7375565610859729, 'Recall': 0.5659722222222222, 'F1': 0.6404715127701375, 'TP': 163, 'FP': 58, 'FN': 125}} | {'Precision': 0.7003290826820239, 'Recall': 0.5205340038314177, 'F1': 0.597021470670783, 'TP': 287, 'FP': 121, 'FN': 262}  | {'TP': 118, 'FP': 94, 'FN': 172, 'Accuracy': 0.3072916666666667, 'Precision': 0.5566037735849056, 'Recall': 0.4068965517241379, 'F1': 0.4701195219123506} | {'Precision': 0.7008706228708695, 'Recall': 0.5157800192726664, 'F1': 0.5941818974406791} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7730061349693251, 'Recall': 0.7590361445783133, 'F1': 0.7659574468085106, 'TP': 126, 'FP': 37, 'FN': 40}, 'effect': {'Precision': 0.864321608040201, 'Recall': 0.8472906403940886, 'F1': 0.8557213930348258, 'TP': 172, 'FP': 27, 'FN': 31}}     | {'Precision': 0.818663871504763, 'Recall': 0.8031633924862009, 'F1': 0.8108394199216682, 'TP': 298, 'FP': 64, 'FN': 71}    | {'TP': 123, 'FP': 64, 'FN': 73, 'Accuracy': 0.47307692307692306, 'Precision': 0.6577540106951871, 'Recall': 0.6275510204081632, 'F1': 0.6422976501305484} | {'Precision': 0.7740322981818765, 'Recall': 0.6835413050522692, 'F1': 0.72284725659704}   | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7701863354037267, 'Recall': 0.7469879518072289, 'F1': 0.7584097859327217, 'TP': 124, 'FP': 37, 'FN': 42}, 'effect': {'Precision': 0.8578947368421053, 'Recall': 0.8190954773869347, 'F1': 0.8380462724935733, 'TP': 163, 'FP': 27, 'FN': 36}}    | {'Precision': 0.8140405361229159, 'Recall': 0.7830417145970818, 'F1': 0.7982280292131475, 'TP': 287, 'FP': 64, 'FN': 78}   | {'TP': 118, 'FP': 64, 'FN': 77, 'Accuracy': 0.4555984555984556, 'Precision': 0.6483516483516484, 'Recall': 0.6051282051282051, 'F1': 0.6259946949602122}  | {'Precision': 0.7693570656067478, 'Recall': 0.6693598073292435, 'F1': 0.7132091413040879} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.4827586206896552, 'F1': 0.56, 'TP': 126, 'FP': 63, 'FN': 135}, 'effect': {'Precision': 0.7478260869565218, 'Recall': 0.589041095890411, 'F1': 0.6590038314176245, 'TP': 172, 'FP': 58, 'FN': 120}}                 | {'Precision': 0.7072463768115942, 'Recall': 0.5358998582900331, 'F1': 0.6095019157088123, 'TP': 298, 'FP': 121, 'FN': 255} | {'TP': 123, 'FP': 94, 'FN': 168, 'Accuracy': 0.3194805194805195, 'Precision': 0.5668202764976958, 'Recall': 0.422680412371134, 'F1': 0.48425196850393704} | {'Precision': 0.7065818885516563, 'Recall': 0.5261632576412035, 'F1': 0.6030528613172176} | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6631016042780749, 'Recall': 0.47509578544061304, 'F1': 0.5535714285714286, 'TP': 124, 'FP': 63, 'FN': 137}, 'effect': {'Precision': 0.7375565610859729, 'Recall': 0.5659722222222222, 'F1': 0.6404715127701375, 'TP': 163, 'FP': 58, 'FN': 125}} | {'Precision': 0.7003290826820239, 'Recall': 0.5205340038314177, 'F1': 0.597021470670783, 'TP': 287, 'FP': 121, 'FN': 262}  | {'TP': 118, 'FP': 94, 'FN': 172, 'Accuracy': 0.3072916666666667, 'Precision': 0.5566037735849056, 'Recall': 0.4068965517241379, 'F1': 0.4701195219123506} | {'Precision': 0.7008706228708695, 'Recall': 0.5157800192726664, 'F1': 0.5941818974406791} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7730061349693251, 'Recall': 0.7590361445783133, 'F1': 0.7659574468085106, 'TP': 126, 'FP': 37, 'FN': 40}, 'effect': {'Precision': 0.864321608040201, 'Recall': 0.8472906403940886, 'F1': 0.8557213930348258, 'TP': 172, 'FP': 27, 'FN': 31}}     | {'Precision': 0.818663871504763, 'Recall': 0.8031633924862009, 'F1': 0.8108394199216682, 'TP': 298, 'FP': 64, 'FN': 71}    | {'TP': 123, 'FP': 64, 'FN': 73, 'Accuracy': 0.47307692307692306, 'Precision': 0.6577540106951871, 'Recall': 0.6275510204081632, 'F1': 0.6422976501305484} | {'Precision': 0.7740322981818765, 'Recall': 0.6835413050522692, 'F1': 0.72284725659704}   | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7701863354037267, 'Recall': 0.7469879518072289, 'F1': 0.7584097859327217, 'TP': 124, 'FP': 37, 'FN': 42}, 'effect': {'Precision': 0.8578947368421053, 'Recall': 0.8190954773869347, 'F1': 0.8380462724935733, 'TP': 163, 'FP': 27, 'FN': 36}}    | {'Precision': 0.8140405361229159, 'Recall': 0.7830417145970818, 'F1': 0.7982280292131475, 'TP': 287, 'FP': 64, 'FN': 78}   | {'TP': 118, 'FP': 64, 'FN': 77, 'Accuracy': 0.4555984555984556, 'Precision': 0.6483516483516484, 'Recall': 0.6051282051282051, 'F1': 0.6259946949602122}  | {'Precision': 0.7693570656067478, 'Recall': 0.6693598073292435, 'F1': 0.7132091413040879} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                        | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6684491978609626, 'Recall': 0.4789272030651341, 'F1': 0.5580357142857143, 'TP': 125, 'FP': 62, 'FN': 136}, 'effect': {'Precision': 0.7478260869565218, 'Recall': 0.589041095890411, 'F1': 0.6590038314176245, 'TP': 172, 'FP': 58, 'FN': 120}}   | {'Precision': 0.7081376424087422, 'Recall': 0.5339841494777725, 'F1': 0.6085197728516694, 'TP': 297, 'FP': 120, 'FN': 256} | {'TP': 122, 'FP': 93, 'FN': 169, 'Accuracy': 0.3177083333333333, 'Precision': 0.5674418604651162, 'Recall': 0.41924398625429554, 'F1': 0.48221343873517786}  | {'Precision': 0.7070861717398458, 'Recall': 0.5243792126648372, 'F1': 0.6020459704419169} | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6648648648648648, 'Recall': 0.47126436781609193, 'F1': 0.5515695067264574, 'TP': 123, 'FP': 62, 'FN': 138}, 'effect': {'Precision': 0.7375565610859729, 'Recall': 0.5659722222222222, 'F1': 0.6404715127701375, 'TP': 163, 'FP': 58, 'FN': 125}} | {'Precision': 0.7012107129754188, 'Recall': 0.5186182950191571, 'F1': 0.5960205097482975, 'TP': 286, 'FP': 120, 'FN': 263} | {'TP': 117, 'FP': 93, 'FN': 173, 'Accuracy': 0.30548302872062666, 'Precision': 0.5571428571428572, 'Recall': 0.40344827586206894, 'F1': 0.46799999999999997} | {'Precision': 0.7013441941546517, 'Recall': 0.5139920243812232, 'F1': 0.5931417364957335} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7763975155279503, 'Recall': 0.7530120481927711, 'F1': 0.764525993883792, 'TP': 125, 'FP': 36, 'FN': 41}, 'effect': {'Precision': 0.864321608040201, 'Recall': 0.8472906403940886, 'F1': 0.8557213930348258, 'TP': 172, 'FP': 27, 'FN': 31}}      | {'Precision': 0.8203595617840757, 'Recall': 0.8001513442934298, 'F1': 0.8101236934593089, 'TP': 297, 'FP': 63, 'FN': 72}   | {'TP': 122, 'FP': 63, 'FN': 74, 'Accuracy': 0.47104247104247104, 'Precision': 0.6594594594594595, 'Recall': 0.6224489795918368, 'F1': 0.6404199475065617}    | {'Precision': 0.7751660111964047, 'Recall': 0.6808366087159033, 'F1': 0.7219827802349247} | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7735849056603774, 'Recall': 0.7409638554216867, 'F1': 0.756923076923077, 'TP': 123, 'FP': 36, 'FN': 43}, 'effect': {'Precision': 0.8578947368421053, 'Recall': 0.8190954773869347, 'F1': 0.8380462724935733, 'TP': 163, 'FP': 27, 'FN': 36}}     | {'Precision': 0.8157398212512413, 'Recall': 0.7800296664043107, 'F1': 0.7974846747083252, 'TP': 286, 'FP': 63, 'FN': 79}   | {'TP': 117, 'FP': 63, 'FN': 78, 'Accuracy': 0.45348837209302323, 'Precision': 0.65, 'Recall': 0.6, 'F1': 0.624}                                              | {'Precision': 0.7704729445323067, 'Recall': 0.6666463895555848, 'F1': 0.7122964581490763} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.47509578544061304, 'F1': 0.5548098434004475, 'TP': 124, 'FP': 62, 'FN': 137}, 'effect': {'Precision': 0.7510917030567685, 'Recall': 0.589041095890411, 'F1': 0.6602687140115163, 'TP': 172, 'FP': 57, 'FN': 120}} | {'Precision': 0.7088791848617175, 'Recall': 0.5320684406655121, 'F1': 0.6075392787059819, 'TP': 296, 'FP': 119, 'FN': 257} | {'TP': 121, 'FP': 92, 'FN': 170, 'Accuracy': 0.31592689295039167, 'Precision': 0.568075117370892, 'Recall': 0.41580756013745707, 'F1': 0.4801587301587302} | {'Precision': 0.7075444381927628, 'Recall': 0.5225951676884709, 'F1': 0.6010342362012052} | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6630434782608695, 'Recall': 0.4674329501915709, 'F1': 0.548314606741573, 'TP': 122, 'FP': 62, 'FN': 139}, 'effect': {'Precision': 0.740909090909091, 'Recall': 0.5659722222222222, 'F1': 0.641732283464567, 'TP': 163, 'FP': 57, 'FN': 125}}    | {'Precision': 0.7019762845849802, 'Recall': 0.5167025862068966, 'F1': 0.59502344510307, 'TP': 285, 'FP': 119, 'FN': 264}   | {'TP': 116, 'FP': 92, 'FN': 174, 'Accuracy': 0.3036649214659686, 'Precision': 0.5576923076923077, 'Recall': 0.4, 'F1': 0.46586345381526106}                | {'Precision': 0.7017825348743223, 'Recall': 0.51220402948978, 'F1': 0.5920971995524115}   | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.775, 'Recall': 0.7469879518072289, 'F1': 0.7607361963190183, 'TP': 124, 'FP': 36, 'FN': 42}, 'effect': {'Precision': 0.8686868686868687, 'Recall': 0.8472906403940886, 'F1': 0.85785536159601, 'TP': 172, 'FP': 26, 'FN': 31}}                  | {'Precision': 0.8218434343434344, 'Recall': 0.7971392961006587, 'F1': 0.8092957789575141, 'TP': 296, 'FP': 62, 'FN': 73}   | {'TP': 121, 'FP': 62, 'FN': 75, 'Accuracy': 0.4689922480620155, 'Precision': 0.6612021857923497, 'Recall': 0.6173469387755102, 'F1': 0.6385224274406331}   | {'Precision': 0.7762415441604876, 'Recall': 0.6781319123795374, 'F1': 0.7210743020456837} | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7721518987341772, 'Recall': 0.7349397590361446, 'F1': 0.7530864197530864, 'TP': 122, 'FP': 36, 'FN': 44}, 'effect': {'Precision': 0.8624338624338624, 'Recall': 0.8190954773869347, 'F1': 0.8402061855670103, 'TP': 163, 'FP': 26, 'FN': 36}}   | {'Precision': 0.8172928805840198, 'Recall': 0.7770176182115396, 'F1': 0.7966463026600483, 'TP': 285, 'FP': 62, 'FN': 80}   | {'TP': 116, 'FP': 62, 'FN': 79, 'Accuracy': 0.45136186770428016, 'Precision': 0.651685393258427, 'Recall': 0.5948717948717949, 'F1': 0.6219839142091154}   | {'Precision': 0.7715524287293754, 'Recall': 0.663932971781926, 'F1': 0.711344972202689}   | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6648648648648648, 'Recall': 0.47126436781609193, 'F1': 0.5515695067264574, 'TP': 123, 'FP': 62, 'FN': 138}, 'effect': {'Precision': 0.7510917030567685, 'Recall': 0.589041095890411, 'F1': 0.6602687140115163, 'TP': 172, 'FP': 57, 'FN': 120}} | {'Precision': 0.7079782839608166, 'Recall': 0.5301527318532515, 'F1': 0.6059191103689869, 'TP': 295, 'FP': 119, 'FN': 258} | {'TP': 121, 'FP': 90, 'FN': 170, 'Accuracy': 0.31758530183727035, 'Precision': 0.5734597156398105, 'Recall': 0.41580756013745707, 'F1': 0.4820717131474104} | {'Precision': 0.7090390039821021, 'Recall': 0.521956598084384, 'F1': 0.6011318410851002}  | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6612021857923497, 'Recall': 0.46360153256704983, 'F1': 0.545045045045045, 'TP': 121, 'FP': 62, 'FN': 140}, 'effect': {'Precision': 0.740909090909091, 'Recall': 0.5659722222222222, 'F1': 0.641732283464567, 'TP': 163, 'FP': 57, 'FN': 125}}   | {'Precision': 0.7010556383507203, 'Recall': 0.514786877394636, 'F1': 0.5933886642548061, 'TP': 284, 'FP': 119, 'FN': 265}  | {'TP': 116, 'FP': 90, 'FN': 174, 'Accuracy': 0.30526315789473685, 'Precision': 0.5631067961165048, 'Recall': 0.4, 'F1': 0.46774193548387094}                | {'Precision': 0.7032804822709681, 'Recall': 0.5115654598856931, 'F1': 0.5921784331591935} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7735849056603774, 'Recall': 0.7409638554216867, 'F1': 0.756923076923077, 'TP': 123, 'FP': 36, 'FN': 43}, 'effect': {'Precision': 0.8686868686868687, 'Recall': 0.8472906403940886, 'F1': 0.85785536159601, 'TP': 172, 'FP': 26, 'FN': 31}}      | {'Precision': 0.8211358871736231, 'Recall': 0.7941272479078877, 'F1': 0.8073892192595435, 'TP': 295, 'FP': 62, 'FN': 74}   | {'TP': 121, 'FP': 60, 'FN': 75, 'Accuracy': 0.47265625, 'Precision': 0.6685082872928176, 'Recall': 0.6173469387755102, 'F1': 0.6419098143236074}            | {'Precision': 0.7784410622707066, 'Recall': 0.6771278963152804, 'F1': 0.7215679111073515} | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7707006369426752, 'Recall': 0.7289156626506024, 'F1': 0.7492260061919506, 'TP': 121, 'FP': 36, 'FN': 45}, 'effect': {'Precision': 0.8624338624338624, 'Recall': 0.8190954773869347, 'F1': 0.8402061855670103, 'TP': 163, 'FP': 26, 'FN': 36}}   | {'Precision': 0.8165672496882688, 'Recall': 0.7740055700187685, 'F1': 0.7947160958794804, 'TP': 284, 'FP': 62, 'FN': 81}   | {'TP': 116, 'FP': 60, 'FN': 79, 'Accuracy': 0.4549019607843137, 'Precision': 0.6590909090909091, 'Recall': 0.5948717948717949, 'F1': 0.6253369272237197}    | {'Precision': 0.7737790570416189, 'Recall': 0.662928955717669, 'F1': 0.7118192409473677}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6721311475409836, 'Recall': 0.47126436781609193, 'F1': 0.554054054054054, 'TP': 123, 'FP': 60, 'FN': 138}, 'effect': {'Precision': 0.7533039647577092, 'Recall': 0.5856164383561644, 'F1': 0.6589595375722542, 'TP': 171, 'FP': 56, 'FN': 121}} | {'Precision': 0.7127175561493464, 'Recall': 0.5284404030861282, 'F1': 0.606506795813154, 'TP': 294, 'FP': 116, 'FN': 259}  | {'TP': 121, 'FP': 85, 'FN': 170, 'Accuracy': 0.32180851063829785, 'Precision': 0.587378640776699, 'Recall': 0.41580756013745707, 'F1': 0.48692152917505027} | {'Precision': 0.7152584030905748, 'Recall': 0.5213858218286763, 'F1': 0.6029443415757026} | all_documents   | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.6685082872928176, 'Recall': 0.46360153256704983, 'F1': 0.5475113122171946, 'TP': 121, 'FP': 60, 'FN': 140}, 'effect': {'Precision': 0.7431192660550459, 'Recall': 0.5625, 'F1': 0.6403162055335968, 'TP': 162, 'FP': 56, 'FN': 126}}            | {'Precision': 0.7058137766739317, 'Recall': 0.5130507662835249, 'F1': 0.5939137588753958, 'TP': 283, 'FP': 116, 'FN': 266} | {'TP': 116, 'FP': 85, 'FN': 174, 'Accuracy': 0.30933333333333335, 'Precision': 0.5771144278606966, 'Recall': 0.4, 'F1': 0.4725050916496945}                 | {'Precision': 0.7095357389601024, 'Recall': 0.5109867561819894, 'F1': 0.5939411834213312} | all_documents   | discovery   |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7834394904458599, 'Recall': 0.7409638554216867, 'F1': 0.7616099071207432, 'TP': 123, 'FP': 34, 'FN': 43}, 'effect': {'Precision': 0.8724489795918368, 'Recall': 0.8423645320197044, 'F1': 0.8571428571428571, 'TP': 171, 'FP': 25, 'FN': 32}}   | {'Precision': 0.8279442350188484, 'Recall': 0.7916641937206956, 'F1': 0.8093763821318001, 'TP': 294, 'FP': 59, 'FN': 75}   | {'TP': 121, 'FP': 57, 'FN': 75, 'Accuracy': 0.4782608695652174, 'Precision': 0.6797752808988764, 'Recall': 0.6173469387755102, 'F1': 0.6470588235294117}    | {'Precision': 0.7844661760878013, 'Recall': 0.6763068782528832, 'F1': 0.7239466351333718} | filtered_causal | coverage    |
| {'TP': 137, 'FP': 25, 'FN': 84, 'TN': 206, 'Precision': 0.845679012345679, 'Recall': 0.6199095022624435, 'F1': 0.7154046997389034, 'Accuracy': 0.7588495575221239, 'N': 452} | {'cause': {'Precision': 0.7806451612903226, 'Recall': 0.7289156626506024, 'F1': 0.7538940809968847, 'TP': 121, 'FP': 34, 'FN': 45}, 'effect': {'Precision': 0.8663101604278075, 'Recall': 0.8140703517587939, 'F1': 0.8393782383419689, 'TP': 162, 'FP': 25, 'FN': 37}}   | {'Precision': 0.823477660859065, 'Recall': 0.7714930072046982, 'F1': 0.7966361596694268, 'TP': 283, 'FP': 59, 'FN': 82}    | {'TP': 116, 'FP': 57, 'FN': 79, 'Accuracy': 0.4603174603174603, 'Precision': 0.6705202312138728, 'Recall': 0.5948717948717949, 'F1': 0.6304347826086957}    | {'Precision': 0.7798923014728723, 'Recall': 0.6620914347796455, 'F1': 0.7141585473390086} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce__span_only__thr0.95.csv)

## bert-gce-softmax

### Classification: `cls+span`

#### Threshold = 0.60

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7603686635944701, 'Recall': 0.6111111111111112, 'F1': 0.6776180698151951, 'TP': 165, 'FP': 52, 'FN': 105}, 'effect': {'Precision': 0.7832699619771863, 'Recall': 0.6754098360655738, 'F1': 0.7253521126760565, 'TP': 206, 'FP': 57, 'FN': 99}} | {'Precision': 0.7718193127858282, 'Recall': 0.6432604735883425, 'F1': 0.7014850912456259, 'TP': 371, 'FP': 109, 'FN': 204} | {'TP': 173, 'FP': 98, 'FN': 138, 'Accuracy': 0.4229828850855746, 'Precision': 0.6383763837638377, 'Recall': 0.5562700964630225, 'F1': 0.5945017182130584}  | {'Precision': 0.7610412911150819, 'Recall': 0.6381542322493993, 'F1': 0.6940188203999926} | all_documents   | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7475728155339806, 'Recall': 0.5900383141762452, 'F1': 0.6595289079229122, 'TP': 154, 'FP': 52, 'FN': 107}, 'effect': {'Precision': 0.7625, 'Recall': 0.6354166666666666, 'F1': 0.6931818181818181, 'TP': 183, 'FP': 57, 'FN': 105}}            | {'Precision': 0.7550364077669902, 'Recall': 0.6127274904214559, 'F1': 0.6763553630523651, 'TP': 337, 'FP': 109, 'FN': 212} | {'TP': 148, 'FP': 98, 'FN': 142, 'Accuracy': 0.38144329896907214, 'Precision': 0.6016260162601627, 'Recall': 0.5103448275862069, 'F1': 0.5522388059701493} | {'Precision': 0.7431968669409109, 'Recall': 0.6126681482348318, 'F1': 0.6715546069212692} | all_documents   | discovery   |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8823529411764706, 'Recall': 0.825, 'F1': 0.8527131782945736, 'TP': 165, 'FP': 22, 'FN': 35}, 'effect': {'Precision': 0.8803418803418803, 'Recall': 0.8547717842323651, 'F1': 0.8673684210526316, 'TP': 206, 'FP': 28, 'FN': 35}}               | {'Precision': 0.8813474107591754, 'Recall': 0.8398858921161825, 'F1': 0.8600407996736026, 'TP': 371, 'FP': 50, 'FN': 70}   | {'TP': 173, 'FP': 63, 'FN': 68, 'Accuracy': 0.569078947368421, 'Precision': 0.7330508474576272, 'Recall': 0.7178423236514523, 'F1': 0.7253668763102725}    | {'Precision': 0.8291088116707943, 'Recall': 0.7575534474881559, 'F1': 0.7904924425750561} | filtered_causal | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.875, 'Recall': 0.806282722513089, 'F1': 0.8392370572207085, 'TP': 154, 'FP': 22, 'FN': 37}, 'effect': {'Precision': 0.8672985781990521, 'Recall': 0.8169642857142857, 'F1': 0.8413793103448276, 'TP': 183, 'FP': 28, 'FN': 41}}                | {'Precision': 0.871149289099526, 'Recall': 0.8116235041136873, 'F1': 0.8403081837827681, 'TP': 337, 'FP': 50, 'FN': 78}    | {'TP': 148, 'FP': 63, 'FN': 72, 'Accuracy': 0.5229681978798587, 'Precision': 0.7014218009478673, 'Recall': 0.6727272727272727, 'F1': 0.6867749419953596}   | {'Precision': 0.8151664222809911, 'Recall': 0.7330943011792642, 'F1': 0.7710509258398072} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7627906976744186, 'Recall': 0.6074074074074074, 'F1': 0.6762886597938144, 'TP': 164, 'FP': 51, 'FN': 106}, 'effect': {'Precision': 0.7807692307692308, 'Recall': 0.6721854304635762, 'F1': 0.7224199288256229, 'TP': 203, 'FP': 57, 'FN': 99}} | {'Precision': 0.7717799642218247, 'Recall': 0.6397964189354918, 'F1': 0.6993542943097186, 'TP': 367, 'FP': 108, 'FN': 205} | {'TP': 169, 'FP': 97, 'FN': 139, 'Accuracy': 0.41728395061728396, 'Precision': 0.6353383458646616, 'Recall': 0.5487012987012987, 'F1': 0.5888501742160278} | {'Precision': 0.7600154956273556, 'Recall': 0.6344766147778743, 'F1': 0.6914247067556799} | all_documents   | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.75, 'Recall': 0.5862068965517241, 'F1': 0.6580645161290322, 'TP': 153, 'FP': 51, 'FN': 108}, 'effect': {'Precision': 0.7625, 'Recall': 0.6354166666666666, 'F1': 0.6931818181818181, 'TP': 183, 'FP': 57, 'FN': 105}}                          | {'Precision': 0.75625, 'Recall': 0.6108117816091954, 'F1': 0.6756231671554251, 'TP': 336, 'FP': 108, 'FN': 213}            | {'TP': 147, 'FP': 97, 'FN': 143, 'Accuracy': 0.3798449612403101, 'Precision': 0.6024590163934426, 'Recall': 0.506896551724138, 'F1': 0.550561797752809}    | {'Precision': 0.7438790643963409, 'Recall': 0.6108801533433886, 'F1': 0.6707515388831758} | all_documents   | discovery   |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8817204301075269, 'Recall': 0.82, 'F1': 0.849740932642487, 'TP': 164, 'FP': 22, 'FN': 36}, 'effect': {'Precision': 0.8787878787878788, 'Recall': 0.8529411764705882, 'F1': 0.8656716417910447, 'TP': 203, 'FP': 28, 'FN': 35}}                 | {'Precision': 0.8802541544477028, 'Recall': 0.8364705882352941, 'F1': 0.8577062872167658, 'TP': 367, 'FP': 50, 'FN': 71}   | {'TP': 169, 'FP': 62, 'FN': 69, 'Accuracy': 0.5633333333333334, 'Precision': 0.7316017316017316, 'Recall': 0.7100840336134454, 'F1': 0.7206823027718551}   | {'Precision': 0.8282613542816715, 'Recall': 0.7538289161818574, 'F1': 0.7881527472433048} | filtered_causal | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8742857142857143, 'Recall': 0.8010471204188482, 'F1': 0.836065573770492, 'TP': 153, 'FP': 22, 'FN': 38}, 'effect': {'Precision': 0.8672985781990521, 'Recall': 0.8169642857142857, 'F1': 0.8413793103448276, 'TP': 183, 'FP': 28, 'FN': 41}}   | {'Precision': 0.8707921462423832, 'Recall': 0.8090057030665669, 'F1': 0.8387224420576598, 'TP': 336, 'FP': 50, 'FN': 79}   | {'TP': 147, 'FP': 62, 'FN': 73, 'Accuracy': 0.5212765957446809, 'Precision': 0.7033492822966507, 'Recall': 0.6681818181818182, 'F1': 0.6853146853146853}   | {'Precision': 0.8156898684448713, 'Recall': 0.7307065493150725, 'F1': 0.7700355930378796} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7663551401869159, 'Recall': 0.6074074074074074, 'F1': 0.6776859504132231, 'TP': 164, 'FP': 50, 'FN': 106}, 'effect': {'Precision': 0.7790697674418605, 'Recall': 0.6677740863787376, 'F1': 0.7191413237924866, 'TP': 201, 'FP': 57, 'FN': 100}} | {'Precision': 0.7727124538143881, 'Recall': 0.6375907468930725, 'F1': 0.6984136371028549, 'TP': 365, 'FP': 107, 'FN': 206} | {'TP': 168, 'FP': 96, 'FN': 139, 'Accuracy': 0.41687344913151364, 'Precision': 0.6363636363636364, 'Recall': 0.5472312703583062, 'F1': 0.5884413309982487} | {'Precision': 0.7606680889912015, 'Recall': 0.6332513813160704, 'F1': 0.690974873280799}  | all_documents   | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7536945812807881, 'Recall': 0.5862068965517241, 'F1': 0.6594827586206896, 'TP': 153, 'FP': 50, 'FN': 108}, 'effect': {'Precision': 0.7615062761506276, 'Recall': 0.6319444444444444, 'F1': 0.6907020872865274, 'TP': 182, 'FP': 57, 'FN': 106}} | {'Precision': 0.7576004287157079, 'Recall': 0.6090756704980842, 'F1': 0.6750924229536085, 'TP': 335, 'FP': 107, 'FN': 214} | {'TP': 147, 'FP': 96, 'FN': 143, 'Accuracy': 0.38082901554404147, 'Precision': 0.6049382716049383, 'Recall': 0.506896551724138, 'F1': 0.551594746716698}   | {'Precision': 0.7451556257054088, 'Recall': 0.6103014496396849, 'F1': 0.6709189404705334} | all_documents   | discovery   |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8864864864864865, 'Recall': 0.82, 'F1': 0.8519480519480519, 'TP': 164, 'FP': 21, 'FN': 36}, 'effect': {'Precision': 0.8777292576419214, 'Recall': 0.8481012658227848, 'F1': 0.8626609442060086, 'TP': 201, 'FP': 28, 'FN': 36}}                 | {'Precision': 0.8821078720642039, 'Recall': 0.8340506329113924, 'F1': 0.8573044980770302, 'TP': 365, 'FP': 49, 'FN': 72}   | {'TP': 168, 'FP': 61, 'FN': 69, 'Accuracy': 0.5637583892617449, 'Precision': 0.7336244541484717, 'Recall': 0.7088607594936709, 'F1': 0.7210300429184548}   | {'Precision': 0.8295535010027519, 'Recall': 0.7526145063672987, 'F1': 0.7881347309122595} | filtered_causal | coverage    |
| {'TP': 158, 'FP': 23, 'FN': 63, 'TN': 208, 'Precision': 0.8729281767955801, 'Recall': 0.7149321266968326, 'F1': 0.7860696517412935, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8793103448275862, 'Recall': 0.8010471204188482, 'F1': 0.8383561643835615, 'TP': 153, 'FP': 21, 'FN': 38}, 'effect': {'Precision': 0.8666666666666667, 'Recall': 0.8125, 'F1': 0.8387096774193549, 'TP': 182, 'FP': 28, 'FN': 42}}               | {'Precision': 0.8729885057471265, 'Recall': 0.8067735602094241, 'F1': 0.8385329209014583, 'TP': 335, 'FP': 49, 'FN': 80}   | {'TP': 147, 'FP': 61, 'FN': 73, 'Accuracy': 0.5231316725978647, 'Precision': 0.7067307692307693, 'Recall': 0.6681818181818182, 'F1': 0.6869158878504672}   | {'Precision': 0.8175491505911586, 'Recall': 0.7299625016960251, 'F1': 0.7705061534977397} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7688679245283019, 'Recall': 0.6014760147601476, 'F1': 0.6749482401656315, 'TP': 163, 'FP': 49, 'FN': 108}, 'effect': {'Precision': 0.7834645669291339, 'Recall': 0.6611295681063123, 'F1': 0.7171171171171171, 'TP': 199, 'FP': 55, 'FN': 102}} | {'Precision': 0.776166245728718, 'Recall': 0.6313027914332299, 'F1': 0.6960326786413743, 'TP': 362, 'FP': 104, 'FN': 210}  | {'TP': 165, 'FP': 91, 'FN': 140, 'Accuracy': 0.4166666666666667, 'Precision': 0.64453125, 'Recall': 0.5409836065573771, 'F1': 0.5882352941176472}         | {'Precision': 0.7643065726503133, 'Recall': 0.6275645459365372, 'F1': 0.689103455591328}  | all_documents   | coverage    |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7562189054726368, 'Recall': 0.5823754789272031, 'F1': 0.6580086580086579, 'TP': 152, 'FP': 49, 'FN': 109}, 'effect': {'Precision': 0.7659574468085106, 'Recall': 0.625, 'F1': 0.6883365200764818, 'TP': 180, 'FP': 55, 'FN': 108}}              | {'Precision': 0.7610881761405737, 'Recall': 0.6036877394636015, 'F1': 0.6731725890425699, 'TP': 332, 'FP': 104, 'FN': 217} | {'TP': 146, 'FP': 91, 'FN': 144, 'Accuracy': 0.38320209973753283, 'Precision': 0.6160337552742616, 'Recall': 0.503448275862069, 'F1': 0.5540796963946869} | {'Precision': 0.7497813845456859, 'Recall': 0.6058477517148916, 'F1': 0.6700982264840731} | all_documents   | discovery   |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.8907103825136612, 'Recall': 0.815, 'F1': 0.8511749347258486, 'TP': 163, 'FP': 20, 'FN': 37}, 'effect': {'Precision': 0.8805309734513275, 'Recall': 0.8432203389830508, 'F1': 0.8614718614718614, 'TP': 199, 'FP': 27, 'FN': 37}}                | {'Precision': 0.8856206779824943, 'Recall': 0.8291101694915254, 'F1': 0.8563233980988549, 'TP': 362, 'FP': 47, 'FN': 74}   | {'TP': 165, 'FP': 57, 'FN': 69, 'Accuracy': 0.5670103092783505, 'Precision': 0.7432432432432432, 'Recall': 0.7051282051282052, 'F1': 0.7236842105263159}  | {'Precision': 0.8336953811493201, 'Recall': 0.7482152048129117, 'F1': 0.7876833342133778} | filtered_causal | coverage    |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.8837209302325582, 'Recall': 0.8, 'F1': 0.839779005524862, 'TP': 152, 'FP': 20, 'FN': 38}, 'effect': {'Precision': 0.8695652173913043, 'Recall': 0.8071748878923767, 'F1': 0.8372093023255814, 'TP': 180, 'FP': 27, 'FN': 43}}                   | {'Precision': 0.8766430738119313, 'Recall': 0.8035874439461883, 'F1': 0.8384941539252218, 'TP': 332, 'FP': 47, 'FN': 81}   | {'TP': 146, 'FP': 57, 'FN': 73, 'Accuracy': 0.5289855072463768, 'Precision': 0.7192118226600985, 'Recall': 0.6666666666666666, 'F1': 0.6919431279620852}  | {'Precision': 0.822692372898084, 'Recall': 0.7268871168106198, 'F1': 0.7711598919674231}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                    | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7677725118483413, 'Recall': 0.5977859778597786, 'F1': 0.6721991701244814, 'TP': 162, 'FP': 49, 'FN': 109}, 'effect': {'Precision': 0.7817460317460317, 'Recall': 0.6566666666666666, 'F1': 0.7137681159420289, 'TP': 197, 'FP': 55, 'FN': 103}} | {'Precision': 0.7747592717971865, 'Recall': 0.6272263222632226, 'F1': 0.6929836430332552, 'TP': 359, 'FP': 104, 'FN': 212} | {'TP': 163, 'FP': 90, 'FN': 141, 'Accuracy': 0.4137055837563452, 'Precision': 0.6442687747035574, 'Recall': 0.5361842105263158, 'F1': 0.585278276481149} | {'Precision': 0.7637500895743221, 'Recall': 0.6246059242028478, 'F1': 0.6871014378431223} | all_documents   | coverage    |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.755, 'Recall': 0.578544061302682, 'F1': 0.6550976138828634, 'TP': 151, 'FP': 49, 'FN': 110}, 'effect': {'Precision': 0.7649572649572649, 'Recall': 0.6215277777777778, 'F1': 0.6858237547892722, 'TP': 179, 'FP': 55, 'FN': 109}}               | {'Precision': 0.7599786324786324, 'Recall': 0.6000359195402298, 'F1': 0.6704606843360678, 'TP': 330, 'FP': 104, 'FN': 219} | {'TP': 145, 'FP': 90, 'FN': 145, 'Accuracy': 0.3815789473684211, 'Precision': 0.6170212765957447, 'Recall': 0.5, 'F1': 0.5523809523809523}               | {'Precision': 0.7497407104321998, 'Recall': 0.6034810531197449, 'F1': 0.6686280102439942} | all_documents   | discovery   |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.8901098901098901, 'Recall': 0.81, 'F1': 0.8481675392670157, 'TP': 162, 'FP': 20, 'FN': 38}, 'effect': {'Precision': 0.8794642857142857, 'Recall': 0.8382978723404255, 'F1': 0.8583877995642702, 'TP': 197, 'FP': 27, 'FN': 38}}                 | {'Precision': 0.8847870879120878, 'Recall': 0.8241489361702128, 'F1': 0.853277669415643, 'TP': 359, 'FP': 47, 'FN': 76}    | {'TP': 163, 'FP': 56, 'FN': 70, 'Accuracy': 0.5640138408304498, 'Precision': 0.7442922374429224, 'Recall': 0.6995708154506438, 'F1': 0.7212389380530972} | {'Precision': 0.8337671825257441, 'Recall': 0.7447089971466204, 'F1': 0.7858530004945677} | filtered_causal | coverage    |
| {'TP': 157, 'FP': 23, 'FN': 64, 'TN': 208, 'Precision': 0.8722222222222222, 'Recall': 0.7104072398190046, 'F1': 0.7830423940149626, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.8830409356725146, 'Recall': 0.7947368421052632, 'F1': 0.8365650969529086, 'TP': 151, 'FP': 20, 'FN': 39}, 'effect': {'Precision': 0.8689320388349514, 'Recall': 0.8026905829596412, 'F1': 0.8344988344988344, 'TP': 179, 'FP': 27, 'FN': 44}}   | {'Precision': 0.8759864872537331, 'Recall': 0.7987137125324522, 'F1': 0.8355319657258715, 'TP': 330, 'FP': 47, 'FN': 83}   | {'TP': 145, 'FP': 56, 'FN': 74, 'Accuracy': 0.5272727272727272, 'Precision': 0.7213930348258707, 'Recall': 0.6621004566210046, 'F1': 0.6904761904761905} | {'Precision': 0.8232005814339419, 'Recall': 0.7237404696574871, 'F1': 0.7696835167390083} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                            | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 157, 'FP': 22, 'FN': 64, 'TN': 209, 'Precision': 0.8770949720670391, 'Recall': 0.7104072398190046, 'F1': 0.785, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7718446601941747, 'Recall': 0.5888888888888889, 'F1': 0.6680672268907564, 'TP': 159, 'FP': 47, 'FN': 111}, 'effect': {'Precision': 0.7854251012145749, 'Recall': 0.6488294314381271, 'F1': 0.7106227106227105, 'TP': 194, 'FP': 53, 'FN': 105}} | {'Precision': 0.7786348807043748, 'Recall': 0.6188591601635081, 'F1': 0.6893449687567335, 'TP': 353, 'FP': 100, 'FN': 216} | {'TP': 161, 'FP': 84, 'FN': 143, 'Accuracy': 0.41494845360824745, 'Precision': 0.6571428571428571, 'Recall': 0.5296052631578947, 'F1': 0.5865209471766849} | {'Precision': 0.7709575699714236, 'Recall': 0.6196238877134691, 'F1': 0.6869553053111394} | all_documents   | coverage    |
| {'TP': 157, 'FP': 22, 'FN': 64, 'TN': 209, 'Precision': 0.8770949720670391, 'Recall': 0.7104072398190046, 'F1': 0.785, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.7602040816326531, 'Recall': 0.5708812260536399, 'F1': 0.6520787746170679, 'TP': 149, 'FP': 47, 'FN': 112}, 'effect': {'Precision': 0.7695652173913043, 'Recall': 0.6145833333333334, 'F1': 0.6833976833976835, 'TP': 177, 'FP': 53, 'FN': 111}} | {'Precision': 0.7648846495119788, 'Recall': 0.5927322796934866, 'F1': 0.6677382290073757, 'TP': 326, 'FP': 100, 'FN': 223} | {'TP': 143, 'FP': 84, 'FN': 147, 'Accuracy': 0.38235294117647056, 'Precision': 0.6299559471365639, 'Recall': 0.49310344827586206, 'F1': 0.553191489361702} | {'Precision': 0.7573118562385273, 'Recall': 0.598747655929451, 'F1': 0.6686432394563594}  | all_documents   | discovery   |
| {'TP': 157, 'FP': 22, 'FN': 64, 'TN': 209, 'Precision': 0.8770949720670391, 'Recall': 0.7104072398190046, 'F1': 0.785, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8932584269662921, 'Recall': 0.7989949748743719, 'F1': 0.843501326259947, 'TP': 159, 'FP': 19, 'FN': 40}, 'effect': {'Precision': 0.8818181818181818, 'Recall': 0.8290598290598291, 'F1': 0.8546255506607929, 'TP': 194, 'FP': 26, 'FN': 40}}    | {'Precision': 0.8875383043922369, 'Recall': 0.8140274019671006, 'F1': 0.8490634384603699, 'TP': 353, 'FP': 45, 'FN': 80}   | {'TP': 161, 'FP': 52, 'FN': 72, 'Accuracy': 0.5649122807017544, 'Precision': 0.755868544600939, 'Recall': 0.6909871244635193, 'F1': 0.7219730941704036}    | {'Precision': 0.8401672736867383, 'Recall': 0.7384739220832081, 'F1': 0.7853455108769246} | filtered_causal | coverage    |
| {'TP': 157, 'FP': 22, 'FN': 64, 'TN': 209, 'Precision': 0.8770949720670391, 'Recall': 0.7104072398190046, 'F1': 0.785, 'Accuracy': 0.8097345132743363, 'N': 452} | {'cause': {'Precision': 0.8869047619047619, 'Recall': 0.7842105263157895, 'F1': 0.8324022346368715, 'TP': 149, 'FP': 19, 'FN': 41}, 'effect': {'Precision': 0.8719211822660099, 'Recall': 0.7937219730941704, 'F1': 0.8309859154929577, 'TP': 177, 'FP': 26, 'FN': 46}}   | {'Precision': 0.8794129720853858, 'Recall': 0.7889662497049799, 'F1': 0.8316940750649147, 'TP': 326, 'FP': 45, 'FN': 87}   | {'TP': 143, 'FP': 52, 'FN': 76, 'Accuracy': 0.5276752767527675, 'Precision': 0.7333333333333333, 'Recall': 0.6529680365296804, 'F1': 0.6908212560386474}   | {'Precision': 0.8299470924952527, 'Recall': 0.7174471753512215, 'F1': 0.7691717770345207} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                               | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 156, 'FP': 22, 'FN': 65, 'TN': 209, 'Precision': 0.8764044943820225, 'Recall': 0.7058823529411765, 'F1': 0.7819548872180451, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7889447236180904, 'Recall': 0.5814814814814815, 'F1': 0.6695095948827292, 'TP': 157, 'FP': 42, 'FN': 113}, 'effect': {'Precision': 0.7966804979253111, 'Recall': 0.6421404682274248, 'F1': 0.7111111111111111, 'TP': 192, 'FP': 49, 'FN': 107}} | {'Precision': 0.7928126107717008, 'Recall': 0.6118109748544531, 'F1': 0.6903103529969201, 'TP': 349, 'FP': 91, 'FN': 220} | {'TP': 159, 'FP': 73, 'FN': 145, 'Accuracy': 0.4217506631299735, 'Precision': 0.6853448275862069, 'Recall': 0.5230263157894737, 'F1': 0.5932835820895522} | {'Precision': 0.7848539775799767, 'Recall': 0.6135732145283678, 'F1': 0.6885162741015058} | all_documents   | coverage    |
| {'TP': 156, 'FP': 22, 'FN': 65, 'TN': 209, 'Precision': 0.8764044943820225, 'Recall': 0.7058823529411765, 'F1': 0.7819548872180451, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.7777777777777778, 'Recall': 0.5632183908045977, 'F1': 0.6533333333333333, 'TP': 147, 'FP': 42, 'FN': 114}, 'effect': {'Precision': 0.78125, 'Recall': 0.6076388888888888, 'F1': 0.68359375, 'TP': 175, 'FP': 49, 'FN': 113}}                    | {'Precision': 0.7795138888888888, 'Recall': 0.5854286398467432, 'F1': 0.6684635416666667, 'TP': 322, 'FP': 91, 'FN': 227} | {'TP': 141, 'FP': 73, 'FN': 149, 'Accuracy': 0.3884297520661157, 'Precision': 0.6588785046728972, 'Recall': 0.4862068965517241, 'F1': 0.5595238095238094} | {'Precision': 0.7715989626479361, 'Recall': 0.5925059631132146, 'F1': 0.6699807461361736} | all_documents   | discovery   |
| {'TP': 156, 'FP': 22, 'FN': 65, 'TN': 209, 'Precision': 0.8764044943820225, 'Recall': 0.7058823529411765, 'F1': 0.7819548872180451, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.9181286549707602, 'Recall': 0.7929292929292929, 'F1': 0.8509485094850948, 'TP': 157, 'FP': 14, 'FN': 41}, 'effect': {'Precision': 0.897196261682243, 'Recall': 0.8240343347639485, 'F1': 0.8590604026845639, 'TP': 192, 'FP': 22, 'FN': 41}}    | {'Precision': 0.9076624583265016, 'Recall': 0.8084818138466208, 'F1': 0.8550044560848293, 'TP': 349, 'FP': 36, 'FN': 82}  | {'TP': 159, 'FP': 41, 'FN': 73, 'Accuracy': 0.5824175824175825, 'Precision': 0.795, 'Recall': 0.6853448275862069, 'F1': 0.7361111111111112}               | {'Precision': 0.8596889842361747, 'Recall': 0.7332363314580014, 'F1': 0.7910234848046619} | filtered_causal | coverage    |
| {'TP': 156, 'FP': 22, 'FN': 65, 'TN': 209, 'Precision': 0.8764044943820225, 'Recall': 0.7058823529411765, 'F1': 0.7819548872180451, 'Accuracy': 0.8075221238938053, 'N': 452} | {'cause': {'Precision': 0.9130434782608695, 'Recall': 0.7777777777777778, 'F1': 0.84, 'TP': 147, 'FP': 14, 'FN': 42}, 'effect': {'Precision': 0.8883248730964467, 'Recall': 0.7882882882882883, 'F1': 0.8353221957040573, 'TP': 175, 'FP': 22, 'FN': 47}}                 | {'Precision': 0.9006841756786581, 'Recall': 0.7830330330330331, 'F1': 0.8376610978520287, 'TP': 322, 'FP': 36, 'FN': 89}  | {'TP': 141, 'FP': 41, 'FN': 77, 'Accuracy': 0.5444015444015444, 'Precision': 0.7747252747252747, 'Recall': 0.6467889908256881, 'F1': 0.7050000000000001}  | {'Precision': 0.8506046482619851, 'Recall': 0.7119014589332991, 'F1': 0.774871995023358}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__cls+span__thr0.95.csv)

### Classification: `span_only`

#### Threshold = 0.60

| Task1                                                                                                                                                            | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.759090909090909, 'Recall': 0.6185185185185185, 'F1': 0.6816326530612244, 'TP': 167, 'FP': 53, 'FN': 103}, 'effect': {'Precision': 0.7790262172284644, 'Recall': 0.6819672131147541, 'F1': 0.7272727272727272, 'TP': 208, 'FP': 59, 'FN': 97}}   | {'Precision': 0.7690585631596867, 'Recall': 0.6502428658166363, 'F1': 0.7044526901669759, 'TP': 375, 'FP': 112, 'FN': 200} | {'TP': 175, 'FP': 100, 'FN': 136, 'Accuracy': 0.4257907542579075, 'Precision': 0.6363636363636364, 'Recall': 0.5627009646302251, 'F1': 0.5972696245733788} | {'Precision': 0.760140733174441, 'Recall': 0.647150205925726, 'F1': 0.6989280143784722}   | all_documents   | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7464114832535885, 'Recall': 0.5977011494252874, 'F1': 0.6638297872340426, 'TP': 156, 'FP': 53, 'FN': 105}, 'effect': {'Precision': 0.7581967213114754, 'Recall': 0.6423611111111112, 'F1': 0.6954887218045114, 'TP': 185, 'FP': 59, 'FN': 103}} | {'Precision': 0.752304102282532, 'Recall': 0.6200311302681993, 'F1': 0.679659254519277, 'TP': 341, 'FP': 112, 'FN': 208}   | {'TP': 150, 'FP': 100, 'FN': 140, 'Accuracy': 0.38461538461538464, 'Precision': 0.6, 'Recall': 0.5172413793103449, 'F1': 0.5555555555555556}               | {'Precision': 0.742434700760844, 'Recall': 0.6219264323029536, 'F1': 0.6767588461566314}  | all_documents   | discovery   |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8789473684210526, 'Recall': 0.8226600985221675, 'F1': 0.8498727735368956, 'TP': 167, 'FP': 23, 'FN': 36}, 'effect': {'Precision': 0.8739495798319328, 'Recall': 0.8489795918367347, 'F1': 0.8612836438923396, 'TP': 208, 'FP': 30, 'FN': 37}}   | {'Precision': 0.8764484741264926, 'Recall': 0.8358198451794511, 'F1': 0.8555782087146175, 'TP': 375, 'FP': 53, 'FN': 73}   | {'TP': 175, 'FP': 65, 'FN': 70, 'Accuracy': 0.5645161290322581, 'Precision': 0.7291666666666666, 'Recall': 0.7142857142857143, 'F1': 0.7216494845360826}   | {'Precision': 0.8268717135977197, 'Recall': 0.7595374489318273, 'F1': 0.7907631405485872} | filtered_causal | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8715083798882681, 'Recall': 0.8041237113402062, 'F1': 0.836461126005362, 'TP': 156, 'FP': 23, 'FN': 38}, 'effect': {'Precision': 0.8604651162790697, 'Recall': 0.8114035087719298, 'F1': 0.8352144469525958, 'TP': 185, 'FP': 30, 'FN': 43}}    | {'Precision': 0.8659867480836689, 'Recall': 0.807763610056068, 'F1': 0.835837786478979, 'TP': 341, 'FP': 53, 'FN': 81}     | {'TP': 150, 'FP': 65, 'FN': 74, 'Accuracy': 0.5190311418685121, 'Precision': 0.6976744186046512, 'Recall': 0.6696428571428571, 'F1': 0.683371298405467}    | {'Precision': 0.8128870555627734, 'Recall': 0.735304418176414, 'F1': 0.7714236044265025}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                            | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7614678899082569, 'Recall': 0.6148148148148148, 'F1': 0.680327868852459, 'TP': 166, 'FP': 52, 'FN': 104}, 'effect': {'Precision': 0.7765151515151515, 'Recall': 0.6788079470198676, 'F1': 0.724381625441696, 'TP': 205, 'FP': 59, 'FN': 97}}   | {'Precision': 0.7689915207117042, 'Recall': 0.6468113809173412, 'F1': 0.7023547471470775, 'TP': 371, 'FP': 111, 'FN': 201} | {'TP': 171, 'FP': 99, 'FN': 137, 'Accuracy': 0.4201474201474201, 'Precision': 0.6333333333333333, 'Recall': 0.5551948051948052, 'F1': 0.5916955017301038}  | {'Precision': 0.7591082846816791, 'Recall': 0.6435043244808211, 'F1': 0.6963706590907477} | all_documents   | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.748792270531401, 'Recall': 0.5938697318007663, 'F1': 0.6623931623931625, 'TP': 155, 'FP': 52, 'FN': 106}, 'effect': {'Precision': 0.7581967213114754, 'Recall': 0.6423611111111112, 'F1': 0.6954887218045114, 'TP': 185, 'FP': 59, 'FN': 103}} | {'Precision': 0.7534944959214382, 'Recall': 0.6181154214559388, 'F1': 0.6789409420988369, 'TP': 340, 'FP': 111, 'FN': 209} | {'TP': 149, 'FP': 99, 'FN': 141, 'Accuracy': 0.38303341902313626, 'Precision': 0.6008064516129032, 'Recall': 0.5137931034482759, 'F1': 0.5539033457249071} | {'Precision': 0.7431003158447805, 'Recall': 0.6201384374115104, 'F1': 0.6759686720729352} | all_documents   | discovery   |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8783068783068783, 'Recall': 0.8177339901477833, 'F1': 0.846938775510204, 'TP': 166, 'FP': 23, 'FN': 37}, 'effect': {'Precision': 0.8723404255319149, 'Recall': 0.8471074380165289, 'F1': 0.859538784067086, 'TP': 205, 'FP': 30, 'FN': 37}}    | {'Precision': 0.8753236519193965, 'Recall': 0.832420714082156, 'F1': 0.8532387797886449, 'TP': 371, 'FP': 53, 'FN': 74}    | {'TP': 171, 'FP': 64, 'FN': 71, 'Accuracy': 0.5588235294117647, 'Precision': 0.7276595744680852, 'Recall': 0.7066115702479339, 'F1': 0.7169811320754718}   | {'Precision': 0.8259944087958272, 'Recall': 0.7558463572201356, 'F1': 0.7884272134197262} | filtered_causal | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8707865168539326, 'Recall': 0.7989690721649485, 'F1': 0.8333333333333334, 'TP': 155, 'FP': 23, 'FN': 39}, 'effect': {'Precision': 0.8604651162790697, 'Recall': 0.8114035087719298, 'F1': 0.8352144469525958, 'TP': 185, 'FP': 30, 'FN': 43}}  | {'Precision': 0.8656258165665012, 'Recall': 0.8051862904684391, 'F1': 0.8342738901429646, 'TP': 340, 'FP': 53, 'FN': 82}   | {'TP': 149, 'FP': 64, 'FN': 75, 'Accuracy': 0.5173611111111112, 'Precision': 0.6995305164319249, 'Recall': 0.6651785714285714, 'F1': 0.6819221967963387}   | {'Precision': 0.8133854443328087, 'Recall': 0.7329572164091092, 'F1': 0.7704192717781216} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                            | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7649769585253456, 'Recall': 0.6148148148148148, 'F1': 0.6817248459958932, 'TP': 166, 'FP': 51, 'FN': 104}, 'effect': {'Precision': 0.7748091603053435, 'Recall': 0.6744186046511628, 'F1': 0.7211367673179397, 'TP': 203, 'FP': 59, 'FN': 98}} | {'Precision': 0.7698930594153446, 'Recall': 0.6446167097329888, 'F1': 0.7014308066569165, 'TP': 369, 'FP': 110, 'FN': 202} | {'TP': 170, 'FP': 98, 'FN': 137, 'Accuracy': 0.41975308641975306, 'Precision': 0.6343283582089553, 'Recall': 0.5537459283387622, 'F1': 0.591304347826087}  | {'Precision': 0.7597404725414333, 'Recall': 0.642289808467356, 'F1': 0.6959322942926884}  | all_documents   | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7524271844660194, 'Recall': 0.5938697318007663, 'F1': 0.6638115631691649, 'TP': 155, 'FP': 51, 'FN': 106}, 'effect': {'Precision': 0.757201646090535, 'Recall': 0.6388888888888888, 'F1': 0.6930320150659133, 'TP': 184, 'FP': 59, 'FN': 104}} | {'Precision': 0.7548144152782772, 'Recall': 0.6163793103448276, 'F1': 0.678421789117539, 'TP': 339, 'FP': 110, 'FN': 210}  | {'TP': 149, 'FP': 98, 'FN': 141, 'Accuracy': 0.38402061855670105, 'Precision': 0.6032388663967612, 'Recall': 0.5137931034482759, 'F1': 0.5549348230912476} | {'Precision': 0.7443510938916794, 'Recall': 0.6195597337078067, 'F1': 0.6761394468679495} | all_documents   | discovery   |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8829787234042553, 'Recall': 0.8177339901477833, 'F1': 0.8491048593350384, 'TP': 166, 'FP': 22, 'FN': 37}, 'effect': {'Precision': 0.871244635193133, 'Recall': 0.8423236514522822, 'F1': 0.8565400843881856, 'TP': 203, 'FP': 30, 'FN': 38}}   | {'Precision': 0.8771116792986942, 'Recall': 0.8300288208000327, 'F1': 0.852822471861612, 'TP': 369, 'FP': 52, 'FN': 75}    | {'TP': 170, 'FP': 63, 'FN': 71, 'Accuracy': 0.5592105263157895, 'Precision': 0.7296137339055794, 'Recall': 0.7053941908713693, 'F1': 0.7172995780590717}   | {'Precision': 0.8272418044014245, 'Recall': 0.7546432663339062, 'F1': 0.7883945927719153} | filtered_causal | coverage    |
| {'TP': 161, 'FP': 23, 'FN': 60, 'TN': 208, 'Precision': 0.875, 'Recall': 0.7285067873303167, 'F1': 0.7950617283950617, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8757062146892656, 'Recall': 0.7989690721649485, 'F1': 0.8355795148247979, 'TP': 155, 'FP': 22, 'FN': 39}, 'effect': {'Precision': 0.8598130841121495, 'Recall': 0.8070175438596491, 'F1': 0.832579185520362, 'TP': 184, 'FP': 30, 'FN': 44}}   | {'Precision': 0.8677596494007076, 'Recall': 0.8029933080122988, 'F1': 0.83407935017258, 'TP': 339, 'FP': 52, 'FN': 83}     | {'TP': 149, 'FP': 63, 'FN': 75, 'Accuracy': 0.519163763066202, 'Precision': 0.7028301886792453, 'Recall': 0.6651785714285714, 'F1': 0.68348623853211}      | {'Precision': 0.8151966126933177, 'Recall': 0.7322262222570624, 'F1': 0.7708757723665839} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7674418604651163, 'Recall': 0.6088560885608856, 'F1': 0.6790123456790124, 'TP': 165, 'FP': 50, 'FN': 106}, 'effect': {'Precision': 0.7790697674418605, 'Recall': 0.6677740863787376, 'F1': 0.7191413237924866, 'TP': 201, 'FP': 57, 'FN': 100}} | {'Precision': 0.7732558139534884, 'Recall': 0.6383150874698116, 'F1': 0.6990768347357494, 'TP': 366, 'FP': 107, 'FN': 206} | {'TP': 167, 'FP': 93, 'FN': 138, 'Accuracy': 0.41959798994974873, 'Precision': 0.6423076923076924, 'Recall': 0.5475409836065573, 'F1': 0.5911504424778762} | {'Precision': 0.7632934820506304, 'Recall': 0.6366126571762858, 'F1': 0.6941021617114727} | all_documents   | coverage    |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7549019607843137, 'Recall': 0.5900383141762452, 'F1': 0.6623655913978495, 'TP': 154, 'FP': 50, 'FN': 107}, 'effect': {'Precision': 0.7615062761506276, 'Recall': 0.6319444444444444, 'F1': 0.6907020872865274, 'TP': 182, 'FP': 57, 'FN': 106}} | {'Precision': 0.7582041184674706, 'Recall': 0.6109913793103448, 'F1': 0.6765338393421885, 'TP': 336, 'FP': 107, 'FN': 213} | {'TP': 148, 'FP': 93, 'FN': 142, 'Accuracy': 0.38642297650130547, 'Precision': 0.6141078838174274, 'Recall': 0.5103448275862069, 'F1': 0.5574387947269303} | {'Precision': 0.7488763140585362, 'Recall': 0.6151060357830135, 'F1': 0.675350613996637}  | all_documents   | discovery   |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8870967741935484, 'Recall': 0.812807881773399, 'F1': 0.8483290488431876, 'TP': 165, 'FP': 21, 'FN': 38}, 'effect': {'Precision': 0.8739130434782608, 'Recall': 0.8375, 'F1': 0.8553191489361702, 'TP': 201, 'FP': 29, 'FN': 39}}                | {'Precision': 0.8805049088359046, 'Recall': 0.8251539408866995, 'F1': 0.8518240988896789, 'TP': 366, 'FP': 50, 'FN': 77}   | {'TP': 167, 'FP': 59, 'FN': 71, 'Accuracy': 0.5622895622895623, 'Precision': 0.7389380530973452, 'Recall': 0.7016806722689075, 'F1': 0.7198275862068966}   | {'Precision': 0.8312533006079867, 'Recall': 0.7502721712026986, 'F1': 0.787910297672456}  | filtered_causal | coverage    |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.88, 'Recall': 0.7979274611398963, 'F1': 0.8369565217391304, 'TP': 154, 'FP': 21, 'FN': 39}, 'effect': {'Precision': 0.8625592417061612, 'Recall': 0.801762114537445, 'F1': 0.8310502283105022, 'TP': 182, 'FP': 29, 'FN': 45}}                  | {'Precision': 0.8712796208530806, 'Recall': 0.7998447878386706, 'F1': 0.8340033750248164, 'TP': 336, 'FP': 50, 'FN': 84}   | {'TP': 148, 'FP': 59, 'FN': 75, 'Accuracy': 0.524822695035461, 'Precision': 0.714975845410628, 'Recall': 0.6636771300448431, 'F1': 0.6883720930232559}     | {'Precision': 0.820190802051473, 'Recall': 0.7291679394453342, 'F1': 0.7714848919896214}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7663551401869159, 'Recall': 0.6051660516605166, 'F1': 0.6762886597938144, 'TP': 164, 'FP': 50, 'FN': 107}, 'effect': {'Precision': 0.77734375, 'Recall': 0.6633333333333333, 'F1': 0.7158273381294964, 'TP': 199, 'FP': 57, 'FN': 101}}         | {'Precision': 0.7718494450934579, 'Recall': 0.634249692496925, 'F1': 0.6960579989616553, 'TP': 363, 'FP': 107, 'FN': 208}  | {'TP': 165, 'FP': 92, 'FN': 139, 'Accuracy': 0.4166666666666667, 'Precision': 0.642023346303502, 'Recall': 0.5427631578947368, 'F1': 0.5882352941176471}  | {'Precision': 0.7627299104292233, 'Recall': 0.63366491694805, 'F1': 0.6921241670000314}   | all_documents   | coverage    |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7536945812807881, 'Recall': 0.5862068965517241, 'F1': 0.6594827586206896, 'TP': 153, 'FP': 50, 'FN': 108}, 'effect': {'Precision': 0.7605042016806722, 'Recall': 0.6284722222222222, 'F1': 0.6882129277566539, 'TP': 181, 'FP': 57, 'FN': 107}} | {'Precision': 0.7570993914807302, 'Recall': 0.6073395593869731, 'F1': 0.6738478431886717, 'TP': 334, 'FP': 107, 'FN': 215} | {'TP': 147, 'FP': 92, 'FN': 143, 'Accuracy': 0.38481675392670156, 'Precision': 0.6150627615062761, 'Recall': 0.506896551724138, 'F1': 0.5557655954631381} | {'Precision': 0.7488263642925723, 'Recall': 0.6127393371878666, 'F1': 0.673897548857534}  | all_documents   | discovery   |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8864864864864865, 'Recall': 0.8078817733990148, 'F1': 0.845360824742268, 'TP': 164, 'FP': 21, 'FN': 39}, 'effect': {'Precision': 0.8728070175438597, 'Recall': 0.8326359832635983, 'F1': 0.8522483940042828, 'TP': 199, 'FP': 29, 'FN': 40}}    | {'Precision': 0.8796467520151732, 'Recall': 0.8202588783313065, 'F1': 0.8488046093732754, 'TP': 363, 'FP': 50, 'FN': 79}   | {'TP': 165, 'FP': 58, 'FN': 72, 'Accuracy': 0.559322033898305, 'Precision': 0.7399103139013453, 'Recall': 0.6962025316455697, 'F1': 0.7173913043478262}   | {'Precision': 0.8312913352690763, 'Recall': 0.7468144368097883, 'F1': 0.7860917072139646} | filtered_causal | coverage    |
| {'TP': 160, 'FP': 23, 'FN': 61, 'TN': 208, 'Precision': 0.8743169398907104, 'Recall': 0.7239819004524887, 'F1': 0.7920792079207921, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.8793103448275862, 'Recall': 0.7927461139896373, 'F1': 0.8337874659400545, 'TP': 153, 'FP': 21, 'FN': 40}, 'effect': {'Precision': 0.861904761904762, 'Recall': 0.7973568281938326, 'F1': 0.8283752860411899, 'TP': 181, 'FP': 29, 'FN': 46}}    | {'Precision': 0.8706075533661741, 'Recall': 0.7950514710917349, 'F1': 0.8310813759906222, 'TP': 334, 'FP': 50, 'FN': 86}   | {'TP': 147, 'FP': 58, 'FN': 76, 'Accuracy': 0.5231316725978647, 'Precision': 0.7170731707317073, 'Recall': 0.6591928251121076, 'F1': 0.6869158878504672}  | {'Precision': 0.8206658879961971, 'Recall': 0.7260753988854437, 'F1': 0.7700254905872939} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 160, 'FP': 22, 'FN': 61, 'TN': 209, 'Precision': 0.8791208791208791, 'Recall': 0.7239819004524887, 'F1': 0.794044665012407, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7703349282296651, 'Recall': 0.5962962962962963, 'F1': 0.6722338204592903, 'TP': 161, 'FP': 48, 'FN': 109}, 'effect': {'Precision': 0.7808764940239044, 'Recall': 0.6555183946488294, 'F1': 0.7127272727272728, 'TP': 196, 'FP': 55, 'FN': 103}} | {'Precision': 0.7756057111267847, 'Recall': 0.6259073454725629, 'F1': 0.6924805465932815, 'TP': 357, 'FP': 103, 'FN': 212} | {'TP': 163, 'FP': 86, 'FN': 141, 'Accuracy': 0.41794871794871796, 'Precision': 0.6546184738955824, 'Recall': 0.5361842105263158, 'F1': 0.5895117540687161} | {'Precision': 0.7697816880477487, 'Recall': 0.6286911521504558, 'F1': 0.6920123218914682} | all_documents   | coverage    |
| {'TP': 160, 'FP': 22, 'FN': 61, 'TN': 209, 'Precision': 0.8791208791208791, 'Recall': 0.7239819004524887, 'F1': 0.794044665012407, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.7587939698492462, 'Recall': 0.578544061302682, 'F1': 0.6565217391304348, 'TP': 151, 'FP': 48, 'FN': 110}, 'effect': {'Precision': 0.7649572649572649, 'Recall': 0.6215277777777778, 'F1': 0.6858237547892722, 'TP': 179, 'FP': 55, 'FN': 109}}  | {'Precision': 0.7618756174032555, 'Recall': 0.6000359195402298, 'F1': 0.6711727469598534, 'TP': 330, 'FP': 103, 'FN': 219} | {'TP': 145, 'FP': 86, 'FN': 145, 'Accuracy': 0.38563829787234044, 'Precision': 0.6277056277056277, 'Recall': 0.5, 'F1': 0.5566218809980806}                | {'Precision': 0.7562340414099208, 'Recall': 0.6080059399975729, 'F1': 0.6739464309901136} | all_documents   | discovery   |
| {'TP': 160, 'FP': 22, 'FN': 61, 'TN': 209, 'Precision': 0.8791208791208791, 'Recall': 0.7239819004524887, 'F1': 0.794044665012407, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8895027624309392, 'Recall': 0.7970297029702971, 'F1': 0.8407310704960834, 'TP': 161, 'FP': 20, 'FN': 41}, 'effect': {'Precision': 0.875, 'Recall': 0.8235294117647058, 'F1': 0.8484848484848485, 'TP': 196, 'FP': 28, 'FN': 42}}                | {'Precision': 0.8822513812154695, 'Recall': 0.8102795573675015, 'F1': 0.844607959490466, 'TP': 357, 'FP': 48, 'FN': 83}    | {'TP': 163, 'FP': 54, 'FN': 74, 'Accuracy': 0.5601374570446735, 'Precision': 0.7511520737327189, 'Recall': 0.6877637130801688, 'F1': 0.7180616740088106}   | {'Precision': 0.8375081113563558, 'Recall': 0.7406750569667198, 'F1': 0.7855714328372279} | filtered_causal | coverage    |
| {'TP': 160, 'FP': 22, 'FN': 61, 'TN': 209, 'Precision': 0.8791208791208791, 'Recall': 0.7239819004524887, 'F1': 0.794044665012407, 'Accuracy': 0.8163716814159292, 'N': 452} | {'cause': {'Precision': 0.8830409356725146, 'Recall': 0.7823834196891192, 'F1': 0.8296703296703298, 'TP': 151, 'FP': 20, 'FN': 42}, 'effect': {'Precision': 0.8647342995169082, 'Recall': 0.788546255506608, 'F1': 0.8248847926267281, 'TP': 179, 'FP': 28, 'FN': 48}}    | {'Precision': 0.8738876175947115, 'Recall': 0.7854648375978636, 'F1': 0.827277561148529, 'TP': 330, 'FP': 48, 'FN': 90}    | {'TP': 145, 'FP': 54, 'FN': 78, 'Accuracy': 0.5234657039711191, 'Precision': 0.7286432160804021, 'Recall': 0.6502242152466368, 'F1': 0.6872037914691943}   | {'Precision': 0.8272172375986643, 'Recall': 0.719890317765663, 'F1': 0.7695086725433767}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                               | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 159, 'FP': 22, 'FN': 62, 'TN': 209, 'Precision': 0.8784530386740331, 'Recall': 0.7194570135746606, 'F1': 0.7910447761194029, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7871287128712872, 'Recall': 0.5888888888888889, 'F1': 0.6737288135593221, 'TP': 159, 'FP': 43, 'FN': 111}, 'effect': {'Precision': 0.7918367346938775, 'Recall': 0.6488294314381271, 'F1': 0.713235294117647, 'TP': 194, 'FP': 51, 'FN': 105}} | {'Precision': 0.7894827237825823, 'Recall': 0.6188591601635081, 'F1': 0.6934820538384845, 'TP': 353, 'FP': 94, 'FN': 216} | {'TP': 161, 'FP': 75, 'FN': 143, 'Accuracy': 0.42480211081794195, 'Precision': 0.6822033898305084, 'Recall': 0.5296052631578947, 'F1': 0.5962962962962962} | {'Precision': 0.7833797174290412, 'Recall': 0.6226404789653545, 'F1': 0.6936077087513945} | all_documents   | coverage    |
| {'TP': 159, 'FP': 22, 'FN': 62, 'TN': 209, 'Precision': 0.8784530386740331, 'Recall': 0.7194570135746606, 'F1': 0.7910447761194029, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.7760416666666666, 'Recall': 0.5708812260536399, 'F1': 0.6578366445916115, 'TP': 149, 'FP': 43, 'FN': 112}, 'effect': {'Precision': 0.7763157894736842, 'Recall': 0.6145833333333334, 'F1': 0.686046511627907, 'TP': 177, 'FP': 51, 'FN': 111}} | {'Precision': 0.7761787280701754, 'Recall': 0.5927322796934866, 'F1': 0.6719415781097593, 'TP': 326, 'FP': 94, 'FN': 223} | {'TP': 143, 'FP': 75, 'FN': 147, 'Accuracy': 0.3917808219178082, 'Precision': 0.6559633027522935, 'Recall': 0.49310344827586206, 'F1': 0.5629921259842519} | {'Precision': 0.7701983564988341, 'Recall': 0.6017642471813364, 'F1': 0.6753261600711381} | all_documents   | discovery   |
| {'TP': 159, 'FP': 22, 'FN': 62, 'TN': 209, 'Precision': 0.8784530386740331, 'Recall': 0.7194570135746606, 'F1': 0.7910447761194029, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.9137931034482759, 'Recall': 0.7910447761194029, 'F1': 0.848, 'TP': 159, 'FP': 15, 'FN': 42}, 'effect': {'Precision': 0.8899082568807339, 'Recall': 0.8185654008438819, 'F1': 0.8527472527472527, 'TP': 194, 'FP': 24, 'FN': 43}}               | {'Precision': 0.901850680164505, 'Recall': 0.8048050884816424, 'F1': 0.8503736263736263, 'TP': 353, 'FP': 39, 'FN': 85}   | {'TP': 161, 'FP': 43, 'FN': 75, 'Accuracy': 0.5770609318996416, 'Precision': 0.7892156862745098, 'Recall': 0.6822033898305084, 'F1': 0.7318181818181817}   | {'Precision': 0.8565064683710158, 'Recall': 0.735488497295604, 'F1': 0.7910788614370704}  | filtered_causal | coverage    |
| {'TP': 159, 'FP': 22, 'FN': 62, 'TN': 209, 'Precision': 0.8784530386740331, 'Recall': 0.7194570135746606, 'F1': 0.7910447761194029, 'Accuracy': 0.8141592920353983, 'N': 452} | {'cause': {'Precision': 0.9085365853658537, 'Recall': 0.7760416666666666, 'F1': 0.8370786516853933, 'TP': 149, 'FP': 15, 'FN': 43}, 'effect': {'Precision': 0.8805970149253731, 'Recall': 0.7831858407079646, 'F1': 0.82903981264637, 'TP': 177, 'FP': 24, 'FN': 49}}    | {'Precision': 0.8945668001456134, 'Recall': 0.7796137536873156, 'F1': 0.8330592321658816, 'TP': 326, 'FP': 39, 'FN': 92}  | {'TP': 143, 'FP': 43, 'FN': 79, 'Accuracy': 0.539622641509434, 'Precision': 0.7688172043010753, 'Recall': 0.6441441441441441, 'F1': 0.7009803921568627}    | {'Precision': 0.8472790143735739, 'Recall': 0.7144049704687068, 'F1': 0.7750281334807158} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-softmax__span_only__thr0.95.csv)

## bert-gce-freeze-softmax

### Classification: `cls+span`

#### Threshold = 0.60

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                               | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 93, 'FP': 8, 'FN': 128, 'TN': 223, 'Precision': 0.9207920792079208, 'Recall': 0.42081447963800905, 'F1': 0.577639751552795, 'Accuracy': 0.6991150442477876, 'N': 452} | {'cause': {'Precision': 0.7293233082706767, 'Recall': 0.35144927536231885, 'F1': 0.47432762836185816, 'TP': 97, 'FP': 36, 'FN': 179}, 'effect': {'Precision': 0.7988165680473372, 'Recall': 0.4470198675496689, 'F1': 0.5732484076433121, 'TP': 135, 'FP': 34, 'FN': 167}} | {'Precision': 0.764069938159007, 'Recall': 0.39923457145599384, 'F1': 0.5237880180025851, 'TP': 232, 'FP': 70, 'FN': 346} | {'TP': 109, 'FP': 79, 'FN': 205, 'Accuracy': 0.27735368956743, 'Precision': 0.5797872340425532, 'Recall': 0.3471337579617834, 'F1': 0.43426294820717126}    | {'Precision': 0.7548830838031604, 'Recall': 0.3890609363519288, 'F1': 0.5118969059208505} | all_documents   | coverage    |
| {'TP': 93, 'FP': 8, 'FN': 128, 'TN': 223, 'Precision': 0.9207920792079208, 'Recall': 0.42081447963800905, 'F1': 0.577639751552795, 'Accuracy': 0.6991150442477876, 'N': 452} | {'cause': {'Precision': 0.6949152542372882, 'Recall': 0.31417624521072796, 'F1': 0.43271767810026385, 'TP': 82, 'FP': 36, 'FN': 179}, 'effect': {'Precision': 0.7763157894736842, 'Recall': 0.4097222222222222, 'F1': 0.5363636363636364, 'TP': 118, 'FP': 34, 'FN': 170}} | {'Precision': 0.7356155218554862, 'Recall': 0.3619492337164751, 'F1': 0.4845406572319501, 'TP': 200, 'FP': 70, 'FN': 349} | {'TP': 83, 'FP': 79, 'FN': 207, 'Accuracy': 0.22493224932249323, 'Precision': 0.5123456790123457, 'Recall': 0.28620689655172415, 'F1': 0.36725663716814166} | {'Precision': 0.7229177600252509, 'Recall': 0.3563235366354028, 'F1': 0.4764790153176289} | all_documents   | discovery   |
| {'TP': 93, 'FP': 8, 'FN': 128, 'TN': 223, 'Precision': 0.9207920792079208, 'Recall': 0.42081447963800905, 'F1': 0.577639751552795, 'Accuracy': 0.6991150442477876, 'N': 452} | {'cause': {'Precision': 0.8083333333333333, 'Recall': 0.7461538461538462, 'F1': 0.776, 'TP': 97, 'FP': 23, 'FN': 33}, 'effect': {'Precision': 0.8598726114649682, 'Recall': 0.8490566037735849, 'F1': 0.8544303797468354, 'TP': 135, 'FP': 22, 'FN': 24}}                  | {'Precision': 0.8341029723991508, 'Recall': 0.7976052249637156, 'F1': 0.8152151898734177, 'TP': 232, 'FP': 45, 'FN': 57}  | {'TP': 109, 'FP': 62, 'FN': 54, 'Accuracy': 0.48444444444444446, 'Precision': 0.6374269005847953, 'Recall': 0.6687116564417178, 'F1': 0.6526946107784432}   | {'Precision': 0.7974406507306223, 'Recall': 0.6290437870144808, 'F1': 0.6818498507348854} | filtered_causal | coverage    |
| {'TP': 93, 'FP': 8, 'FN': 128, 'TN': 223, 'Precision': 0.9207920792079208, 'Recall': 0.42081447963800905, 'F1': 0.577639751552795, 'Accuracy': 0.6991150442477876, 'N': 452} | {'cause': {'Precision': 0.780952380952381, 'Recall': 0.7130434782608696, 'F1': 0.7454545454545454, 'TP': 82, 'FP': 23, 'FN': 33}, 'effect': {'Precision': 0.8428571428571429, 'Recall': 0.8137931034482758, 'F1': 0.8280701754385965, 'TP': 118, 'FP': 22, 'FN': 27}}      | {'Precision': 0.8119047619047619, 'Recall': 0.7634182908545727, 'F1': 0.786762360446571, 'TP': 200, 'FP': 45, 'FN': 60}   | {'TP': 83, 'FP': 62, 'FN': 56, 'Accuracy': 0.4129353233830846, 'Precision': 0.5724137931034483, 'Recall': 0.5971223021582733, 'F1': 0.5845070422535211}     | {'Precision': 0.7683702114053771, 'Recall': 0.5937850242169517, 'F1': 0.6496363847509623} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                               | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 92, 'FP': 8, 'FN': 129, 'TN': 223, 'Precision': 0.92, 'Recall': 0.416289592760181, 'F1': 0.573208722741433, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.7384615384615385, 'Recall': 0.3490909090909091, 'F1': 0.47407407407407415, 'TP': 96, 'FP': 34, 'FN': 179}, 'effect': {'Precision': 0.793939393939394, 'Recall': 0.43812709030100333, 'F1': 0.564655172413793, 'TP': 131, 'FP': 34, 'FN': 168}} | {'Precision': 0.7662004662004662, 'Recall': 0.3936089996959562, 'F1': 0.5193646232439336, 'TP': 227, 'FP': 68, 'FN': 347} | {'TP': 106, 'FP': 74, 'FN': 205, 'Accuracy': 0.2753246753246753, 'Precision': 0.5888888888888889, 'Recall': 0.3408360128617363, 'F1': 0.4317718940936863}  | {'Precision': 0.7583631183631184, 'Recall': 0.3835782017726245, 'F1': 0.508115080026351}  | all_documents   | coverage    |
| {'TP': 92, 'FP': 8, 'FN': 129, 'TN': 223, 'Precision': 0.92, 'Recall': 0.416289592760181, 'F1': 0.573208722741433, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.7068965517241379, 'Recall': 0.31417624521072796, 'F1': 0.4350132625994695, 'TP': 82, 'FP': 34, 'FN': 179}, 'effect': {'Precision': 0.7748344370860927, 'Recall': 0.40625, 'F1': 0.5330296127562643, 'TP': 117, 'FP': 34, 'FN': 171}}           | {'Precision': 0.7408654944051153, 'Recall': 0.360213122605364, 'F1': 0.4840214376778669, 'TP': 199, 'FP': 68, 'FN': 350}  | {'TP': 83, 'FP': 74, 'FN': 207, 'Accuracy': 0.22802197802197802, 'Precision': 0.5286624203821656, 'Recall': 0.28620689655172415, 'F1': 0.3713646532438479} | {'Precision': 0.729842638262427, 'Recall': 0.3542365373057564, 'F1': 0.47619827122104924} | all_documents   | discovery   |
| {'TP': 92, 'FP': 8, 'FN': 129, 'TN': 223, 'Precision': 0.92, 'Recall': 0.416289592760181, 'F1': 0.573208722741433, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.8135593220338984, 'Recall': 0.75, 'F1': 0.7804878048780488, 'TP': 96, 'FP': 22, 'FN': 32}, 'effect': {'Precision': 0.8562091503267973, 'Recall': 0.8451612903225807, 'F1': 0.8506493506493505, 'TP': 131, 'FP': 22, 'FN': 24}}                 | {'Precision': 0.8348842361803479, 'Recall': 0.7975806451612903, 'F1': 0.8155685777636996, 'TP': 227, 'FP': 44, 'FN': 56}  | {'TP': 106, 'FP': 58, 'FN': 53, 'Accuracy': 0.48847926267281105, 'Precision': 0.6463414634146342, 'Recall': 0.6666666666666666, 'F1': 0.6563467492260062}  | {'Precision': 0.8004085665316607, 'Recall': 0.6268456348627126, 'F1': 0.6817080165770463} | filtered_causal | coverage    |
| {'TP': 92, 'FP': 8, 'FN': 129, 'TN': 223, 'Precision': 0.92, 'Recall': 0.416289592760181, 'F1': 0.573208722741433, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.7884615384615384, 'Recall': 0.7192982456140351, 'F1': 0.7522935779816514, 'TP': 82, 'FP': 22, 'FN': 32}, 'effect': {'Precision': 0.841726618705036, 'Recall': 0.8125, 'F1': 0.8268551236749117, 'TP': 117, 'FP': 22, 'FN': 27}}                | {'Precision': 0.8150940785832872, 'Recall': 0.7658991228070176, 'F1': 0.7895743508282815, 'TP': 199, 'FP': 44, 'FN': 59}  | {'TP': 83, 'FP': 58, 'FN': 55, 'Accuracy': 0.42346938775510207, 'Precision': 0.5886524822695035, 'Recall': 0.6014492753623188, 'F1': 0.5949820788530465}   | {'Precision': 0.7745821869509303, 'Recall': 0.5945459969765058, 'F1': 0.6525883841409205} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                             | Task2                                                                                                                                                                                                                                                                       | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 91, 'FP': 8, 'FN': 130, 'TN': 223, 'Precision': 0.9191919191919192, 'Recall': 0.4117647058823529, 'F1': 0.56875, 'Accuracy': 0.6946902654867256, 'N': 452} | {'cause': {'Precision': 0.7322834645669292, 'Recall': 0.33941605839416056, 'F1': 0.46384039900249374, 'TP': 93, 'FP': 34, 'FN': 181}, 'effect': {'Precision': 0.7926829268292683, 'Recall': 0.43478260869565216, 'F1': 0.5615550755939525, 'TP': 130, 'FP': 34, 'FN': 169}} | {'Precision': 0.7624831956980987, 'Recall': 0.38709933354490633, 'F1': 0.5126977372982231, 'TP': 223, 'FP': 68, 'FN': 350} | {'TP': 104, 'FP': 73, 'FN': 206, 'Accuracy': 0.27154046997389036, 'Precision': 0.5875706214689266, 'Recall': 0.33548387096774196, 'F1': 0.4271047227926078} | {'Precision': 0.7564152454529816, 'Recall': 0.37811597013166703, 'F1': 0.502850820030277} | all_documents   | coverage    |
| {'TP': 91, 'FP': 8, 'FN': 130, 'TN': 223, 'Precision': 0.9191919191919192, 'Recall': 0.4117647058823529, 'F1': 0.56875, 'Accuracy': 0.6946902654867256, 'N': 452} | {'cause': {'Precision': 0.7017543859649122, 'Recall': 0.3065134099616858, 'F1': 0.42666666666666664, 'TP': 80, 'FP': 34, 'FN': 181}, 'effect': {'Precision': 0.7733333333333333, 'Recall': 0.4027777777777778, 'F1': 0.5296803652968036, 'TP': 116, 'FP': 34, 'FN': 172}}   | {'Precision': 0.7375438596491228, 'Recall': 0.3546455938697318, 'F1': 0.47817351598173513, 'TP': 196, 'FP': 68, 'FN': 353} | {'TP': 82, 'FP': 73, 'FN': 208, 'Accuracy': 0.22589531680440772, 'Precision': 0.5290322580645161, 'Recall': 0.2827586206896552, 'F1': 0.36853932584269666}  | {'Precision': 0.7285893456351861, 'Recall': 0.34972297348058, 'F1': 0.47182094727481055}  | all_documents   | discovery   |
| {'TP': 91, 'FP': 8, 'FN': 130, 'TN': 223, 'Precision': 0.9191919191919192, 'Recall': 0.4117647058823529, 'F1': 0.56875, 'Accuracy': 0.6946902654867256, 'N': 452} | {'cause': {'Precision': 0.808695652173913, 'Recall': 0.7380952380952381, 'F1': 0.7717842323651453, 'TP': 93, 'FP': 22, 'FN': 33}, 'effect': {'Precision': 0.8552631578947368, 'Recall': 0.8441558441558441, 'F1': 0.8496732026143792, 'TP': 130, 'FP': 22, 'FN': 24}}       | {'Precision': 0.831979405034325, 'Recall': 0.7911255411255411, 'F1': 0.8107287174897622, 'TP': 223, 'FP': 44, 'FN': 57}    | {'TP': 104, 'FP': 57, 'FN': 53, 'Accuracy': 0.48598130841121495, 'Precision': 0.6459627329192547, 'Recall': 0.6624203821656051, 'F1': 0.6540880503144654}   | {'Precision': 0.7990446857151663, 'Recall': 0.6217702097244997, 'F1': 0.6778555892680758} | filtered_causal | coverage    |
| {'TP': 91, 'FP': 8, 'FN': 130, 'TN': 223, 'Precision': 0.9191919191919192, 'Recall': 0.4117647058823529, 'F1': 0.56875, 'Accuracy': 0.6946902654867256, 'N': 452} | {'cause': {'Precision': 0.7843137254901961, 'Recall': 0.7079646017699115, 'F1': 0.7441860465116279, 'TP': 80, 'FP': 22, 'FN': 33}, 'effect': {'Precision': 0.8405797101449275, 'Recall': 0.8111888111888111, 'F1': 0.8256227758007118, 'TP': 116, 'FP': 22, 'FN': 27}}      | {'Precision': 0.8124467178175618, 'Recall': 0.7595767064793613, 'F1': 0.7849044111561698, 'TP': 196, 'FP': 44, 'FN': 60}   | {'TP': 82, 'FP': 57, 'FN': 55, 'Accuracy': 0.422680412371134, 'Precision': 0.5899280575539568, 'Recall': 0.5985401459854015, 'F1': 0.5942028985507246}      | {'Precision': 0.7738555648544793, 'Recall': 0.5899605194490385, 'F1': 0.6492857699022981} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                 | Task3                                                                                                                                                      | Total_Macro                                                                                | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 90, 'FP': 8, 'FN': 131, 'TN': 223, 'Precision': 0.9183673469387755, 'Recall': 0.4072398190045249, 'F1': 0.5642633228840125, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.728, 'Recall': 0.3333333333333333, 'F1': 0.457286432160804, 'TP': 91, 'FP': 34, 'FN': 182}, 'effect': {'Precision': 0.7901234567901234, 'Recall': 0.42953020134228187, 'F1': 0.5565217391304347, 'TP': 128, 'FP': 34, 'FN': 170}}                | {'Precision': 0.7590617283950617, 'Recall': 0.3814317673378076, 'F1': 0.5069040856456193, 'TP': 219, 'FP': 68, 'FN': 352}   | {'TP': 102, 'FP': 70, 'FN': 207, 'Accuracy': 0.2691292875989446, 'Precision': 0.5930232558139535, 'Recall': 0.3300970873786408, 'F1': 0.42411642411642414} | {'Precision': 0.7568174437159302, 'Recall': 0.3729228912403244, 'F1': 0.498427944215352}   | all_documents   | coverage    |
| {'TP': 90, 'FP': 8, 'FN': 131, 'TN': 223, 'Precision': 0.9183673469387755, 'Recall': 0.4072398190045249, 'F1': 0.5642633228840125, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.6991150442477876, 'Recall': 0.30268199233716475, 'F1': 0.42245989304812837, 'TP': 79, 'FP': 34, 'FN': 182}, 'effect': {'Precision': 0.7718120805369127, 'Recall': 0.3993055555555556, 'F1': 0.5263157894736843, 'TP': 115, 'FP': 34, 'FN': 173}} | {'Precision': 0.7354635623923502, 'Recall': 0.35099377394636017, 'F1': 0.47438784126090633, 'TP': 194, 'FP': 68, 'FN': 355} | {'TP': 81, 'FP': 70, 'FN': 209, 'Accuracy': 0.225, 'Precision': 0.5364238410596026, 'Recall': 0.2793103448275862, 'F1': 0.36734693877551017}               | {'Precision': 0.7300849167969093, 'Recall': 0.3458479792594904, 'F1': 0.46866603430680964} | all_documents   | discovery   |
| {'TP': 90, 'FP': 8, 'FN': 131, 'TN': 223, 'Precision': 0.9183673469387755, 'Recall': 0.4072398190045249, 'F1': 0.5642633228840125, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.8053097345132744, 'Recall': 0.7398373983739838, 'F1': 0.7711864406779662, 'TP': 91, 'FP': 22, 'FN': 32}, 'effect': {'Precision': 0.8533333333333334, 'Recall': 0.847682119205298, 'F1': 0.8504983388704319, 'TP': 128, 'FP': 22, 'FN': 23}}      | {'Precision': 0.8293215339233039, 'Recall': 0.7937597587896409, 'F1': 0.810842389774199, 'TP': 219, 'FP': 44, 'FN': 55}     | {'TP': 102, 'FP': 54, 'FN': 52, 'Accuracy': 0.49038461538461536, 'Precision': 0.6538461538461539, 'Recall': 0.6623376623376623, 'F1': 0.6580645161290323}  | {'Precision': 0.8005116782360777, 'Recall': 0.6211124133772761, 'F1': 0.677723409595748}   | filtered_causal | coverage    |
| {'TP': 90, 'FP': 8, 'FN': 131, 'TN': 223, 'Precision': 0.9183673469387755, 'Recall': 0.4072398190045249, 'F1': 0.5642633228840125, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.7821782178217822, 'Recall': 0.7117117117117117, 'F1': 0.7452830188679245, 'TP': 79, 'FP': 22, 'FN': 32}, 'effect': {'Precision': 0.8394160583941606, 'Recall': 0.8156028368794326, 'F1': 0.8273381294964028, 'TP': 115, 'FP': 22, 'FN': 26}}     | {'Precision': 0.8107971381079714, 'Recall': 0.7636572742955721, 'F1': 0.7863105741821637, 'TP': 194, 'FP': 44, 'FN': 58}    | {'TP': 81, 'FP': 54, 'FN': 54, 'Accuracy': 0.42857142857142855, 'Precision': 0.6, 'Recall': 0.6, 'F1': 0.6}                                                | {'Precision': 0.7763881616822489, 'Recall': 0.5902990311000323, 'F1': 0.6501912990220587}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                                | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 89, 'FP': 8, 'FN': 132, 'TN': 223, 'Precision': 0.9175257731958762, 'Recall': 0.40271493212669685, 'F1': 0.559748427672956, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.7317073170731707, 'Recall': 0.32967032967032966, 'F1': 0.45454545454545453, 'TP': 90, 'FP': 33, 'FN': 183}, 'effect': {'Precision': 0.7875, 'Recall': 0.42424242424242425, 'F1': 0.5514223194748359, 'TP': 126, 'FP': 34, 'FN': 171}}          | {'Precision': 0.7596036585365853, 'Recall': 0.37695637695637696, 'F1': 0.5029838870101452, 'TP': 216, 'FP': 67, 'FN': 354} | {'TP': 100, 'FP': 67, 'FN': 208, 'Accuracy': 0.26666666666666666, 'Precision': 0.5988023952095808, 'Recall': 0.3246753246753247, 'F1': 0.42105263157894735} | {'Precision': 0.7586439423140141, 'Recall': 0.3681155445861328, 'F1': 0.4945949820873495}  | all_documents   | coverage    |
| {'TP': 89, 'FP': 8, 'FN': 132, 'TN': 223, 'Precision': 0.9175257731958762, 'Recall': 0.40271493212669685, 'F1': 0.559748427672956, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.7027027027027027, 'Recall': 0.2988505747126437, 'F1': 0.4193548387096775, 'TP': 78, 'FP': 33, 'FN': 183}, 'effect': {'Precision': 0.7687074829931972, 'Recall': 0.3923611111111111, 'F1': 0.5195402298850574, 'TP': 113, 'FP': 34, 'FN': 175}} | {'Precision': 0.73570509284795, 'Recall': 0.3456058429118774, 'F1': 0.46944753429736746, 'TP': 191, 'FP': 67, 'FN': 358}   | {'TP': 79, 'FP': 67, 'FN': 211, 'Accuracy': 0.22128851540616246, 'Precision': 0.541095890410959, 'Recall': 0.27241379310344827, 'F1': 0.3623853211009175}   | {'Precision': 0.7314422521515951, 'Recall': 0.34024485604734084, 'F1': 0.4638604276904137} | all_documents   | discovery   |
| {'TP': 89, 'FP': 8, 'FN': 132, 'TN': 223, 'Precision': 0.9175257731958762, 'Recall': 0.40271493212669685, 'F1': 0.559748427672956, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.8108108108108109, 'Recall': 0.7377049180327869, 'F1': 0.7725321888412018, 'TP': 90, 'FP': 21, 'FN': 32}, 'effect': {'Precision': 0.8513513513513513, 'Recall': 0.8456375838926175, 'F1': 0.8484848484848485, 'TP': 126, 'FP': 22, 'FN': 23}}   | {'Precision': 0.8310810810810811, 'Recall': 0.7916712509627022, 'F1': 0.8105085186630252, 'TP': 216, 'FP': 43, 'FN': 55}   | {'TP': 100, 'FP': 51, 'FN': 52, 'Accuracy': 0.49261083743842365, 'Precision': 0.6622516556291391, 'Recall': 0.6578947368421053, 'F1': 0.6600660066006602}   | {'Precision': 0.803619503302032, 'Recall': 0.6174269733105014, 'F1': 0.6767743176455471}   | filtered_causal | coverage    |
| {'TP': 89, 'FP': 8, 'FN': 132, 'TN': 223, 'Precision': 0.9175257731958762, 'Recall': 0.40271493212669685, 'F1': 0.559748427672956, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.7878787878787878, 'Recall': 0.7090909090909091, 'F1': 0.7464114832535885, 'TP': 78, 'FP': 21, 'FN': 32}, 'effect': {'Precision': 0.837037037037037, 'Recall': 0.8071428571428572, 'F1': 0.8218181818181819, 'TP': 113, 'FP': 22, 'FN': 27}}    | {'Precision': 0.8124579124579124, 'Recall': 0.7581168831168832, 'F1': 0.7841148325358852, 'TP': 191, 'FP': 43, 'FN': 59}   | {'TP': 79, 'FP': 51, 'FN': 55, 'Accuracy': 0.42702702702702705, 'Precision': 0.6076923076923076, 'Recall': 0.5895522388059702, 'F1': 0.5984848484848485}    | {'Precision': 0.7792253311153655, 'Recall': 0.5834613513498501, 'F1': 0.6474493695645632}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 89, 'FP': 7, 'FN': 132, 'TN': 224, 'Precision': 0.9270833333333334, 'Recall': 0.40271493212669685, 'F1': 0.5615141955835962, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.7416666666666667, 'Recall': 0.326007326007326, 'F1': 0.4529262086513995, 'TP': 89, 'FP': 31, 'FN': 184}, 'effect': {'Precision': 0.7935483870967742, 'Recall': 0.4155405405405405, 'F1': 0.5454545454545454, 'TP': 123, 'FP': 32, 'FN': 173}}  | {'Precision': 0.7676075268817204, 'Recall': 0.37077393327393326, 'F1': 0.4991903770529724, 'TP': 212, 'FP': 63, 'FN': 357} | {'TP': 98, 'FP': 63, 'FN': 210, 'Accuracy': 0.2641509433962264, 'Precision': 0.6086956521739131, 'Recall': 0.3181818181818182, 'F1': 0.41791044776119407} | {'Precision': 0.7677955041296557, 'Recall': 0.3638902278608161, 'F1': 0.4928716734659209} | all_documents   | coverage    |
| {'TP': 89, 'FP': 7, 'FN': 132, 'TN': 224, 'Precision': 0.9270833333333334, 'Recall': 0.40271493212669685, 'F1': 0.5615141955835962, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.7129629629629629, 'Recall': 0.2950191570881226, 'F1': 0.4173441734417344, 'TP': 77, 'FP': 31, 'FN': 184}, 'effect': {'Precision': 0.7777777777777778, 'Recall': 0.3888888888888889, 'F1': 0.5185185185185185, 'TP': 112, 'FP': 32, 'FN': 176}} | {'Precision': 0.7453703703703703, 'Recall': 0.34195402298850575, 'F1': 0.4679313459801264, 'TP': 189, 'FP': 63, 'FN': 360} | {'TP': 78, 'FP': 63, 'FN': 212, 'Accuracy': 0.22096317280453256, 'Precision': 0.5531914893617021, 'Recall': 0.2689655172413793, 'F1': 0.3619489559164733} | {'Precision': 0.741881731021802, 'Recall': 0.337878157452194, 'F1': 0.463798165826732}    | all_documents   | discovery   |
| {'TP': 89, 'FP': 7, 'FN': 132, 'TN': 224, 'Precision': 0.9270833333333334, 'Recall': 0.40271493212669685, 'F1': 0.5615141955835962, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.8165137614678899, 'Recall': 0.7295081967213115, 'F1': 0.7705627705627706, 'TP': 89, 'FP': 20, 'FN': 33}, 'effect': {'Precision': 0.8541666666666666, 'Recall': 0.831081081081081, 'F1': 0.8424657534246575, 'TP': 123, 'FP': 21, 'FN': 25}}    | {'Precision': 0.8353402140672783, 'Recall': 0.7802946389011962, 'F1': 0.806514261993714, 'TP': 212, 'FP': 41, 'FN': 58}    | {'TP': 98, 'FP': 48, 'FN': 54, 'Accuracy': 0.49, 'Precision': 0.6712328767123288, 'Recall': 0.6447368421052632, 'F1': 0.6577181208053691}                 | {'Precision': 0.8112188080376468, 'Recall': 0.6092488043777188, 'F1': 0.6752488594608931} | filtered_causal | coverage    |
| {'TP': 89, 'FP': 7, 'FN': 132, 'TN': 224, 'Precision': 0.9270833333333334, 'Recall': 0.40271493212669685, 'F1': 0.5615141955835962, 'Accuracy': 0.6924778761061947, 'N': 452} | {'cause': {'Precision': 0.7938144329896907, 'Recall': 0.7, 'F1': 0.7439613526570047, 'TP': 77, 'FP': 20, 'FN': 33}, 'effect': {'Precision': 0.8421052631578947, 'Recall': 0.8, 'F1': 0.8205128205128205, 'TP': 112, 'FP': 21, 'FN': 28}}                                 | {'Precision': 0.8179598480737926, 'Recall': 0.75, 'F1': 0.7822370865849126, 'TP': 189, 'FP': 41, 'FN': 61}                 | {'TP': 78, 'FP': 48, 'FN': 56, 'Accuracy': 0.42857142857142855, 'Precision': 0.6190476190476191, 'Recall': 0.582089552238806, 'F1': 0.6}                  | {'Precision': 0.7880302668182484, 'Recall': 0.5782681614551676, 'F1': 0.6479170940561696} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                                 | Task3                                                                                                                                                     | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 88, 'FP': 7, 'FN': 133, 'TN': 224, 'Precision': 0.9263157894736842, 'Recall': 0.39819004524886875, 'F1': 0.5569620253164557, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.75, 'Recall': 0.3088235294117647, 'F1': 0.4375, 'TP': 84, 'FP': 28, 'FN': 188}, 'effect': {'Precision': 0.8013698630136986, 'Recall': 0.39661016949152544, 'F1': 0.5306122448979591, 'TP': 117, 'FP': 29, 'FN': 178}}                          | {'Precision': 0.7756849315068493, 'Recall': 0.35271684945164505, 'F1': 0.48405612244897955, 'TP': 201, 'FP': 57, 'FN': 366} | {'TP': 92, 'FP': 53, 'FN': 214, 'Accuracy': 0.2562674094707521, 'Precision': 0.6344827586206897, 'Recall': 0.3006535947712418, 'F1': 0.40798226164079826} | {'Precision': 0.778827826533741, 'Recall': 0.3505201631572519, 'F1': 0.48300013646874446} | all_documents   | coverage    |
| {'TP': 88, 'FP': 7, 'FN': 133, 'TN': 224, 'Precision': 0.9263157894736842, 'Recall': 0.39819004524886875, 'F1': 0.5569620253164557, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.7227722772277227, 'Recall': 0.2796934865900383, 'F1': 0.4033149171270718, 'TP': 73, 'FP': 28, 'FN': 188}, 'effect': {'Precision': 0.7867647058823529, 'Recall': 0.3715277777777778, 'F1': 0.5047169811320755, 'TP': 107, 'FP': 29, 'FN': 181}} | {'Precision': 0.7547684915550379, 'Recall': 0.32561063218390807, 'F1': 0.45401594912957366, 'TP': 180, 'FP': 57, 'FN': 369} | {'TP': 74, 'FP': 53, 'FN': 216, 'Accuracy': 0.21574344023323616, 'Precision': 0.5826771653543307, 'Recall': 0.25517241379310346, 'F1': 0.354916067146283} | {'Precision': 0.754587148794351, 'Recall': 0.3263243637419601, 'F1': 0.45529801386410407} | all_documents   | discovery   |
| {'TP': 88, 'FP': 7, 'FN': 133, 'TN': 224, 'Precision': 0.9263157894736842, 'Recall': 0.39819004524886875, 'F1': 0.5569620253164557, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.8235294117647058, 'Recall': 0.7, 'F1': 0.7567567567567567, 'TP': 84, 'FP': 18, 'FN': 36}, 'effect': {'Precision': 0.8666666666666667, 'Recall': 0.8068965517241379, 'F1': 0.8357142857142856, 'TP': 117, 'FP': 18, 'FN': 28}}                  | {'Precision': 0.8450980392156863, 'Recall': 0.7534482758620689, 'F1': 0.7962355212355212, 'TP': 201, 'FP': 36, 'FN': 64}    | {'TP': 92, 'FP': 39, 'FN': 56, 'Accuracy': 0.4919786096256685, 'Precision': 0.7022900763358778, 'Recall': 0.6216216216216216, 'F1': 0.6594982078853046}   | {'Precision': 0.8245679683417494, 'Recall': 0.5910866475775197, 'F1': 0.6708985848124271} | filtered_causal | coverage    |
| {'TP': 88, 'FP': 7, 'FN': 133, 'TN': 224, 'Precision': 0.9263157894736842, 'Recall': 0.39819004524886875, 'F1': 0.5569620253164557, 'Accuracy': 0.6902654867256637, 'N': 452} | {'cause': {'Precision': 0.8021978021978022, 'Recall': 0.6697247706422018, 'F1': 0.7300000000000001, 'TP': 73, 'FP': 18, 'FN': 36}, 'effect': {'Precision': 0.856, 'Recall': 0.7753623188405797, 'F1': 0.8136882129277566, 'TP': 107, 'FP': 18, 'FN': 31}}                | {'Precision': 0.8290989010989012, 'Recall': 0.7225435447413908, 'F1': 0.7718441064638784, 'TP': 180, 'FP': 36, 'FN': 67}    | {'TP': 74, 'FP': 39, 'FN': 58, 'Accuracy': 0.4327485380116959, 'Precision': 0.6548672566371682, 'Recall': 0.5606060606060606, 'F1': 0.6040816326530611}   | {'Precision': 0.8034273157365845, 'Recall': 0.5604465501987733, 'F1': 0.6442959214777984} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__cls+span__thr0.95.csv)

### Classification: `span_only`

#### Threshold = 0.60

| Task1                                                                                                                                                                           | Task2                                                                                                                                                                                                                                                                       | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                                | scenario        | eval_mode   |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 102, 'FP': 13, 'FN': 119, 'TN': 218, 'Precision': 0.8869565217391304, 'Recall': 0.46153846153846156, 'F1': 0.6071428571428571, 'Accuracy': 0.7079646017699115, 'N': 452} | {'cause': {'Precision': 0.6933333333333334, 'Recall': 0.37681159420289856, 'F1': 0.4882629107981222, 'TP': 104, 'FP': 46, 'FN': 172}, 'effect': {'Precision': 0.7580645161290323, 'Recall': 0.46688741721854304, 'F1': 0.5778688524590163, 'TP': 141, 'FP': 45, 'FN': 161}} | {'Precision': 0.7256989247311828, 'Recall': 0.4218495057107208, 'F1': 0.5330658816285693, 'TP': 245, 'FP': 91, 'FN': 333}  | {'TP': 113, 'FP': 95, 'FN': 201, 'Accuracy': 0.2762836185819071, 'Precision': 0.5432692307692307, 'Recall': 0.35987261146496813, 'F1': 0.4329501915708812} | {'Precision': 0.7186415590798481, 'Recall': 0.4144201929047168, 'F1': 0.5243863101141025}  | all_documents   | coverage    |
| {'TP': 102, 'FP': 13, 'FN': 119, 'TN': 218, 'Precision': 0.8869565217391304, 'Recall': 0.46153846153846156, 'F1': 0.6071428571428571, 'Accuracy': 0.7079646017699115, 'N': 452} | {'cause': {'Precision': 0.6592592592592592, 'Recall': 0.34099616858237547, 'F1': 0.4494949494949495, 'TP': 89, 'FP': 46, 'FN': 172}, 'effect': {'Precision': 0.7337278106508875, 'Recall': 0.4305555555555556, 'F1': 0.5426695842450766, 'TP': 124, 'FP': 45, 'FN': 164}}   | {'Precision': 0.6964935349550734, 'Recall': 0.3857758620689655, 'F1': 0.49608226687001306, 'TP': 213, 'FP': 91, 'FN': 336} | {'TP': 87, 'FP': 95, 'FN': 203, 'Accuracy': 0.22597402597402597, 'Precision': 0.47802197802197804, 'Recall': 0.3, 'F1': 0.36864406779661013}               | {'Precision': 0.6871573449053939, 'Recall': 0.38243810786914234, 'F1': 0.4906230639364934} | all_documents   | discovery   |
| {'TP': 102, 'FP': 13, 'FN': 119, 'TN': 218, 'Precision': 0.8869565217391304, 'Recall': 0.46153846153846156, 'F1': 0.6071428571428571, 'Accuracy': 0.7079646017699115, 'N': 452} | {'cause': {'Precision': 0.8, 'Recall': 0.7591240875912408, 'F1': 0.7790262172284643, 'TP': 104, 'FP': 26, 'FN': 33}, 'effect': {'Precision': 0.844311377245509, 'Recall': 0.844311377245509, 'F1': 0.8443113772455091, 'TP': 141, 'FP': 26, 'FN': 26}}                      | {'Precision': 0.8221556886227546, 'Recall': 0.8017177324183749, 'F1': 0.8116687972369867, 'TP': 245, 'FP': 52, 'FN': 59}   | {'TP': 113, 'FP': 68, 'FN': 56, 'Accuracy': 0.4767932489451477, 'Precision': 0.6243093922651933, 'Recall': 0.6686390532544378, 'F1': 0.6457142857142857}   | {'Precision': 0.7778072008756928, 'Recall': 0.6439650824037582, 'F1': 0.6881753133647098}  | filtered_causal | coverage    |
| {'TP': 102, 'FP': 13, 'FN': 119, 'TN': 218, 'Precision': 0.8869565217391304, 'Recall': 0.46153846153846156, 'F1': 0.6071428571428571, 'Accuracy': 0.7079646017699115, 'N': 452} | {'cause': {'Precision': 0.7739130434782608, 'Recall': 0.7295081967213115, 'F1': 0.751054852320675, 'TP': 89, 'FP': 26, 'FN': 33}, 'effect': {'Precision': 0.8266666666666667, 'Recall': 0.8104575163398693, 'F1': 0.8184818481848185, 'TP': 124, 'FP': 26, 'FN': 29}}       | {'Precision': 0.8002898550724638, 'Recall': 0.7699828565305904, 'F1': 0.7847683502527467, 'TP': 213, 'FP': 52, 'FN': 62}   | {'TP': 87, 'FP': 68, 'FN': 58, 'Accuracy': 0.4084507042253521, 'Precision': 0.5612903225806452, 'Recall': 0.6, 'F1': 0.58}                                 | {'Precision': 0.7495122331307464, 'Recall': 0.6105071060230173, 'F1': 0.6573037357985346}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.60.csv)

#### Threshold = 0.70

| Task1                                                                                                                                                                           | Task2                                                                                                                                                                                                                                                                        | Task2_macro                                                                                                                 | Task3                                                                                                                                                    | Total_Macro                                                                                 | scenario        | eval_mode   |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 101, 'FP': 13, 'FN': 120, 'TN': 218, 'Precision': 0.8859649122807017, 'Recall': 0.45701357466063347, 'F1': 0.6029850746268657, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.7006802721088435, 'Recall': 0.37454545454545457, 'F1': 0.48815165876777256, 'TP': 103, 'FP': 44, 'FN': 172}, 'effect': {'Precision': 0.7569060773480663, 'Recall': 0.45819397993311034, 'F1': 0.5708333333333333, 'TP': 137, 'FP': 44, 'FN': 162}} | {'Precision': 0.7287931747284548, 'Recall': 0.41636971723928246, 'F1': 0.5294924960505529, 'TP': 240, 'FP': 88, 'FN': 334}  | {'TP': 110, 'FP': 89, 'FN': 201, 'Accuracy': 0.275, 'Precision': 0.5527638190954773, 'Recall': 0.3536977491961415, 'F1': 0.4313725490196078}             | {'Precision': 0.7225073020348779, 'Recall': 0.40902701369868577, 'F1': 0.5212833732323422}  | all_documents   | coverage    |
| {'TP': 101, 'FP': 13, 'FN': 120, 'TN': 218, 'Precision': 0.8859649122807017, 'Recall': 0.45701357466063347, 'F1': 0.6029850746268657, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.6691729323308271, 'Recall': 0.34099616858237547, 'F1': 0.45177664974619286, 'TP': 89, 'FP': 44, 'FN': 172}, 'effect': {'Precision': 0.7365269461077845, 'Recall': 0.4270833333333333, 'F1': 0.5406593406593406, 'TP': 123, 'FP': 44, 'FN': 165}}   | {'Precision': 0.7028499392193057, 'Recall': 0.38403975095785436, 'F1': 0.49621799520276677, 'TP': 212, 'FP': 88, 'FN': 337} | {'TP': 87, 'FP': 89, 'FN': 203, 'Accuracy': 0.22955145118733508, 'Precision': 0.4943181818181818, 'Recall': 0.3, 'F1': 0.37339055793991416}              | {'Precision': 0.6943776777727297, 'Recall': 0.38035110853949594, 'F1': 0.49086454258984885} | all_documents   | discovery   |
| {'TP': 101, 'FP': 13, 'FN': 120, 'TN': 218, 'Precision': 0.8859649122807017, 'Recall': 0.45701357466063347, 'F1': 0.6029850746268657, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.8046875, 'Recall': 0.762962962962963, 'F1': 0.7832699619771862, 'TP': 103, 'FP': 25, 'FN': 32}, 'effect': {'Precision': 0.8404907975460123, 'Recall': 0.8404907975460123, 'F1': 0.8404907975460123, 'TP': 137, 'FP': 26, 'FN': 26}}                | {'Precision': 0.8225891487730062, 'Recall': 0.8017268802544877, 'F1': 0.8118803797615992, 'TP': 240, 'FP': 51, 'FN': 58}    | {'TP': 110, 'FP': 64, 'FN': 55, 'Accuracy': 0.48034934497816595, 'Precision': 0.632183908045977, 'Recall': 0.6666666666666666, 'F1': 0.6489675516224189} | {'Precision': 0.7802459896998949, 'Recall': 0.6418023738605959, 'F1': 0.6879443353369613}   | filtered_causal | coverage    |
| {'TP': 101, 'FP': 13, 'FN': 120, 'TN': 218, 'Precision': 0.8859649122807017, 'Recall': 0.45701357466063347, 'F1': 0.6029850746268657, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.7807017543859649, 'Recall': 0.7355371900826446, 'F1': 0.7574468085106383, 'TP': 89, 'FP': 25, 'FN': 32}, 'effect': {'Precision': 0.825503355704698, 'Recall': 0.8092105263157895, 'F1': 0.8172757475083058, 'TP': 123, 'FP': 26, 'FN': 29}}        | {'Precision': 0.8031025550453315, 'Recall': 0.772373858199217, 'F1': 0.7873612780094721, 'TP': 212, 'FP': 51, 'FN': 61}     | {'TP': 87, 'FP': 64, 'FN': 57, 'Accuracy': 0.4182692307692308, 'Precision': 0.5761589403973509, 'Recall': 0.6041666666666666, 'F1': 0.5898305084745762}  | {'Precision': 0.755075469241128, 'Recall': 0.6111846998421724, 'F1': 0.660058953703638}     | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.70.csv)

#### Threshold = 0.75

| Task1                                                                                                                                                                           | Task2                                                                                                                                                                                                                                                                       | Task2_macro                                                                                                                | Task3                                                                                                                                                       | Total_Macro                                                                                | scenario        | eval_mode   |
|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 100, 'FP': 12, 'FN': 121, 'TN': 219, 'Precision': 0.8928571428571429, 'Recall': 0.45248868778280543, 'F1': 0.6006006006006006, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.6993006993006993, 'Recall': 0.36496350364963503, 'F1': 0.4796163069544364, 'TP': 100, 'FP': 43, 'FN': 174}, 'effect': {'Precision': 0.7597765363128491, 'Recall': 0.45484949832775917, 'F1': 0.5690376569037657, 'TP': 136, 'FP': 43, 'FN': 163}} | {'Precision': 0.7295386178067742, 'Recall': 0.4099065009886971, 'F1': 0.5243269819291011, 'TP': 236, 'FP': 86, 'FN': 337}  | {'TP': 108, 'FP': 87, 'FN': 202, 'Accuracy': 0.27204030226700254, 'Precision': 0.5538461538461539, 'Recall': 0.34838709677419355, 'F1': 0.4277227722772277} | {'Precision': 0.7254139715033571, 'Recall': 0.40359409518189865, 'F1': 0.5175501182689765} | all_documents   | coverage    |
| {'TP': 100, 'FP': 12, 'FN': 121, 'TN': 219, 'Precision': 0.8928571428571429, 'Recall': 0.45248868778280543, 'F1': 0.6006006006006006, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.6692307692307692, 'Recall': 0.3333333333333333, 'F1': 0.44501278772378516, 'TP': 87, 'FP': 43, 'FN': 174}, 'effect': {'Precision': 0.7393939393939394, 'Recall': 0.4236111111111111, 'F1': 0.5386313465783665, 'TP': 122, 'FP': 43, 'FN': 166}}   | {'Precision': 0.7043123543123543, 'Recall': 0.3784722222222222, 'F1': 0.49182206715107585, 'TP': 209, 'FP': 86, 'FN': 340} | {'TP': 86, 'FP': 87, 'FN': 204, 'Accuracy': 0.22811671087533156, 'Precision': 0.49710982658959535, 'Recall': 0.296551724137931, 'F1': 0.3714902807775377}   | {'Precision': 0.6980931079196976, 'Recall': 0.37583754471431957, 'F1': 0.4879709828430714} | all_documents   | discovery   |
| {'TP': 100, 'FP': 12, 'FN': 121, 'TN': 219, 'Precision': 0.8928571428571429, 'Recall': 0.45248868778280543, 'F1': 0.6006006006006006, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.8, 'Recall': 0.7518796992481203, 'F1': 0.7751937984496123, 'TP': 100, 'FP': 25, 'FN': 33}, 'effect': {'Precision': 0.8395061728395061, 'Recall': 0.8395061728395061, 'F1': 0.8395061728395061, 'TP': 136, 'FP': 26, 'FN': 26}}                    | {'Precision': 0.8197530864197531, 'Recall': 0.7956929360438132, 'F1': 0.8073499856445592, 'TP': 236, 'FP': 51, 'FN': 59}   | {'TP': 108, 'FP': 63, 'FN': 55, 'Accuracy': 0.4778761061946903, 'Precision': 0.631578947368421, 'Recall': 0.6625766871165644, 'F1': 0.6467065868263473}     | {'Precision': 0.7813963922151057, 'Recall': 0.636919436981061, 'F1': 0.684885724357169}    | filtered_causal | coverage    |
| {'TP': 100, 'FP': 12, 'FN': 121, 'TN': 219, 'Precision': 0.8928571428571429, 'Recall': 0.45248868778280543, 'F1': 0.6006006006006006, 'Accuracy': 0.7057522123893806, 'N': 452} | {'cause': {'Precision': 0.7767857142857143, 'Recall': 0.725, 'F1': 0.75, 'TP': 87, 'FP': 25, 'FN': 33}, 'effect': {'Precision': 0.8243243243243243, 'Recall': 0.8079470198675497, 'F1': 0.8160535117056857, 'TP': 122, 'FP': 26, 'FN': 29}}                                 | {'Precision': 0.8005550193050193, 'Recall': 0.7664735099337748, 'F1': 0.7830267558528429, 'TP': 209, 'FP': 51, 'FN': 62}   | {'TP': 86, 'FP': 63, 'FN': 57, 'Accuracy': 0.4174757281553398, 'Precision': 0.5771812080536913, 'Recall': 0.6013986013986014, 'F1': 0.589041095890411}      | {'Precision': 0.7568644567386178, 'Recall': 0.6067869330383938, 'F1': 0.6575561507812848}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.75.csv)

#### Threshold = 0.80

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                      | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                               | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 99, 'FP': 12, 'FN': 122, 'TN': 219, 'Precision': 0.8918918918918919, 'Recall': 0.4479638009049774, 'F1': 0.5963855421686747, 'Accuracy': 0.7035398230088495, 'N': 452} | {'cause': {'Precision': 0.6950354609929078, 'Recall': 0.358974358974359, 'F1': 0.4734299516908213, 'TP': 98, 'FP': 43, 'FN': 175}, 'effect': {'Precision': 0.7570621468926554, 'Recall': 0.44966442953020136, 'F1': 0.5642105263157895, 'TP': 134, 'FP': 43, 'FN': 164}}   | {'Precision': 0.7260488039427816, 'Recall': 0.40431939425228014, 'F1': 0.5188202390033054, 'TP': 232, 'FP': 86, 'FN': 339} | {'TP': 106, 'FP': 84, 'FN': 203, 'Accuracy': 0.2697201017811705, 'Precision': 0.5578947368421052, 'Recall': 0.343042071197411, 'F1': 0.4248496993987976}   | {'Precision': 0.7252784775589262, 'Recall': 0.3984417554515562, 'F1': 0.5133518268569258} | all_documents   | coverage    |
| {'TP': 99, 'FP': 12, 'FN': 122, 'TN': 219, 'Precision': 0.8918918918918919, 'Recall': 0.4479638009049774, 'F1': 0.5963855421686747, 'Accuracy': 0.7035398230088495, 'N': 452} | {'cause': {'Precision': 0.6666666666666666, 'Recall': 0.32950191570881227, 'F1': 0.44102564102564107, 'TP': 86, 'FP': 43, 'FN': 175}, 'effect': {'Precision': 0.7378048780487805, 'Recall': 0.4201388888888889, 'F1': 0.5353982300884955, 'TP': 121, 'FP': 43, 'FN': 167}} | {'Precision': 0.7022357723577235, 'Recall': 0.3748204022988506, 'F1': 0.4882119355570683, 'TP': 207, 'FP': 86, 'FN': 342}  | {'TP': 85, 'FP': 84, 'FN': 205, 'Accuracy': 0.22727272727272727, 'Precision': 0.5029585798816568, 'Recall': 0.29310344827586204, 'F1': 0.3703703703703704} | {'Precision': 0.6990287480437574, 'Recall': 0.37196255049323, 'F1': 0.4849892826987045}   | all_documents   | discovery   |
| {'TP': 99, 'FP': 12, 'FN': 122, 'TN': 219, 'Precision': 0.8918918918918919, 'Recall': 0.4479638009049774, 'F1': 0.5963855421686747, 'Accuracy': 0.7035398230088495, 'N': 452} | {'cause': {'Precision': 0.7967479674796748, 'Recall': 0.7538461538461538, 'F1': 0.7747035573122529, 'TP': 98, 'FP': 25, 'FN': 32}, 'effect': {'Precision': 0.8375, 'Recall': 0.8427672955974843, 'F1': 0.8401253918495297, 'TP': 134, 'FP': 26, 'FN': 25}}                 | {'Precision': 0.8171239837398374, 'Recall': 0.7983067247218191, 'F1': 0.8074144745808913, 'TP': 232, 'FP': 51, 'FN': 57}   | {'TP': 106, 'FP': 60, 'FN': 54, 'Accuracy': 0.4818181818181818, 'Precision': 0.6385542168674698, 'Recall': 0.6625, 'F1': 0.6503067484662576}               | {'Precision': 0.7825233641663997, 'Recall': 0.6362568418755988, 'F1': 0.6847022550719412} | filtered_causal | coverage    |
| {'TP': 99, 'FP': 12, 'FN': 122, 'TN': 219, 'Precision': 0.8918918918918919, 'Recall': 0.4479638009049774, 'F1': 0.5963855421686747, 'Accuracy': 0.7035398230088495, 'N': 452} | {'cause': {'Precision': 0.7747747747747747, 'Recall': 0.7288135593220338, 'F1': 0.7510917030567685, 'TP': 86, 'FP': 25, 'FN': 32}, 'effect': {'Precision': 0.8231292517006803, 'Recall': 0.8120805369127517, 'F1': 0.8175675675675675, 'TP': 121, 'FP': 26, 'FN': 28}}     | {'Precision': 0.7989520132377275, 'Recall': 0.7704470481173928, 'F1': 0.784329635312168, 'TP': 207, 'FP': 51, 'FN': 60}    | {'TP': 85, 'FP': 60, 'FN': 56, 'Accuracy': 0.4228855721393035, 'Precision': 0.5862068965517241, 'Recall': 0.6028368794326241, 'F1': 0.5944055944055944}    | {'Precision': 0.7590169338937812, 'Recall': 0.6070825761516647, 'F1': 0.6583735906288124} | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.80.csv)

#### Threshold = 0.85

| Task1                                                                                                                                                                         | Task2                                                                                                                                                                                                                                                                    | Task2_macro                                                                                                               | Task3                                                                                                                                                       | Total_Macro                                                                                | scenario        | eval_mode   |
|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 98, 'FP': 12, 'FN': 123, 'TN': 219, 'Precision': 0.8909090909090909, 'Recall': 0.4434389140271493, 'F1': 0.5921450151057401, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.697841726618705, 'Recall': 0.3553113553113553, 'F1': 0.47087378640776706, 'TP': 97, 'FP': 42, 'FN': 176}, 'effect': {'Precision': 0.7542857142857143, 'Recall': 0.4444444444444444, 'F1': 0.5593220338983051, 'TP': 132, 'FP': 43, 'FN': 165}} | {'Precision': 0.7260637204522097, 'Recall': 0.3998778998778999, 'F1': 0.5150979101530361, 'TP': 229, 'FP': 85, 'FN': 341} | {'TP': 104, 'FP': 80, 'FN': 204, 'Accuracy': 0.26804123711340205, 'Precision': 0.5652173913043478, 'Recall': 0.33766233766233766, 'F1': 0.4227642276422764} | {'Precision': 0.7273967342218827, 'Recall': 0.39365971718912895, 'F1': 0.5100023843003508} | all_documents   | coverage    |
| {'TP': 98, 'FP': 12, 'FN': 123, 'TN': 219, 'Precision': 0.8909090909090909, 'Recall': 0.4434389140271493, 'F1': 0.5921450151057401, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.6692913385826772, 'Recall': 0.32567049808429116, 'F1': 0.4381443298969072, 'TP': 85, 'FP': 42, 'FN': 176}, 'effect': {'Precision': 0.7345679012345679, 'Recall': 0.4131944444444444, 'F1': 0.528888888888889, 'TP': 119, 'FP': 43, 'FN': 169}} | {'Precision': 0.7019296199086226, 'Recall': 0.3694324712643678, 'F1': 0.4835166093928981, 'TP': 204, 'FP': 85, 'FN': 345} | {'TP': 83, 'FP': 80, 'FN': 207, 'Accuracy': 0.22432432432432434, 'Precision': 0.50920245398773, 'Recall': 0.28620689655172415, 'F1': 0.3664459161147903}    | {'Precision': 0.7006803882684812, 'Recall': 0.3663594272810804, 'F1': 0.4807025135378096}  | all_documents   | discovery   |
| {'TP': 98, 'FP': 12, 'FN': 123, 'TN': 219, 'Precision': 0.8909090909090909, 'Recall': 0.4434389140271493, 'F1': 0.5921450151057401, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.8016528925619835, 'Recall': 0.751937984496124, 'F1': 0.776, 'TP': 97, 'FP': 24, 'FN': 32}, 'effect': {'Precision': 0.8354430379746836, 'Recall': 0.8407643312101911, 'F1': 0.8380952380952381, 'TP': 132, 'FP': 26, 'FN': 25}}                 | {'Precision': 0.8185479652683335, 'Recall': 0.7963511578531576, 'F1': 0.807047619047619, 'TP': 229, 'FP': 50, 'FN': 57}   | {'TP': 104, 'FP': 57, 'FN': 54, 'Accuracy': 0.48372093023255813, 'Precision': 0.6459627329192547, 'Recall': 0.6582278481012658, 'F1': 0.652037617554859}    | {'Precision': 0.785139929698893, 'Recall': 0.6326726399938575, 'F1': 0.6837434172360727}   | filtered_causal | coverage    |
| {'TP': 98, 'FP': 12, 'FN': 123, 'TN': 219, 'Precision': 0.8909090909090909, 'Recall': 0.4434389140271493, 'F1': 0.5921450151057401, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.7798165137614679, 'Recall': 0.7264957264957265, 'F1': 0.7522123893805308, 'TP': 85, 'FP': 24, 'FN': 32}, 'effect': {'Precision': 0.8206896551724138, 'Recall': 0.8040540540540541, 'F1': 0.8122866894197952, 'TP': 119, 'FP': 26, 'FN': 29}}   | {'Precision': 0.8002530844669409, 'Recall': 0.7652748902748903, 'F1': 0.782249539400163, 'TP': 204, 'FP': 50, 'FN': 61}   | {'TP': 83, 'FP': 57, 'FN': 57, 'Accuracy': 0.4213197969543147, 'Precision': 0.5928571428571429, 'Recall': 0.5928571428571429, 'F1': 0.5928571428571429}     | {'Precision': 0.7613397727443916, 'Recall': 0.6005236490530609, 'F1': 0.655750565787682}   | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.85.csv)

#### Threshold = 0.90

| Task1                                                                                                                                                                          | Task2                                                                                                                                                                                                                                                                     | Task2_macro                                                                                                                | Task3                                                                                                                                                      | Total_Macro                                                                                | scenario        | eval_mode   |
|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 97, 'FP': 11, 'FN': 124, 'TN': 220, 'Precision': 0.8981481481481481, 'Recall': 0.43891402714932126, 'F1': 0.5896656534954408, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.7037037037037037, 'Recall': 0.34798534798534797, 'F1': 0.4656862745098038, 'TP': 95, 'FP': 40, 'FN': 178}, 'effect': {'Precision': 0.7678571428571429, 'Recall': 0.4358108108108108, 'F1': 0.5560344827586207, 'TP': 129, 'FP': 39, 'FN': 167}} | {'Precision': 0.7357804232804233, 'Recall': 0.3918980793980794, 'F1': 0.5108603786342123, 'TP': 224, 'FP': 79, 'FN': 345}  | {'TP': 102, 'FP': 74, 'FN': 206, 'Accuracy': 0.2670157068062827, 'Precision': 0.5795454545454546, 'Recall': 0.33116883116883117, 'F1': 0.4214876033057851} | {'Precision': 0.7378246753246754, 'Recall': 0.38732697923874393, 'F1': 0.5073378784784793} | all_documents   | coverage    |
| {'TP': 97, 'FP': 11, 'FN': 124, 'TN': 220, 'Precision': 0.8981481481481481, 'Recall': 0.43891402714932126, 'F1': 0.5896656534954408, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.6747967479674797, 'Recall': 0.31800766283524906, 'F1': 0.4322916666666667, 'TP': 83, 'FP': 40, 'FN': 178}, 'effect': {'Precision': 0.7515923566878981, 'Recall': 0.4097222222222222, 'F1': 0.5303370786516854, 'TP': 118, 'FP': 39, 'FN': 170}} | {'Precision': 0.7131945523276889, 'Recall': 0.36386494252873564, 'F1': 0.4813143726591761, 'TP': 201, 'FP': 79, 'FN': 348} | {'TP': 82, 'FP': 74, 'FN': 208, 'Accuracy': 0.22527472527472528, 'Precision': 0.5256410256410257, 'Recall': 0.2827586206896552, 'F1': 0.36771300448430494} | {'Precision': 0.7123279087056208, 'Recall': 0.361845863455904, 'F1': 0.4795643435463072}   | all_documents   | discovery   |
| {'TP': 97, 'FP': 11, 'FN': 124, 'TN': 220, 'Precision': 0.8981481481481481, 'Recall': 0.43891402714932126, 'F1': 0.5896656534954408, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.8050847457627118, 'Recall': 0.7421875, 'F1': 0.7723577235772356, 'TP': 95, 'FP': 23, 'FN': 33}, 'effect': {'Precision': 0.8431372549019608, 'Recall': 0.832258064516129, 'F1': 0.8376623376623378, 'TP': 129, 'FP': 24, 'FN': 26}}              | {'Precision': 0.8241110003323363, 'Recall': 0.7872227822580645, 'F1': 0.8050100306197867, 'TP': 224, 'FP': 47, 'FN': 59}   | {'TP': 102, 'FP': 53, 'FN': 55, 'Accuracy': 0.4857142857142857, 'Precision': 0.6580645161290323, 'Recall': 0.6496815286624203, 'F1': 0.6538461538461539}   | {'Precision': 0.7934412215365056, 'Recall': 0.6252727793566021, 'F1': 0.6828406126537937}  | filtered_causal | coverage    |
| {'TP': 97, 'FP': 11, 'FN': 124, 'TN': 220, 'Precision': 0.8981481481481481, 'Recall': 0.43891402714932126, 'F1': 0.5896656534954408, 'Accuracy': 0.7013274336283186, 'N': 452} | {'cause': {'Precision': 0.7830188679245284, 'Recall': 0.7155172413793104, 'F1': 0.7477477477477479, 'TP': 83, 'FP': 23, 'FN': 33}, 'effect': {'Precision': 0.8309859154929577, 'Recall': 0.8027210884353742, 'F1': 0.8166089965397924, 'TP': 118, 'FP': 24, 'FN': 29}}    | {'Precision': 0.807002391708743, 'Recall': 0.7591191649073423, 'F1': 0.7821783721437701, 'TP': 201, 'FP': 47, 'FN': 62}    | {'TP': 82, 'FP': 53, 'FN': 57, 'Accuracy': 0.4270833333333333, 'Precision': 0.6074074074074074, 'Recall': 0.5899280575539568, 'F1': 0.5985401459854014}    | {'Precision': 0.7708526490880995, 'Recall': 0.5959870832035401, 'F1': 0.6567947238748707}  | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.90.csv)

#### Threshold = 0.95

| Task1                                                                                                                                                                        | Task2                                                                                                                                                                                                                                                                   | Task2_macro                                                                                                                | Task3                                                                                                                                                     | Total_Macro                                                                                 | scenario        | eval_mode   |
|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------------------|:----------------|:------------|
| {'TP': 95, 'FP': 11, 'FN': 126, 'TN': 220, 'Precision': 0.8962264150943396, 'Recall': 0.4298642533936652, 'F1': 0.581039755351682, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.712, 'Recall': 0.3272058823529412, 'F1': 0.44836272040302266, 'TP': 89, 'FP': 36, 'FN': 183}, 'effect': {'Precision': 0.7784810126582279, 'Recall': 0.41694915254237286, 'F1': 0.543046357615894, 'TP': 123, 'FP': 35, 'FN': 172}}            | {'Precision': 0.7452405063291139, 'Recall': 0.37207751744765705, 'F1': 0.4957045390094583, 'TP': 212, 'FP': 71, 'FN': 355} | {'TP': 96, 'FP': 62, 'FN': 210, 'Accuracy': 0.2608695652173913, 'Precision': 0.6075949367088608, 'Recall': 0.3137254901960784, 'F1': 0.41379310344827586} | {'Precision': 0.7496872860441047, 'Recall': 0.3718890870124669, 'F1': 0.4968457992698054}   | all_documents   | coverage    |
| {'TP': 95, 'FP': 11, 'FN': 126, 'TN': 220, 'Precision': 0.8962264150943396, 'Recall': 0.4298642533936652, 'F1': 0.581039755351682, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.6842105263157895, 'Recall': 0.2988505747126437, 'F1': 0.4160000000000001, 'TP': 78, 'FP': 36, 'FN': 183}, 'effect': {'Precision': 0.7635135135135135, 'Recall': 0.3923611111111111, 'F1': 0.518348623853211, 'TP': 113, 'FP': 35, 'FN': 175}} | {'Precision': 0.7238620199146515, 'Recall': 0.3456058429118774, 'F1': 0.4671743119266055, 'TP': 191, 'FP': 71, 'FN': 358}  | {'TP': 78, 'FP': 62, 'FN': 212, 'Accuracy': 0.2215909090909091, 'Precision': 0.5571428571428572, 'Recall': 0.2689655172413793, 'F1': 0.3627906976744186}  | {'Precision': 0.7257437640506161, 'Recall': 0.34814520451564057, 'F1': 0.47033492165090207} | all_documents   | discovery   |
| {'TP': 95, 'FP': 11, 'FN': 126, 'TN': 220, 'Precision': 0.8962264150943396, 'Recall': 0.4298642533936652, 'F1': 0.581039755351682, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.8090909090909091, 'Recall': 0.712, 'F1': 0.7574468085106384, 'TP': 89, 'FP': 21, 'FN': 36}, 'effect': {'Precision': 0.8601398601398601, 'Recall': 0.8145695364238411, 'F1': 0.8367346938775511, 'TP': 123, 'FP': 20, 'FN': 28}}               | {'Precision': 0.8346153846153845, 'Recall': 0.7632847682119206, 'F1': 0.7970907511940948, 'TP': 212, 'FP': 41, 'FN': 64}   | {'TP': 96, 'FP': 43, 'FN': 56, 'Accuracy': 0.49230769230769234, 'Precision': 0.6906474820143885, 'Recall': 0.631578947368421, 'F1': 0.6597938144329897}   | {'Precision': 0.8071630939080375, 'Recall': 0.6082426563246689, 'F1': 0.6793081069929222}   | filtered_causal | coverage    |
| {'TP': 95, 'FP': 11, 'FN': 126, 'TN': 220, 'Precision': 0.8962264150943396, 'Recall': 0.4298642533936652, 'F1': 0.581039755351682, 'Accuracy': 0.6969026548672567, 'N': 452} | {'cause': {'Precision': 0.7878787878787878, 'Recall': 0.6842105263157895, 'F1': 0.732394366197183, 'TP': 78, 'FP': 21, 'FN': 36}, 'effect': {'Precision': 0.849624060150376, 'Recall': 0.7847222222222222, 'F1': 0.8158844765342961, 'TP': 113, 'FP': 20, 'FN': 31}}    | {'Precision': 0.8187514240145819, 'Recall': 0.7344663742690059, 'F1': 0.7741394213657395, 'TP': 191, 'FP': 41, 'FN': 67}   | {'TP': 78, 'FP': 43, 'FN': 58, 'Accuracy': 0.43575418994413406, 'Precision': 0.6446280991735537, 'Recall': 0.5735294117647058, 'F1': 0.6070038910505837}  | {'Precision': 0.7865353127608251, 'Recall': 0.5792866798091256, 'F1': 0.6540610225893351}   | filtered_causal | discovery   |

[Predictions CSV](predictions\bert-gce-freeze-softmax__span_only__thr0.95.csv)
