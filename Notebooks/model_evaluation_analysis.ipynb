{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fd2146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parent directory to the system path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4069d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.analysis.evaluation_pipeline import run_full_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b102c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "        'bert-softmax': r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\expert_bert_softmax\\expert_bert_softmax_model.pt\",\n",
    "        'bert-gce': r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\expert_bert_GCE_weakSP\\expert_bert_GCE_weakSP_model.pt\",\n",
    "        'bert-gce-softmax': r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\expert_bert_GCE_Softmax_Normal\\expert_bert_GCE_Softmax_Normal_model.pt\",\n",
    "        'bert-gce-freeze-softmax': r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\expert_bert_GCE_Softmax_Freeze\\expert_bert_GCE_Softmax_Freeze_model.pt\",\n",
    "    }\n",
    "\n",
    "thresholds = [0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "\n",
    "causal_classification_modes = ['cls+span', 'span_only']\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "scenarios = ['all_documents', 'filtered_causal']\n",
    "\n",
    "eval_modes = ['coverage', 'discovery']\n",
    "\n",
    "test_data_dir = r'C:\\Users\\norouzin\\Desktop\\JointLearning\\datasets\\expert_multi_task_data\\test.csv'\n",
    "\n",
    "save_dir = r'C:\\Users\\norouzin\\Desktop\\JointLearning\\Notebooks\\predictions'\n",
    "\n",
    "report_dir = r'C:\\Users\\norouzin\\Desktop\\JointLearning\\Notebooks\\evaluation_report.md'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835287c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRO for JointCausalModel: (<class 'jointlearning.model.JointCausalModel'>, <class 'torch.nn.modules.module.Module'>, <class 'huggingface_hub.hub_mixin.PyTorchModelHubMixin'>, <class 'huggingface_hub.hub_mixin.ModelHubMixin'>, <class 'object'>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.60: 100%|██████████| 15/15 [01:02<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 194 (42.9%)\n",
      "Non-causal samples: 258 (57.1%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1242\n",
      "Total relations created: 355\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.60 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      166\n",
      "  FP          :       28\n",
      "  FN          :       55\n",
      "  TN          :      203\n",
      "  Precision   :   0.8557\n",
      "  Recall      :   0.7511\n",
      "  F1          :   0.8000\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7059\n",
      "    Recall      :   0.6316\n",
      "    F1          :   0.6667\n",
      "    TP          :      168\n",
      "    FP          :       70\n",
      "    FN          :       98\n",
      "  Label: effect\n",
      "    Precision   :   0.7936\n",
      "    Recall      :   0.7336\n",
      "    F1          :   0.7624\n",
      "    TP          :      223\n",
      "    FP          :       58\n",
      "    FN          :       81\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7497\n",
      "  Recall      :   0.6826\n",
      "  F1          :   0.7145\n",
      "  TP          :      391\n",
      "  FP          :      128\n",
      "  FN          :      179\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      192\n",
      "  FP          :      142\n",
      "  FN          :      120\n",
      "  Accuracy    :   0.4229\n",
      "  Precision   :   0.5749\n",
      "  Recall      :   0.6154\n",
      "  F1          :   0.5944\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7268\n",
      "  Recall      :   0.6830\n",
      "  F1          :   0.7030\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.60 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      166\n",
      "  FP          :       28\n",
      "  FN          :       55\n",
      "  TN          :      203\n",
      "  Precision   :   0.8557\n",
      "  Recall      :   0.7511\n",
      "  F1          :   0.8000\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.6930\n",
      "    Recall      :   0.6054\n",
      "    F1          :   0.6462\n",
      "    TP          :      158\n",
      "    FP          :       70\n",
      "    FN          :      103\n",
      "  Label: effect\n",
      "    Precision   :   0.7769\n",
      "    Recall      :   0.7014\n",
      "    F1          :   0.7372\n",
      "    TP          :      202\n",
      "    FP          :       58\n",
      "    FN          :       86\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7350\n",
      "  Recall      :   0.6534\n",
      "  F1          :   0.6917\n",
      "  TP          :      360\n",
      "  FP          :      128\n",
      "  FN          :      189\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      163\n",
      "  FP          :      142\n",
      "  FN          :      127\n",
      "  Accuracy    :   0.3773\n",
      "  Precision   :   0.5344\n",
      "  Recall      :   0.5621\n",
      "  F1          :   0.5479\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7083\n",
      "  Recall      :   0.6555\n",
      "  F1          :   0.6799\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.60 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      166\n",
      "  FP          :       28\n",
      "  FN          :       55\n",
      "  TN          :      203\n",
      "  Precision   :   0.8557\n",
      "  Recall      :   0.7511\n",
      "  F1          :   0.8000\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8195\n",
      "    Recall      :   0.8077\n",
      "    F1          :   0.8136\n",
      "    TP          :      168\n",
      "    FP          :       37\n",
      "    FN          :       40\n",
      "  Label: effect\n",
      "    Precision   :   0.9065\n",
      "    Recall      :   0.8920\n",
      "    F1          :   0.8992\n",
      "    TP          :      223\n",
      "    FP          :       23\n",
      "    FN          :       27\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8630\n",
      "  Recall      :   0.8498\n",
      "  F1          :   0.8564\n",
      "  TP          :      391\n",
      "  FP          :       60\n",
      "  FN          :       67\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      192\n",
      "  FP          :      101\n",
      "  FN          :       65\n",
      "  Accuracy    :   0.5363\n",
      "  Precision   :   0.6553\n",
      "  Recall      :   0.7471\n",
      "  F1          :   0.6982\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7913\n",
      "  Recall      :   0.7827\n",
      "  F1          :   0.7849\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.60 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      166\n",
      "  FP          :       28\n",
      "  FN          :       55\n",
      "  TN          :      203\n",
      "  Precision   :   0.8557\n",
      "  Recall      :   0.7511\n",
      "  F1          :   0.8000\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8103\n",
      "    Recall      :   0.7783\n",
      "    F1          :   0.7940\n",
      "    TP          :      158\n",
      "    FP          :       37\n",
      "    FN          :       45\n",
      "  Label: effect\n",
      "    Precision   :   0.8978\n",
      "    Recall      :   0.8632\n",
      "    F1          :   0.8802\n",
      "    TP          :      202\n",
      "    FP          :       23\n",
      "    FN          :       32\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8540\n",
      "  Recall      :   0.8208\n",
      "  F1          :   0.8371\n",
      "  TP          :      360\n",
      "  FP          :       60\n",
      "  FN          :       77\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      163\n",
      "  FP          :      101\n",
      "  FN          :       72\n",
      "  Accuracy    :   0.4851\n",
      "  Precision   :   0.6174\n",
      "  Recall      :   0.6936\n",
      "  F1          :   0.6533\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7757\n",
      "  Recall      :   0.7552\n",
      "  F1          :   0.7635\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.70: 100%|██████████| 15/15 [00:58<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 193 (42.7%)\n",
      "Non-causal samples: 259 (57.3%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1234\n",
      "Total relations created: 340\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.70 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7094\n",
      "    Recall      :   0.6241\n",
      "    F1          :   0.6640\n",
      "    TP          :      166\n",
      "    FP          :       68\n",
      "    FN          :      100\n",
      "  Label: effect\n",
      "    Precision   :   0.7971\n",
      "    Recall      :   0.7261\n",
      "    F1          :   0.7599\n",
      "    TP          :      220\n",
      "    FP          :       56\n",
      "    FN          :       83\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7533\n",
      "  Recall      :   0.6751\n",
      "  F1          :   0.7120\n",
      "  TP          :      386\n",
      "  FP          :      124\n",
      "  FN          :      183\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      187\n",
      "  FP          :      132\n",
      "  FN          :      123\n",
      "  Accuracy    :   0.4231\n",
      "  Precision   :   0.5862\n",
      "  Recall      :   0.6032\n",
      "  F1          :   0.5946\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7315\n",
      "  Recall      :   0.6750\n",
      "  F1          :   0.7012\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.70 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.6964\n",
      "    Recall      :   0.5977\n",
      "    F1          :   0.6433\n",
      "    TP          :      156\n",
      "    FP          :       68\n",
      "    FN          :      105\n",
      "  Label: effect\n",
      "    Precision   :   0.7812\n",
      "    Recall      :   0.6944\n",
      "    F1          :   0.7353\n",
      "    TP          :      200\n",
      "    FP          :       56\n",
      "    FN          :       88\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7388\n",
      "  Recall      :   0.6461\n",
      "  F1          :   0.6893\n",
      "  TP          :      356\n",
      "  FP          :      124\n",
      "  FN          :      193\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      160\n",
      "  FP          :      132\n",
      "  FN          :      130\n",
      "  Accuracy    :   0.3791\n",
      "  Precision   :   0.5479\n",
      "  Recall      :   0.5517\n",
      "  F1          :   0.5498\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7139\n",
      "  Recall      :   0.6481\n",
      "  F1          :   0.6787\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.70 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8259\n",
      "    Recall      :   0.8019\n",
      "    F1          :   0.8137\n",
      "    TP          :      166\n",
      "    FP          :       35\n",
      "    FN          :       41\n",
      "  Label: effect\n",
      "    Precision   :   0.9129\n",
      "    Recall      :   0.8871\n",
      "    F1          :   0.8998\n",
      "    TP          :      220\n",
      "    FP          :       21\n",
      "    FN          :       28\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8694\n",
      "  Recall      :   0.8445\n",
      "  F1          :   0.8568\n",
      "  TP          :      386\n",
      "  FP          :       56\n",
      "  FN          :       69\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      187\n",
      "  FP          :       91\n",
      "  FN          :       67\n",
      "  Accuracy    :   0.5420\n",
      "  Precision   :   0.6727\n",
      "  Recall      :   0.7362\n",
      "  F1          :   0.7030\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7990\n",
      "  Recall      :   0.7758\n",
      "  F1          :   0.7856\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.70 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8168\n",
      "    Recall      :   0.7723\n",
      "    F1          :   0.7939\n",
      "    TP          :      156\n",
      "    FP          :       35\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.9050\n",
      "    Recall      :   0.8584\n",
      "    F1          :   0.8811\n",
      "    TP          :      200\n",
      "    FP          :       21\n",
      "    FN          :       33\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8609\n",
      "  Recall      :   0.8153\n",
      "  F1          :   0.8375\n",
      "  TP          :      356\n",
      "  FP          :       56\n",
      "  FN          :       79\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      160\n",
      "  FP          :       91\n",
      "  FN          :       74\n",
      "  Accuracy    :   0.4923\n",
      "  Precision   :   0.6375\n",
      "  Recall      :   0.6838\n",
      "  F1          :   0.6598\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7844\n",
      "  Recall      :   0.7486\n",
      "  F1          :   0.7648\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.75: 100%|██████████| 15/15 [01:02<00:00,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 193 (42.7%)\n",
      "Non-causal samples: 259 (57.3%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1228\n",
      "Total relations created: 332\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.75 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7174\n",
      "    Recall      :   0.6226\n",
      "    F1          :   0.6667\n",
      "    TP          :      165\n",
      "    FP          :       65\n",
      "    FN          :      100\n",
      "  Label: effect\n",
      "    Precision   :   0.7993\n",
      "    Recall      :   0.7228\n",
      "    F1          :   0.7591\n",
      "    TP          :      219\n",
      "    FP          :       55\n",
      "    FN          :       84\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7583\n",
      "  Recall      :   0.6727\n",
      "  F1          :   0.7129\n",
      "  TP          :      384\n",
      "  FP          :      120\n",
      "  FN          :      184\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      185\n",
      "  FP          :      126\n",
      "  FN          :      124\n",
      "  Accuracy    :   0.4253\n",
      "  Precision   :   0.5949\n",
      "  Recall      :   0.5987\n",
      "  F1          :   0.5968\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7360\n",
      "  Recall      :   0.6727\n",
      "  F1          :   0.7023\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.75 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7059\n",
      "    Recall      :   0.5977\n",
      "    F1          :   0.6473\n",
      "    TP          :      156\n",
      "    FP          :       65\n",
      "    FN          :      105\n",
      "  Label: effect\n",
      "    Precision   :   0.7835\n",
      "    Recall      :   0.6910\n",
      "    F1          :   0.7343\n",
      "    TP          :      199\n",
      "    FP          :       55\n",
      "    FN          :       89\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7447\n",
      "  Recall      :   0.6443\n",
      "  F1          :   0.6908\n",
      "  TP          :      355\n",
      "  FP          :      120\n",
      "  FN          :      194\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      159\n",
      "  FP          :      126\n",
      "  FN          :      131\n",
      "  Accuracy    :   0.3822\n",
      "  Precision   :   0.5579\n",
      "  Recall      :   0.5483\n",
      "  F1          :   0.5530\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7192\n",
      "  Recall      :   0.6464\n",
      "  F1          :   0.6803\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.75 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8333\n",
      "    Recall      :   0.8010\n",
      "    F1          :   0.8168\n",
      "    TP          :      165\n",
      "    FP          :       33\n",
      "    FN          :       41\n",
      "  Label: effect\n",
      "    Precision   :   0.9163\n",
      "    Recall      :   0.8831\n",
      "    F1          :   0.8994\n",
      "    TP          :      219\n",
      "    FP          :       20\n",
      "    FN          :       29\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8748\n",
      "  Recall      :   0.8420\n",
      "  F1          :   0.8581\n",
      "  TP          :      384\n",
      "  FP          :       53\n",
      "  FN          :       70\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      185\n",
      "  FP          :       86\n",
      "  FN          :       68\n",
      "  Accuracy    :   0.5457\n",
      "  Precision   :   0.6827\n",
      "  Recall      :   0.7312\n",
      "  F1          :   0.7061\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8041\n",
      "  Recall      :   0.7733\n",
      "  F1          :   0.7871\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.75 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8254\n",
      "    Recall      :   0.7723\n",
      "    F1          :   0.7980\n",
      "    TP          :      156\n",
      "    FP          :       33\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.9087\n",
      "    Recall      :   0.8541\n",
      "    F1          :   0.8805\n",
      "    TP          :      199\n",
      "    FP          :       20\n",
      "    FN          :       34\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8670\n",
      "  Recall      :   0.8132\n",
      "  F1          :   0.8392\n",
      "  TP          :      355\n",
      "  FP          :       53\n",
      "  FN          :       80\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      159\n",
      "  FP          :       86\n",
      "  FN          :       75\n",
      "  Accuracy    :   0.4969\n",
      "  Precision   :   0.6490\n",
      "  Recall      :   0.6795\n",
      "  F1          :   0.6639\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7903\n",
      "  Recall      :   0.7464\n",
      "  F1          :   0.7667\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.80: 100%|██████████| 15/15 [01:03<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 193 (42.7%)\n",
      "Non-causal samples: 259 (57.3%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1221\n",
      "Total relations created: 320\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.80 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7162\n",
      "    Recall      :   0.6189\n",
      "    F1          :   0.6640\n",
      "    TP          :      164\n",
      "    FP          :       65\n",
      "    FN          :      101\n",
      "  Label: effect\n",
      "    Precision   :   0.8022\n",
      "    Recall      :   0.7119\n",
      "    F1          :   0.7544\n",
      "    TP          :      215\n",
      "    FP          :       53\n",
      "    FN          :       87\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7592\n",
      "  Recall      :   0.6654\n",
      "  F1          :   0.7092\n",
      "  TP          :      379\n",
      "  FP          :      118\n",
      "  FN          :      188\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      178\n",
      "  FP          :      122\n",
      "  FN          :      128\n",
      "  Accuracy    :   0.4159\n",
      "  Precision   :   0.5933\n",
      "  Recall      :   0.5817\n",
      "  F1          :   0.5875\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7358\n",
      "  Recall      :   0.6646\n",
      "  F1          :   0.6979\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.80 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7045\n",
      "    Recall      :   0.5939\n",
      "    F1          :   0.6445\n",
      "    TP          :      155\n",
      "    FP          :       65\n",
      "    FN          :      106\n",
      "  Label: effect\n",
      "    Precision   :   0.7871\n",
      "    Recall      :   0.6806\n",
      "    F1          :   0.7300\n",
      "    TP          :      196\n",
      "    FP          :       53\n",
      "    FN          :       92\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7458\n",
      "  Recall      :   0.6372\n",
      "  F1          :   0.6872\n",
      "  TP          :      351\n",
      "  FP          :      118\n",
      "  FN          :      198\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      155\n",
      "  FP          :      122\n",
      "  FN          :      135\n",
      "  Accuracy    :   0.3762\n",
      "  Precision   :   0.5596\n",
      "  Recall      :   0.5345\n",
      "  F1          :   0.5467\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7201\n",
      "  Recall      :   0.6394\n",
      "  F1          :   0.6770\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.80 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8325\n",
      "    Recall      :   0.7961\n",
      "    F1          :   0.8139\n",
      "    TP          :      164\n",
      "    FP          :       33\n",
      "    FN          :       42\n",
      "  Label: effect\n",
      "    Precision   :   0.9227\n",
      "    Recall      :   0.8704\n",
      "    F1          :   0.8958\n",
      "    TP          :      215\n",
      "    FP          :       18\n",
      "    FN          :       32\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8776\n",
      "  Recall      :   0.8333\n",
      "  F1          :   0.8549\n",
      "  TP          :      379\n",
      "  FP          :       51\n",
      "  FN          :       74\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      178\n",
      "  FP          :       83\n",
      "  FN          :       72\n",
      "  Accuracy    :   0.5345\n",
      "  Precision   :   0.6820\n",
      "  Recall      :   0.7120\n",
      "  F1          :   0.6967\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8048\n",
      "  Recall      :   0.7640\n",
      "  F1          :   0.7829\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.80 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      165\n",
      "  FP          :       28\n",
      "  FN          :       56\n",
      "  TN          :      203\n",
      "  Precision   :   0.8549\n",
      "  Recall      :   0.7466\n",
      "  F1          :   0.7971\n",
      "  Accuracy    :   0.8142\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8245\n",
      "    Recall      :   0.7673\n",
      "    F1          :   0.7949\n",
      "    TP          :      155\n",
      "    FP          :       33\n",
      "    FN          :       47\n",
      "  Label: effect\n",
      "    Precision   :   0.9159\n",
      "    Recall      :   0.8412\n",
      "    F1          :   0.8770\n",
      "    TP          :      196\n",
      "    FP          :       18\n",
      "    FN          :       37\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8702\n",
      "  Recall      :   0.8043\n",
      "  F1          :   0.8359\n",
      "  TP          :      351\n",
      "  FP          :       51\n",
      "  FN          :       84\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      155\n",
      "  FP          :       83\n",
      "  FN          :       79\n",
      "  Accuracy    :   0.4890\n",
      "  Precision   :   0.6513\n",
      "  Recall      :   0.6624\n",
      "  F1          :   0.6568\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7921\n",
      "  Recall      :   0.7378\n",
      "  F1          :   0.7633\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.85: 100%|██████████| 15/15 [00:57<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 191 (42.3%)\n",
      "Non-causal samples: 261 (57.7%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1205\n",
      "Total relations created: 308\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.85 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      163\n",
      "  FP          :       28\n",
      "  FN          :       58\n",
      "  TN          :      203\n",
      "  Precision   :   0.8534\n",
      "  Recall      :   0.7376\n",
      "  F1          :   0.7913\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7169\n",
      "    Recall      :   0.5925\n",
      "    F1          :   0.6488\n",
      "    TP          :      157\n",
      "    FP          :       62\n",
      "    FN          :      108\n",
      "  Label: effect\n",
      "    Precision   :   0.8038\n",
      "    Recall      :   0.6921\n",
      "    F1          :   0.7438\n",
      "    TP          :      209\n",
      "    FP          :       51\n",
      "    FN          :       93\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7604\n",
      "  Recall      :   0.6423\n",
      "  F1          :   0.6963\n",
      "  TP          :      366\n",
      "  FP          :      113\n",
      "  FN          :      201\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      171\n",
      "  FP          :      117\n",
      "  FN          :      134\n",
      "  Accuracy    :   0.4052\n",
      "  Precision   :   0.5938\n",
      "  Recall      :   0.5607\n",
      "  F1          :   0.5767\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7358\n",
      "  Recall      :   0.6468\n",
      "  F1          :   0.6881\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.85 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      163\n",
      "  FP          :       28\n",
      "  FN          :       58\n",
      "  TN          :      203\n",
      "  Precision   :   0.8534\n",
      "  Recall      :   0.7376\n",
      "  F1          :   0.7913\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7048\n",
      "    Recall      :   0.5670\n",
      "    F1          :   0.6285\n",
      "    TP          :      148\n",
      "    FP          :       62\n",
      "    FN          :      113\n",
      "  Label: effect\n",
      "    Precision   :   0.7893\n",
      "    Recall      :   0.6632\n",
      "    F1          :   0.7208\n",
      "    TP          :      191\n",
      "    FP          :       51\n",
      "    FN          :       97\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7470\n",
      "  Recall      :   0.6151\n",
      "  F1          :   0.6746\n",
      "  TP          :      339\n",
      "  FP          :      113\n",
      "  FN          :      210\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      149\n",
      "  FP          :      117\n",
      "  FN          :      141\n",
      "  Accuracy    :   0.3661\n",
      "  Precision   :   0.5602\n",
      "  Recall      :   0.5138\n",
      "  F1          :   0.5360\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7202\n",
      "  Recall      :   0.6222\n",
      "  F1          :   0.6673\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.85 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      163\n",
      "  FP          :       28\n",
      "  FN          :       58\n",
      "  TN          :      203\n",
      "  Precision   :   0.8534\n",
      "  Recall      :   0.7376\n",
      "  F1          :   0.7913\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8396\n",
      "    Recall      :   0.7734\n",
      "    F1          :   0.8051\n",
      "    TP          :      157\n",
      "    FP          :       30\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.9289\n",
      "    Recall      :   0.8566\n",
      "    F1          :   0.8913\n",
      "    TP          :      209\n",
      "    FP          :       16\n",
      "    FN          :       35\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8842\n",
      "  Recall      :   0.8150\n",
      "  F1          :   0.8482\n",
      "  TP          :      366\n",
      "  FP          :       46\n",
      "  FN          :       81\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      171\n",
      "  FP          :       78\n",
      "  FN          :       75\n",
      "  Accuracy    :   0.5278\n",
      "  Precision   :   0.6867\n",
      "  Recall      :   0.6951\n",
      "  F1          :   0.6909\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8081\n",
      "  Recall      :   0.7492\n",
      "  F1          :   0.7768\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.85 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      163\n",
      "  FP          :       28\n",
      "  FN          :       58\n",
      "  TN          :      203\n",
      "  Precision   :   0.8534\n",
      "  Recall      :   0.7376\n",
      "  F1          :   0.7913\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8315\n",
      "    Recall      :   0.7437\n",
      "    F1          :   0.7851\n",
      "    TP          :      148\n",
      "    FP          :       30\n",
      "    FN          :       51\n",
      "  Label: effect\n",
      "    Precision   :   0.9227\n",
      "    Recall      :   0.8304\n",
      "    F1          :   0.8741\n",
      "    TP          :      191\n",
      "    FP          :       16\n",
      "    FN          :       39\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8771\n",
      "  Recall      :   0.7871\n",
      "  F1          :   0.8296\n",
      "  TP          :      339\n",
      "  FP          :       46\n",
      "  FN          :       90\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      149\n",
      "  FP          :       78\n",
      "  FN          :       82\n",
      "  Accuracy    :   0.4822\n",
      "  Precision   :   0.6564\n",
      "  Recall      :   0.6450\n",
      "  F1          :   0.6507\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7956\n",
      "  Recall      :   0.7232\n",
      "  F1          :   0.7572\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.90: 100%|██████████| 15/15 [01:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 190 (42.0%)\n",
      "Non-causal samples: 262 (58.0%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1188\n",
      "Total relations created: 287\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.90 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      162\n",
      "  FP          :       28\n",
      "  FN          :       59\n",
      "  TN          :      203\n",
      "  Precision   :   0.8526\n",
      "  Recall      :   0.7330\n",
      "  F1          :   0.7883\n",
      "  Accuracy    :   0.8075\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7217\n",
      "    Recall      :   0.5840\n",
      "    F1          :   0.6456\n",
      "    TP          :      153\n",
      "    FP          :       59\n",
      "    FN          :      109\n",
      "  Label: effect\n",
      "    Precision   :   0.8072\n",
      "    Recall      :   0.6745\n",
      "    F1          :   0.7349\n",
      "    TP          :      201\n",
      "    FP          :       48\n",
      "    FN          :       97\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7645\n",
      "  Recall      :   0.6292\n",
      "  F1          :   0.6902\n",
      "  TP          :      354\n",
      "  FP          :      107\n",
      "  FN          :      206\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      164\n",
      "  FP          :      103\n",
      "  FN          :      137\n",
      "  Accuracy    :   0.4059\n",
      "  Precision   :   0.6142\n",
      "  Recall      :   0.5449\n",
      "  F1          :   0.5775\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7438\n",
      "  Recall      :   0.6357\n",
      "  F1          :   0.6853\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.90 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      162\n",
      "  FP          :       28\n",
      "  FN          :       59\n",
      "  TN          :      203\n",
      "  Precision   :   0.8526\n",
      "  Recall      :   0.7330\n",
      "  F1          :   0.7883\n",
      "  Accuracy    :   0.8075\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7136\n",
      "    Recall      :   0.5632\n",
      "    F1          :   0.6296\n",
      "    TP          :      147\n",
      "    FP          :       59\n",
      "    FN          :      114\n",
      "  Label: effect\n",
      "    Precision   :   0.7957\n",
      "    Recall      :   0.6493\n",
      "    F1          :   0.7151\n",
      "    TP          :      187\n",
      "    FP          :       48\n",
      "    FN          :      101\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7547\n",
      "  Recall      :   0.6063\n",
      "  F1          :   0.6723\n",
      "  TP          :      334\n",
      "  FP          :      107\n",
      "  FN          :      215\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      146\n",
      "  FP          :      103\n",
      "  FN          :      144\n",
      "  Accuracy    :   0.3715\n",
      "  Precision   :   0.5863\n",
      "  Recall      :   0.5034\n",
      "  F1          :   0.5417\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7312\n",
      "  Recall      :   0.6142\n",
      "  F1          :   0.6675\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.90 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      162\n",
      "  FP          :       28\n",
      "  FN          :       59\n",
      "  TN          :      203\n",
      "  Precision   :   0.8526\n",
      "  Recall      :   0.7330\n",
      "  F1          :   0.7883\n",
      "  Accuracy    :   0.8075\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8453\n",
      "    Recall      :   0.7688\n",
      "    F1          :   0.8053\n",
      "    TP          :      153\n",
      "    FP          :       28\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.9349\n",
      "    Recall      :   0.8410\n",
      "    F1          :   0.8855\n",
      "    TP          :      201\n",
      "    FP          :       14\n",
      "    FN          :       38\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8901\n",
      "  Recall      :   0.8049\n",
      "  F1          :   0.8454\n",
      "  TP          :      354\n",
      "  FP          :       42\n",
      "  FN          :       84\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      164\n",
      "  FP          :       66\n",
      "  FN          :       77\n",
      "  Accuracy    :   0.5342\n",
      "  Precision   :   0.7130\n",
      "  Recall      :   0.6805\n",
      "  F1          :   0.6964\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8186\n",
      "  Recall      :   0.7395\n",
      "  F1          :   0.7767\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.90 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      162\n",
      "  FP          :       28\n",
      "  FN          :       59\n",
      "  TN          :      203\n",
      "  Precision   :   0.8526\n",
      "  Recall      :   0.7330\n",
      "  F1          :   0.7883\n",
      "  Accuracy    :   0.8075\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8400\n",
      "    Recall      :   0.7424\n",
      "    F1          :   0.7882\n",
      "    TP          :      147\n",
      "    FP          :       28\n",
      "    FN          :       51\n",
      "  Label: effect\n",
      "    Precision   :   0.9303\n",
      "    Recall      :   0.8166\n",
      "    F1          :   0.8698\n",
      "    TP          :      187\n",
      "    FP          :       14\n",
      "    FN          :       42\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8852\n",
      "  Recall      :   0.7795\n",
      "  F1          :   0.8290\n",
      "  TP          :      334\n",
      "  FP          :       42\n",
      "  FN          :       93\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      146\n",
      "  FP          :       66\n",
      "  FN          :       84\n",
      "  Accuracy    :   0.4932\n",
      "  Precision   :   0.6887\n",
      "  Recall      :   0.6348\n",
      "  F1          :   0.6606\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8088\n",
      "  Recall      :   0.7158\n",
      "  F1          :   0.7593\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.95: 100%|██████████| 15/15 [01:04<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 178 (39.4%)\n",
      "Non-causal samples: 274 (60.6%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1147\n",
      "Total relations created: 246\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.95 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      152\n",
      "  FP          :       26\n",
      "  FN          :       69\n",
      "  TN          :      205\n",
      "  Precision   :   0.8539\n",
      "  Recall      :   0.6878\n",
      "  F1          :   0.7619\n",
      "  Accuracy    :   0.7898\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7474\n",
      "    Recall      :   0.5462\n",
      "    F1          :   0.6311\n",
      "    TP          :      142\n",
      "    FP          :       48\n",
      "    FN          :      118\n",
      "  Label: effect\n",
      "    Precision   :   0.8136\n",
      "    Recall      :   0.6007\n",
      "    F1          :   0.6911\n",
      "    TP          :      179\n",
      "    FP          :       41\n",
      "    FN          :      119\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7805\n",
      "  Recall      :   0.5734\n",
      "  F1          :   0.6611\n",
      "  TP          :      321\n",
      "  FP          :       89\n",
      "  FN          :      237\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      147\n",
      "  FP          :       82\n",
      "  FN          :      151\n",
      "  Accuracy    :   0.3868\n",
      "  Precision   :   0.6419\n",
      "  Recall      :   0.4933\n",
      "  F1          :   0.5579\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7588\n",
      "  Recall      :   0.5848\n",
      "  F1          :   0.6603\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.95 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      152\n",
      "  FP          :       26\n",
      "  FN          :       69\n",
      "  TN          :      205\n",
      "  Precision   :   0.8539\n",
      "  Recall      :   0.6878\n",
      "  F1          :   0.7619\n",
      "  Accuracy    :   0.7898\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7419\n",
      "    Recall      :   0.5287\n",
      "    F1          :   0.6174\n",
      "    TP          :      138\n",
      "    FP          :       48\n",
      "    FN          :      123\n",
      "  Label: effect\n",
      "    Precision   :   0.8029\n",
      "    Recall      :   0.5799\n",
      "    F1          :   0.6734\n",
      "    TP          :      167\n",
      "    FP          :       41\n",
      "    FN          :      121\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7724\n",
      "  Recall      :   0.5543\n",
      "  F1          :   0.6454\n",
      "  TP          :      305\n",
      "  FP          :       89\n",
      "  FN          :      244\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      132\n",
      "  FP          :       82\n",
      "  FN          :      158\n",
      "  Accuracy    :   0.3548\n",
      "  Precision   :   0.6168\n",
      "  Recall      :   0.4552\n",
      "  F1          :   0.5238\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7477\n",
      "  Recall      :   0.5658\n",
      "  F1          :   0.6437\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.95 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      152\n",
      "  FP          :       26\n",
      "  FN          :       69\n",
      "  TN          :      205\n",
      "  Precision   :   0.8539\n",
      "  Recall      :   0.6878\n",
      "  F1          :   0.7619\n",
      "  Accuracy    :   0.7898\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8820\n",
      "    Recall      :   0.7594\n",
      "    F1          :   0.8161\n",
      "    TP          :      142\n",
      "    FP          :       19\n",
      "    FN          :       45\n",
      "  Label: effect\n",
      "    Precision   :   0.9521\n",
      "    Recall      :   0.8100\n",
      "    F1          :   0.8753\n",
      "    TP          :      179\n",
      "    FP          :        9\n",
      "    FN          :       42\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9171\n",
      "  Recall      :   0.7847\n",
      "  F1          :   0.8457\n",
      "  TP          :      321\n",
      "  FP          :       28\n",
      "  FN          :       87\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      147\n",
      "  FP          :       47\n",
      "  FN          :       78\n",
      "  Accuracy    :   0.5404\n",
      "  Precision   :   0.7577\n",
      "  Recall      :   0.6533\n",
      "  F1          :   0.7017\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8429\n",
      "  Recall      :   0.7086\n",
      "  F1          :   0.7698\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.95 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      152\n",
      "  FP          :       26\n",
      "  FN          :       69\n",
      "  TN          :      205\n",
      "  Precision   :   0.8539\n",
      "  Recall      :   0.6878\n",
      "  F1          :   0.7619\n",
      "  Accuracy    :   0.7898\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8790\n",
      "    Recall      :   0.7340\n",
      "    F1          :   0.8000\n",
      "    TP          :      138\n",
      "    FP          :       19\n",
      "    FN          :       50\n",
      "  Label: effect\n",
      "    Precision   :   0.9489\n",
      "    Recall      :   0.7915\n",
      "    F1          :   0.8630\n",
      "    TP          :      167\n",
      "    FP          :        9\n",
      "    FN          :       44\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9139\n",
      "  Recall      :   0.7628\n",
      "  F1          :   0.8315\n",
      "  TP          :      305\n",
      "  FP          :       28\n",
      "  FN          :       94\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      132\n",
      "  FP          :       47\n",
      "  FN          :       85\n",
      "  Accuracy    :   0.5000\n",
      "  Precision   :   0.7374\n",
      "  Recall      :   0.6083\n",
      "  F1          :   0.6667\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8351\n",
      "  Recall      :   0.6863\n",
      "  F1          :   0.7534\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | cls+span | thr=0.99: 100%|██████████| 15/15 [01:03<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 123 (27.2%)\n",
      "Non-causal samples: 329 (72.8%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1033\n",
      "Total relations created: 129\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.99 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      103\n",
      "  FP          :       20\n",
      "  FN          :      118\n",
      "  TN          :      211\n",
      "  Precision   :   0.8374\n",
      "  Recall      :   0.4661\n",
      "  F1          :   0.5988\n",
      "  Accuracy    :   0.6947\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7917\n",
      "    Recall      :   0.3668\n",
      "    F1          :   0.5013\n",
      "    TP          :       95\n",
      "    FP          :       25\n",
      "    FN          :      164\n",
      "  Label: effect\n",
      "    Precision   :   0.7953\n",
      "    Recall      :   0.3483\n",
      "    F1          :   0.4844\n",
      "    TP          :      101\n",
      "    FP          :       26\n",
      "    FN          :      189\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7935\n",
      "  Recall      :   0.3575\n",
      "  F1          :   0.4929\n",
      "  TP          :      196\n",
      "  FP          :       51\n",
      "  FN          :      353\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :       84\n",
      "  FP          :       38\n",
      "  FN          :      205\n",
      "  Accuracy    :   0.2569\n",
      "  Precision   :   0.6885\n",
      "  Recall      :   0.2907\n",
      "  F1          :   0.4088\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7731\n",
      "  Recall      :   0.3714\n",
      "  F1          :   0.5002\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.99 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      103\n",
      "  FP          :       20\n",
      "  FN          :      118\n",
      "  TN          :      211\n",
      "  Precision   :   0.8374\n",
      "  Recall      :   0.4661\n",
      "  F1          :   0.5988\n",
      "  Accuracy    :   0.6947\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7899\n",
      "    Recall      :   0.3602\n",
      "    F1          :   0.4947\n",
      "    TP          :       94\n",
      "    FP          :       25\n",
      "    FN          :      167\n",
      "  Label: effect\n",
      "    Precision   :   0.7903\n",
      "    Recall      :   0.3403\n",
      "    F1          :   0.4757\n",
      "    TP          :       98\n",
      "    FP          :       26\n",
      "    FN          :      190\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7901\n",
      "  Recall      :   0.3502\n",
      "  F1          :   0.4852\n",
      "  TP          :      192\n",
      "  FP          :       51\n",
      "  FN          :      357\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :       81\n",
      "  FP          :       38\n",
      "  FN          :      209\n",
      "  Accuracy    :   0.2470\n",
      "  Precision   :   0.6807\n",
      "  Recall      :   0.2793\n",
      "  F1          :   0.3961\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7694\n",
      "  Recall      :   0.3652\n",
      "  F1          :   0.4934\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.99 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      103\n",
      "  FP          :       20\n",
      "  FN          :      118\n",
      "  TN          :      211\n",
      "  Precision   :   0.8374\n",
      "  Recall      :   0.4661\n",
      "  F1          :   0.5988\n",
      "  Accuracy    :   0.6947\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.9406\n",
      "    Recall      :   0.7917\n",
      "    F1          :   0.8597\n",
      "    TP          :       95\n",
      "    FP          :        6\n",
      "    FN          :       25\n",
      "  Label: effect\n",
      "    Precision   :   0.9528\n",
      "    Recall      :   0.7652\n",
      "    F1          :   0.8487\n",
      "    TP          :      101\n",
      "    FP          :        5\n",
      "    FN          :       31\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9467\n",
      "  Recall      :   0.7784\n",
      "  F1          :   0.8542\n",
      "  TP          :      196\n",
      "  FP          :       11\n",
      "  FN          :       56\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :       84\n",
      "  FP          :       18\n",
      "  FN          :       52\n",
      "  Accuracy    :   0.5455\n",
      "  Precision   :   0.8235\n",
      "  Recall      :   0.6176\n",
      "  F1          :   0.7059\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8692\n",
      "  Recall      :   0.6207\n",
      "  F1          :   0.7197\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=cls+span | thr=0.99 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      103\n",
      "  FP          :       20\n",
      "  FN          :      118\n",
      "  TN          :      211\n",
      "  Precision   :   0.8374\n",
      "  Recall      :   0.4661\n",
      "  F1          :   0.5988\n",
      "  Accuracy    :   0.6947\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.9400\n",
      "    Recall      :   0.7705\n",
      "    F1          :   0.8468\n",
      "    TP          :       94\n",
      "    FP          :        6\n",
      "    FN          :       28\n",
      "  Label: effect\n",
      "    Precision   :   0.9515\n",
      "    Recall      :   0.7538\n",
      "    F1          :   0.8412\n",
      "    TP          :       98\n",
      "    FP          :        5\n",
      "    FN          :       32\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9457\n",
      "  Recall      :   0.7622\n",
      "  F1          :   0.8440\n",
      "  TP          :      192\n",
      "  FP          :       11\n",
      "  FN          :       60\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :       81\n",
      "  FP          :       18\n",
      "  FN          :       56\n",
      "  Accuracy    :   0.5226\n",
      "  Precision   :   0.8182\n",
      "  Recall      :   0.5912\n",
      "  F1          :   0.6864\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8671\n",
      "  Recall      :   0.6065\n",
      "  F1          :   0.7098\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.60: 100%|██████████| 15/15 [01:04<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 205 (45.4%)\n",
      "Non-causal samples: 247 (54.6%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1257\n",
      "Total relations created: 371\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.60 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      172\n",
      "  FP          :       33\n",
      "  FN          :       49\n",
      "  TN          :      198\n",
      "  Precision   :   0.8390\n",
      "  Recall      :   0.7783\n",
      "  F1          :   0.8075\n",
      "  Accuracy    :   0.8186\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7016\n",
      "    Recall      :   0.6541\n",
      "    F1          :   0.6770\n",
      "    TP          :      174\n",
      "    FP          :       74\n",
      "    FN          :       92\n",
      "  Label: effect\n",
      "    Precision   :   0.7721\n",
      "    Recall      :   0.7467\n",
      "    F1          :   0.7592\n",
      "    TP          :      227\n",
      "    FP          :       67\n",
      "    FN          :       77\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7369\n",
      "  Recall      :   0.7004\n",
      "  F1          :   0.7181\n",
      "  TP          :      401\n",
      "  FP          :      141\n",
      "  FN          :      169\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      196\n",
      "  FP          :      150\n",
      "  FN          :      116\n",
      "  Accuracy    :   0.4242\n",
      "  Precision   :   0.5665\n",
      "  Recall      :   0.6282\n",
      "  F1          :   0.5957\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7141\n",
      "  Recall      :   0.7023\n",
      "  F1          :   0.7071\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.60 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      172\n",
      "  FP          :       33\n",
      "  FN          :       49\n",
      "  TN          :      198\n",
      "  Precision   :   0.8390\n",
      "  Recall      :   0.7783\n",
      "  F1          :   0.8075\n",
      "  Accuracy    :   0.8186\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.6891\n",
      "    Recall      :   0.6284\n",
      "    F1          :   0.6573\n",
      "    TP          :      164\n",
      "    FP          :       74\n",
      "    FN          :       97\n",
      "  Label: effect\n",
      "    Precision   :   0.7546\n",
      "    Recall      :   0.7153\n",
      "    F1          :   0.7344\n",
      "    TP          :      206\n",
      "    FP          :       67\n",
      "    FN          :       82\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7218\n",
      "  Recall      :   0.6718\n",
      "  F1          :   0.6959\n",
      "  TP          :      370\n",
      "  FP          :      141\n",
      "  FN          :      179\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      167\n",
      "  FP          :      150\n",
      "  FN          :      123\n",
      "  Accuracy    :   0.3795\n",
      "  Precision   :   0.5268\n",
      "  Recall      :   0.5759\n",
      "  F1          :   0.5502\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.6959\n",
      "  Recall      :   0.6753\n",
      "  F1          :   0.6845\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.60 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      172\n",
      "  FP          :       33\n",
      "  FN          :       49\n",
      "  TN          :      198\n",
      "  Precision   :   0.8390\n",
      "  Recall      :   0.7783\n",
      "  F1          :   0.8075\n",
      "  Accuracy    :   0.8186\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8246\n",
      "    Recall      :   0.8093\n",
      "    F1          :   0.8169\n",
      "    TP          :      174\n",
      "    FP          :       37\n",
      "    FN          :       41\n",
      "  Label: effect\n",
      "    Precision   :   0.9008\n",
      "    Recall      :   0.8833\n",
      "    F1          :   0.8919\n",
      "    TP          :      227\n",
      "    FP          :       25\n",
      "    FN          :       30\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8627\n",
      "  Recall      :   0.8463\n",
      "  F1          :   0.8544\n",
      "  TP          :      401\n",
      "  FP          :       62\n",
      "  FN          :       71\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      196\n",
      "  FP          :      103\n",
      "  FN          :       68\n",
      "  Accuracy    :   0.5341\n",
      "  Precision   :   0.6555\n",
      "  Recall      :   0.7424\n",
      "  F1          :   0.6963\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7858\n",
      "  Recall      :   0.7890\n",
      "  F1          :   0.7861\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.60 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      172\n",
      "  FP          :       33\n",
      "  FN          :       49\n",
      "  TN          :      198\n",
      "  Precision   :   0.8390\n",
      "  Recall      :   0.7783\n",
      "  F1          :   0.8075\n",
      "  Accuracy    :   0.8186\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8159\n",
      "    Recall      :   0.7810\n",
      "    F1          :   0.7981\n",
      "    TP          :      164\n",
      "    FP          :       37\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.8918\n",
      "    Recall      :   0.8548\n",
      "    F1          :   0.8729\n",
      "    TP          :      206\n",
      "    FP          :       25\n",
      "    FN          :       35\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8538\n",
      "  Recall      :   0.8179\n",
      "  F1          :   0.8355\n",
      "  TP          :      370\n",
      "  FP          :       62\n",
      "  FN          :       81\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      167\n",
      "  FP          :      103\n",
      "  FN          :       75\n",
      "  Accuracy    :   0.4841\n",
      "  Precision   :   0.6185\n",
      "  Recall      :   0.6901\n",
      "  F1          :   0.6523\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7705\n",
      "  Recall      :   0.7621\n",
      "  F1          :   0.7651\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.70: 100%|██████████| 15/15 [01:06<00:00,  4.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 204 (45.1%)\n",
      "Non-causal samples: 248 (54.9%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1249\n",
      "Total relations created: 356\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.70 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7049\n",
      "    Recall      :   0.6466\n",
      "    F1          :   0.6745\n",
      "    TP          :      172\n",
      "    FP          :       72\n",
      "    FN          :       94\n",
      "  Label: effect\n",
      "    Precision   :   0.7751\n",
      "    Recall      :   0.7393\n",
      "    F1          :   0.7568\n",
      "    TP          :      224\n",
      "    FP          :       65\n",
      "    FN          :       79\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7400\n",
      "  Recall      :   0.6929\n",
      "  F1          :   0.7156\n",
      "  TP          :      396\n",
      "  FP          :      137\n",
      "  FN          :      173\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      191\n",
      "  FP          :      140\n",
      "  FN          :      119\n",
      "  Accuracy    :   0.4244\n",
      "  Precision   :   0.5770\n",
      "  Recall      :   0.6161\n",
      "  F1          :   0.5959\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7184\n",
      "  Recall      :   0.6943\n",
      "  F1          :   0.7054\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.70 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.6923\n",
      "    Recall      :   0.6207\n",
      "    F1          :   0.6545\n",
      "    TP          :      162\n",
      "    FP          :       72\n",
      "    FN          :       99\n",
      "  Label: effect\n",
      "    Precision   :   0.7584\n",
      "    Recall      :   0.7083\n",
      "    F1          :   0.7325\n",
      "    TP          :      204\n",
      "    FP          :       65\n",
      "    FN          :       84\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7253\n",
      "  Recall      :   0.6645\n",
      "  F1          :   0.6935\n",
      "  TP          :      366\n",
      "  FP          :      137\n",
      "  FN          :      183\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      164\n",
      "  FP          :      140\n",
      "  FN          :      126\n",
      "  Accuracy    :   0.3814\n",
      "  Precision   :   0.5395\n",
      "  Recall      :   0.5655\n",
      "  F1          :   0.5522\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7010\n",
      "  Recall      :   0.6679\n",
      "  F1          :   0.6835\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.70 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8309\n",
      "    Recall      :   0.8037\n",
      "    F1          :   0.8171\n",
      "    TP          :      172\n",
      "    FP          :       35\n",
      "    FN          :       42\n",
      "  Label: effect\n",
      "    Precision   :   0.9069\n",
      "    Recall      :   0.8784\n",
      "    F1          :   0.8924\n",
      "    TP          :      224\n",
      "    FP          :       23\n",
      "    FN          :       31\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8689\n",
      "  Recall      :   0.8411\n",
      "  F1          :   0.8548\n",
      "  TP          :      396\n",
      "  FP          :       58\n",
      "  FN          :       73\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      191\n",
      "  FP          :       93\n",
      "  FN          :       70\n",
      "  Accuracy    :   0.5395\n",
      "  Precision   :   0.6725\n",
      "  Recall      :   0.7318\n",
      "  F1          :   0.7009\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7932\n",
      "  Recall      :   0.7822\n",
      "  F1          :   0.7868\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.70 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8223\n",
      "    Recall      :   0.7751\n",
      "    F1          :   0.7980\n",
      "    TP          :      162\n",
      "    FP          :       35\n",
      "    FN          :       47\n",
      "  Label: effect\n",
      "    Precision   :   0.8987\n",
      "    Recall      :   0.8500\n",
      "    F1          :   0.8737\n",
      "    TP          :      204\n",
      "    FP          :       23\n",
      "    FN          :       36\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8605\n",
      "  Recall      :   0.8126\n",
      "  F1          :   0.8358\n",
      "  TP          :      366\n",
      "  FP          :       58\n",
      "  FN          :       83\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      164\n",
      "  FP          :       93\n",
      "  FN          :       77\n",
      "  Accuracy    :   0.4910\n",
      "  Precision   :   0.6381\n",
      "  Recall      :   0.6805\n",
      "  F1          :   0.6586\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7790\n",
      "  Recall      :   0.7556\n",
      "  F1          :   0.7664\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.75: 100%|██████████| 15/15 [01:06<00:00,  4.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 204 (45.1%)\n",
      "Non-causal samples: 248 (54.9%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1243\n",
      "Total relations created: 348\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.75 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7125\n",
      "    Recall      :   0.6453\n",
      "    F1          :   0.6772\n",
      "    TP          :      171\n",
      "    FP          :       69\n",
      "    FN          :       94\n",
      "  Label: effect\n",
      "    Precision   :   0.7770\n",
      "    Recall      :   0.7360\n",
      "    F1          :   0.7559\n",
      "    TP          :      223\n",
      "    FP          :       64\n",
      "    FN          :       80\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7448\n",
      "  Recall      :   0.6906\n",
      "  F1          :   0.7166\n",
      "  TP          :      394\n",
      "  FP          :      133\n",
      "  FN          :      174\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      189\n",
      "  FP          :      134\n",
      "  FN          :      120\n",
      "  Accuracy    :   0.4266\n",
      "  Precision   :   0.5851\n",
      "  Recall      :   0.6117\n",
      "  F1          :   0.5981\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7227\n",
      "  Recall      :   0.6920\n",
      "  F1          :   0.7065\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.75 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7013\n",
      "    Recall      :   0.6207\n",
      "    F1          :   0.6585\n",
      "    TP          :      162\n",
      "    FP          :       69\n",
      "    FN          :       99\n",
      "  Label: effect\n",
      "    Precision   :   0.7603\n",
      "    Recall      :   0.7049\n",
      "    F1          :   0.7315\n",
      "    TP          :      203\n",
      "    FP          :       64\n",
      "    FN          :       85\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7308\n",
      "  Recall      :   0.6628\n",
      "  F1          :   0.6950\n",
      "  TP          :      365\n",
      "  FP          :      133\n",
      "  FN          :      184\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      163\n",
      "  FP          :      134\n",
      "  FN          :      127\n",
      "  Accuracy    :   0.3844\n",
      "  Precision   :   0.5488\n",
      "  Recall      :   0.5621\n",
      "  F1          :   0.5554\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7060\n",
      "  Recall      :   0.6662\n",
      "  F1          :   0.6850\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.75 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8382\n",
      "    Recall      :   0.8028\n",
      "    F1          :   0.8201\n",
      "    TP          :      171\n",
      "    FP          :       33\n",
      "    FN          :       42\n",
      "  Label: effect\n",
      "    Precision   :   0.9102\n",
      "    Recall      :   0.8745\n",
      "    F1          :   0.8920\n",
      "    TP          :      223\n",
      "    FP          :       22\n",
      "    FN          :       32\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8742\n",
      "  Recall      :   0.8387\n",
      "  F1          :   0.8561\n",
      "  TP          :      394\n",
      "  FP          :       55\n",
      "  FN          :       74\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      189\n",
      "  FP          :       88\n",
      "  FN          :       71\n",
      "  Accuracy    :   0.5431\n",
      "  Precision   :   0.6823\n",
      "  Recall      :   0.7269\n",
      "  F1          :   0.7039\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7983\n",
      "  Recall      :   0.7798\n",
      "  F1          :   0.7882\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.75 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8308\n",
      "    Recall      :   0.7751\n",
      "    F1          :   0.8020\n",
      "    TP          :      162\n",
      "    FP          :       33\n",
      "    FN          :       47\n",
      "  Label: effect\n",
      "    Precision   :   0.9022\n",
      "    Recall      :   0.8458\n",
      "    F1          :   0.8731\n",
      "    TP          :      203\n",
      "    FP          :       22\n",
      "    FN          :       37\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8665\n",
      "  Recall      :   0.8105\n",
      "  F1          :   0.8375\n",
      "  TP          :      365\n",
      "  FP          :       55\n",
      "  FN          :       84\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      163\n",
      "  FP          :       88\n",
      "  FN          :       78\n",
      "  Accuracy    :   0.4954\n",
      "  Precision   :   0.6494\n",
      "  Recall      :   0.6763\n",
      "  F1          :   0.6626\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7847\n",
      "  Recall      :   0.7535\n",
      "  F1          :   0.7683\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.80: 100%|██████████| 15/15 [01:07<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 204 (45.1%)\n",
      "Non-causal samples: 248 (54.9%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1236\n",
      "Total relations created: 336\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.80 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7113\n",
      "    Recall      :   0.6415\n",
      "    F1          :   0.6746\n",
      "    TP          :      170\n",
      "    FP          :       69\n",
      "    FN          :       95\n",
      "  Label: effect\n",
      "    Precision   :   0.7794\n",
      "    Recall      :   0.7252\n",
      "    F1          :   0.7513\n",
      "    TP          :      219\n",
      "    FP          :       62\n",
      "    FN          :       83\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7453\n",
      "  Recall      :   0.6833\n",
      "  F1          :   0.7129\n",
      "  TP          :      389\n",
      "  FP          :      131\n",
      "  FN          :      178\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      182\n",
      "  FP          :      130\n",
      "  FN          :      124\n",
      "  Accuracy    :   0.4174\n",
      "  Precision   :   0.5833\n",
      "  Recall      :   0.5948\n",
      "  F1          :   0.5890\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7223\n",
      "  Recall      :   0.6840\n",
      "  F1          :   0.7022\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.80 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7000\n",
      "    Recall      :   0.6169\n",
      "    F1          :   0.6558\n",
      "    TP          :      161\n",
      "    FP          :       69\n",
      "    FN          :      100\n",
      "  Label: effect\n",
      "    Precision   :   0.7634\n",
      "    Recall      :   0.6944\n",
      "    F1          :   0.7273\n",
      "    TP          :      200\n",
      "    FP          :       62\n",
      "    FN          :       88\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7317\n",
      "  Recall      :   0.6557\n",
      "  F1          :   0.6915\n",
      "  TP          :      361\n",
      "  FP          :      131\n",
      "  FN          :      188\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      159\n",
      "  FP          :      130\n",
      "  FN          :      131\n",
      "  Accuracy    :   0.3786\n",
      "  Precision   :   0.5502\n",
      "  Recall      :   0.5483\n",
      "  F1          :   0.5492\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7067\n",
      "  Recall      :   0.6592\n",
      "  F1          :   0.6818\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.80 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8374\n",
      "    Recall      :   0.7981\n",
      "    F1          :   0.8173\n",
      "    TP          :      170\n",
      "    FP          :       33\n",
      "    FN          :       43\n",
      "  Label: effect\n",
      "    Precision   :   0.9163\n",
      "    Recall      :   0.8622\n",
      "    F1          :   0.8884\n",
      "    TP          :      219\n",
      "    FP          :       20\n",
      "    FN          :       35\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8769\n",
      "  Recall      :   0.8302\n",
      "  F1          :   0.8529\n",
      "  TP          :      389\n",
      "  FP          :       53\n",
      "  FN          :       78\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      182\n",
      "  FP          :       85\n",
      "  FN          :       75\n",
      "  Accuracy    :   0.5322\n",
      "  Precision   :   0.6816\n",
      "  Recall      :   0.7082\n",
      "  F1          :   0.6947\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7989\n",
      "  Recall      :   0.7707\n",
      "  F1          :   0.7841\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.80 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      171\n",
      "  FP          :       33\n",
      "  FN          :       50\n",
      "  TN          :      198\n",
      "  Precision   :   0.8382\n",
      "  Recall      :   0.7738\n",
      "  F1          :   0.8047\n",
      "  Accuracy    :   0.8164\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8299\n",
      "    Recall      :   0.7703\n",
      "    F1          :   0.7990\n",
      "    TP          :      161\n",
      "    FP          :       33\n",
      "    FN          :       48\n",
      "  Label: effect\n",
      "    Precision   :   0.9091\n",
      "    Recall      :   0.8333\n",
      "    F1          :   0.8696\n",
      "    TP          :      200\n",
      "    FP          :       20\n",
      "    FN          :       40\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8695\n",
      "  Recall      :   0.8018\n",
      "  F1          :   0.8343\n",
      "  TP          :      361\n",
      "  FP          :       53\n",
      "  FN          :       88\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      159\n",
      "  FP          :       85\n",
      "  FN          :       82\n",
      "  Accuracy    :   0.4877\n",
      "  Precision   :   0.6516\n",
      "  Recall      :   0.6598\n",
      "  F1          :   0.6557\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7865\n",
      "  Recall      :   0.7451\n",
      "  F1          :   0.7649\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.85: 100%|██████████| 15/15 [00:58<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 202 (44.7%)\n",
      "Non-causal samples: 250 (55.3%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1220\n",
      "Total relations created: 324\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.85 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      169\n",
      "  FP          :       33\n",
      "  FN          :       52\n",
      "  TN          :      198\n",
      "  Precision   :   0.8366\n",
      "  Recall      :   0.7647\n",
      "  F1          :   0.7991\n",
      "  Accuracy    :   0.8119\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7118\n",
      "    Recall      :   0.6151\n",
      "    F1          :   0.6599\n",
      "    TP          :      163\n",
      "    FP          :       66\n",
      "    FN          :      102\n",
      "  Label: effect\n",
      "    Precision   :   0.7802\n",
      "    Recall      :   0.7053\n",
      "    F1          :   0.7409\n",
      "    TP          :      213\n",
      "    FP          :       60\n",
      "    FN          :       89\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7460\n",
      "  Recall      :   0.6602\n",
      "  F1          :   0.7004\n",
      "  TP          :      376\n",
      "  FP          :      126\n",
      "  FN          :      191\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      175\n",
      "  FP          :      125\n",
      "  FN          :      130\n",
      "  Accuracy    :   0.4070\n",
      "  Precision   :   0.5833\n",
      "  Recall      :   0.5738\n",
      "  F1          :   0.5785\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7220\n",
      "  Recall      :   0.6662\n",
      "  F1          :   0.6927\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.85 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      169\n",
      "  FP          :       33\n",
      "  FN          :       52\n",
      "  TN          :      198\n",
      "  Precision   :   0.8366\n",
      "  Recall      :   0.7647\n",
      "  F1          :   0.7991\n",
      "  Accuracy    :   0.8119\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7000\n",
      "    Recall      :   0.5900\n",
      "    F1          :   0.6403\n",
      "    TP          :      154\n",
      "    FP          :       66\n",
      "    FN          :      107\n",
      "  Label: effect\n",
      "    Precision   :   0.7647\n",
      "    Recall      :   0.6771\n",
      "    F1          :   0.7182\n",
      "    TP          :      195\n",
      "    FP          :       60\n",
      "    FN          :       93\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7324\n",
      "  Recall      :   0.6336\n",
      "  F1          :   0.6793\n",
      "  TP          :      349\n",
      "  FP          :      126\n",
      "  FN          :      200\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      153\n",
      "  FP          :      125\n",
      "  FN          :      137\n",
      "  Accuracy    :   0.3687\n",
      "  Precision   :   0.5504\n",
      "  Recall      :   0.5276\n",
      "  F1          :   0.5387\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7064\n",
      "  Recall      :   0.6420\n",
      "  F1          :   0.6724\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.85 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      169\n",
      "  FP          :       33\n",
      "  FN          :       52\n",
      "  TN          :      198\n",
      "  Precision   :   0.8366\n",
      "  Recall      :   0.7647\n",
      "  F1          :   0.7991\n",
      "  Accuracy    :   0.8119\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8446\n",
      "    Recall      :   0.7762\n",
      "    F1          :   0.8089\n",
      "    TP          :      163\n",
      "    FP          :       30\n",
      "    FN          :       47\n",
      "  Label: effect\n",
      "    Precision   :   0.9221\n",
      "    Recall      :   0.8486\n",
      "    F1          :   0.8838\n",
      "    TP          :      213\n",
      "    FP          :       18\n",
      "    FN          :       38\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8833\n",
      "  Recall      :   0.8124\n",
      "  F1          :   0.8464\n",
      "  TP          :      376\n",
      "  FP          :       48\n",
      "  FN          :       85\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      175\n",
      "  FP          :       80\n",
      "  FN          :       78\n",
      "  Accuracy    :   0.5255\n",
      "  Precision   :   0.6863\n",
      "  Recall      :   0.6917\n",
      "  F1          :   0.6890\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8021\n",
      "  Recall      :   0.7563\n",
      "  F1          :   0.7781\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.85 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      169\n",
      "  FP          :       33\n",
      "  FN          :       52\n",
      "  TN          :      198\n",
      "  Precision   :   0.8366\n",
      "  Recall      :   0.7647\n",
      "  F1          :   0.7991\n",
      "  Accuracy    :   0.8119\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8370\n",
      "    Recall      :   0.7476\n",
      "    F1          :   0.7897\n",
      "    TP          :      154\n",
      "    FP          :       30\n",
      "    FN          :       52\n",
      "  Label: effect\n",
      "    Precision   :   0.9155\n",
      "    Recall      :   0.8228\n",
      "    F1          :   0.8667\n",
      "    TP          :      195\n",
      "    FP          :       18\n",
      "    FN          :       42\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8762\n",
      "  Recall      :   0.7852\n",
      "  F1          :   0.8282\n",
      "  TP          :      349\n",
      "  FP          :       48\n",
      "  FN          :       94\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      153\n",
      "  FP          :       80\n",
      "  FN          :       85\n",
      "  Accuracy    :   0.4811\n",
      "  Precision   :   0.6567\n",
      "  Recall      :   0.6429\n",
      "  F1          :   0.6497\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7898\n",
      "  Recall      :   0.7309\n",
      "  F1          :   0.7590\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.90: 100%|██████████| 15/15 [01:05<00:00,  4.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 201 (44.5%)\n",
      "Non-causal samples: 251 (55.5%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1203\n",
      "Total relations created: 303\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.90 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      168\n",
      "  FP          :       33\n",
      "  FN          :       53\n",
      "  TN          :      198\n",
      "  Precision   :   0.8358\n",
      "  Recall      :   0.7602\n",
      "  F1          :   0.7962\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7162\n",
      "    Recall      :   0.6069\n",
      "    F1          :   0.6570\n",
      "    TP          :      159\n",
      "    FP          :       63\n",
      "    FN          :      103\n",
      "  Label: effect\n",
      "    Precision   :   0.7824\n",
      "    Recall      :   0.6879\n",
      "    F1          :   0.7321\n",
      "    TP          :      205\n",
      "    FP          :       57\n",
      "    FN          :       93\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7493\n",
      "  Recall      :   0.6474\n",
      "  F1          :   0.6946\n",
      "  TP          :      364\n",
      "  FP          :      120\n",
      "  FN          :      196\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      168\n",
      "  FP          :      111\n",
      "  FN          :      133\n",
      "  Accuracy    :   0.4078\n",
      "  Precision   :   0.6022\n",
      "  Recall      :   0.5581\n",
      "  F1          :   0.5793\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7291\n",
      "  Recall      :   0.6552\n",
      "  F1          :   0.6900\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.90 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      168\n",
      "  FP          :       33\n",
      "  FN          :       53\n",
      "  TN          :      198\n",
      "  Precision   :   0.8358\n",
      "  Recall      :   0.7602\n",
      "  F1          :   0.7962\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7083\n",
      "    Recall      :   0.5862\n",
      "    F1          :   0.6415\n",
      "    TP          :      153\n",
      "    FP          :       63\n",
      "    FN          :      108\n",
      "  Label: effect\n",
      "    Precision   :   0.7702\n",
      "    Recall      :   0.6632\n",
      "    F1          :   0.7127\n",
      "    TP          :      191\n",
      "    FP          :       57\n",
      "    FN          :       97\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7392\n",
      "  Recall      :   0.6247\n",
      "  F1          :   0.6771\n",
      "  TP          :      344\n",
      "  FP          :      120\n",
      "  FN          :      205\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      150\n",
      "  FP          :      111\n",
      "  FN          :      140\n",
      "  Accuracy    :   0.3741\n",
      "  Precision   :   0.5747\n",
      "  Recall      :   0.5172\n",
      "  F1          :   0.5445\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7166\n",
      "  Recall      :   0.6340\n",
      "  F1          :   0.6726\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.90 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      168\n",
      "  FP          :       33\n",
      "  FN          :       53\n",
      "  TN          :      198\n",
      "  Precision   :   0.8358\n",
      "  Recall      :   0.7602\n",
      "  F1          :   0.7962\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8503\n",
      "    Recall      :   0.7718\n",
      "    F1          :   0.8092\n",
      "    TP          :      159\n",
      "    FP          :       28\n",
      "    FN          :       47\n",
      "  Label: effect\n",
      "    Precision   :   0.9276\n",
      "    Recall      :   0.8333\n",
      "    F1          :   0.8779\n",
      "    TP          :      205\n",
      "    FP          :       16\n",
      "    FN          :       41\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8889\n",
      "  Recall      :   0.8026\n",
      "  F1          :   0.8436\n",
      "  TP          :      364\n",
      "  FP          :       44\n",
      "  FN          :       88\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      168\n",
      "  FP          :       68\n",
      "  FN          :       80\n",
      "  Accuracy    :   0.5316\n",
      "  Precision   :   0.7119\n",
      "  Recall      :   0.6774\n",
      "  F1          :   0.6942\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8122\n",
      "  Recall      :   0.7467\n",
      "  F1          :   0.7780\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.90 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      168\n",
      "  FP          :       33\n",
      "  FN          :       53\n",
      "  TN          :      198\n",
      "  Precision   :   0.8358\n",
      "  Recall      :   0.7602\n",
      "  F1          :   0.7962\n",
      "  Accuracy    :   0.8097\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8453\n",
      "    Recall      :   0.7463\n",
      "    F1          :   0.7927\n",
      "    TP          :      153\n",
      "    FP          :       28\n",
      "    FN          :       52\n",
      "  Label: effect\n",
      "    Precision   :   0.9227\n",
      "    Recall      :   0.8093\n",
      "    F1          :   0.8623\n",
      "    TP          :      191\n",
      "    FP          :       16\n",
      "    FN          :       45\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.8840\n",
      "  Recall      :   0.7778\n",
      "  F1          :   0.8275\n",
      "  TP          :      344\n",
      "  FP          :       44\n",
      "  FN          :       97\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      150\n",
      "  FP          :       68\n",
      "  FN          :       87\n",
      "  Accuracy    :   0.4918\n",
      "  Precision   :   0.6881\n",
      "  Recall      :   0.6329\n",
      "  F1          :   0.6593\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8026\n",
      "  Recall      :   0.7236\n",
      "  F1          :   0.7610\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.95: 100%|██████████| 15/15 [01:05<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 452 samples from list input to Doccano format...\n",
      "Processing sample 0/452\n",
      "\n",
      "============================================================\n",
      "CONVERSION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Total samples processed: 452\n",
      "Causal samples: 189 (41.8%)\n",
      "Non-causal samples: 263 (58.2%)\n",
      "Error samples: 0 (0.0%)\n",
      "Total entities created: 1162\n",
      "Total relations created: 262\n",
      "============================================================\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.95 | scenario=all_documents | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                   all_documents — coverage Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      158\n",
      "  FP          :       31\n",
      "  FN          :       63\n",
      "  TN          :      200\n",
      "  Precision   :   0.8360\n",
      "  Recall      :   0.7149\n",
      "  F1          :   0.7707\n",
      "  Accuracy    :   0.7920\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7400\n",
      "    Recall      :   0.5692\n",
      "    F1          :   0.6435\n",
      "    TP          :      148\n",
      "    FP          :       52\n",
      "    FN          :      112\n",
      "  Label: effect\n",
      "    Precision   :   0.7854\n",
      "    Recall      :   0.6141\n",
      "    F1          :   0.6893\n",
      "    TP          :      183\n",
      "    FP          :       50\n",
      "    FN          :      115\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7627\n",
      "  Recall      :   0.5917\n",
      "  F1          :   0.6664\n",
      "  TP          :      331\n",
      "  FP          :      102\n",
      "  FN          :      227\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      151\n",
      "  FP          :       90\n",
      "  FN          :      147\n",
      "  Accuracy    :   0.3892\n",
      "  Precision   :   0.6266\n",
      "  Recall      :   0.5067\n",
      "  F1          :   0.5603\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7417\n",
      "  Recall      :   0.6044\n",
      "  F1          :   0.6658\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.95 | scenario=all_documents | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                  all_documents — discovery Results                   \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      158\n",
      "  FP          :       31\n",
      "  FN          :       63\n",
      "  TN          :      200\n",
      "  Precision   :   0.8360\n",
      "  Recall      :   0.7149\n",
      "  F1          :   0.7707\n",
      "  Accuracy    :   0.7920\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.7347\n",
      "    Recall      :   0.5517\n",
      "    F1          :   0.6302\n",
      "    TP          :      144\n",
      "    FP          :       52\n",
      "    FN          :      117\n",
      "  Label: effect\n",
      "    Precision   :   0.7738\n",
      "    Recall      :   0.5938\n",
      "    F1          :   0.6719\n",
      "    TP          :      171\n",
      "    FP          :       50\n",
      "    FN          :      117\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.7542\n",
      "  Recall      :   0.5727\n",
      "  F1          :   0.6511\n",
      "  TP          :      315\n",
      "  FP          :      102\n",
      "  FN          :      234\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      136\n",
      "  FP          :       90\n",
      "  FN          :      154\n",
      "  Accuracy    :   0.3579\n",
      "  Precision   :   0.6018\n",
      "  Recall      :   0.4690\n",
      "  F1          :   0.5271\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.7307\n",
      "  Recall      :   0.5855\n",
      "  F1          :   0.6496\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.95 | scenario=filtered_causal | eval=coverage ############\n",
      "\n",
      "======================================================================\n",
      "                  filtered_causal — coverage Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      158\n",
      "  FP          :       31\n",
      "  FN          :       63\n",
      "  TN          :      200\n",
      "  Precision   :   0.8360\n",
      "  Recall      :   0.7149\n",
      "  F1          :   0.7707\n",
      "  Accuracy    :   0.7920\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8862\n",
      "    Recall      :   0.7629\n",
      "    F1          :   0.8199\n",
      "    TP          :      148\n",
      "    FP          :       19\n",
      "    FN          :       46\n",
      "  Label: effect\n",
      "    Precision   :   0.9433\n",
      "    Recall      :   0.8026\n",
      "    F1          :   0.8673\n",
      "    TP          :      183\n",
      "    FP          :       11\n",
      "    FN          :       45\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9148\n",
      "  Recall      :   0.7828\n",
      "  F1          :   0.8436\n",
      "  TP          :      331\n",
      "  FP          :       30\n",
      "  FN          :       91\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      151\n",
      "  FP          :       49\n",
      "  FN          :       81\n",
      "  Accuracy    :   0.5374\n",
      "  Precision   :   0.7550\n",
      "  Recall      :   0.6509\n",
      "  F1          :   0.6991\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8352\n",
      "  Recall      :   0.7162\n",
      "  F1          :   0.7711\n",
      "\n",
      "======================================================================\n",
      "\n",
      "\n",
      "############ Model=bert-softmax | class=span_only | thr=0.95 | scenario=filtered_causal | eval=discovery ############\n",
      "\n",
      "======================================================================\n",
      "                 filtered_causal — discovery Results                  \n",
      "======================================================================\n",
      "\n",
      "--- Task1 ---\n",
      "  TP          :      158\n",
      "  FP          :       31\n",
      "  FN          :       63\n",
      "  TN          :      200\n",
      "  Precision   :   0.8360\n",
      "  Recall      :   0.7149\n",
      "  F1          :   0.7707\n",
      "  Accuracy    :   0.7920\n",
      "  N           :      452\n",
      "\n",
      "--- Task2 ---\n",
      "  Label: cause\n",
      "    Precision   :   0.8834\n",
      "    Recall      :   0.7385\n",
      "    F1          :   0.8045\n",
      "    TP          :      144\n",
      "    FP          :       19\n",
      "    FN          :       51\n",
      "  Label: effect\n",
      "    Precision   :   0.9396\n",
      "    Recall      :   0.7844\n",
      "    F1          :   0.8550\n",
      "    TP          :      171\n",
      "    FP          :       11\n",
      "    FN          :       47\n",
      "\n",
      "--- Task2_macro ---\n",
      "  Precision   :   0.9115\n",
      "  Recall      :   0.7614\n",
      "  F1          :   0.8297\n",
      "  TP          :      315\n",
      "  FP          :       30\n",
      "  FN          :       98\n",
      "\n",
      "--- Task3 ---\n",
      "  TP          :      136\n",
      "  FP          :       49\n",
      "  FN          :       88\n",
      "  Accuracy    :   0.4982\n",
      "  Precision   :   0.7351\n",
      "  Recall      :   0.6071\n",
      "  F1          :   0.6650\n",
      "\n",
      "--- Total_Macro ---\n",
      "  Precision   :   0.8275\n",
      "  Recall      :   0.6945\n",
      "  F1          :   0.7552\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bert-softmax | span_only | thr=0.99:  20%|██        | 3/15 [00:16<01:04,  5.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mrun_full_evaluation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_paths\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclassification_modes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_classification_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscenarios\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscenarios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_modes\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_modes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_data_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_predictions_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_report_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwrite_markdown\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\src\\analysis\\evaluation_pipeline.py:201\u001b[39m, in \u001b[36mrun_full_evaluation\u001b[39m\u001b[34m(model_paths, thresholds, classification_modes, scenarios, eval_modes, test_data_path, tokenizer_name, batch_size, device, write_markdown, save_report_dir, save_predictions_dir, seed)\u001b[39m\n\u001b[32m    199\u001b[39m     batch = texts[i * batch_size : (i + \u001b[32m1\u001b[39m) * batch_size]\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         batch_results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrel_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneural_only\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrel_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcause_decision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m     all_results.extend(batch_results)\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Save raw prediction JSON for reproducibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\model.py:306\u001b[39m, in \u001b[36mJointCausalModel.predict\u001b[39m\u001b[34m(self, sents, tokenizer, rel_mode, rel_threshold, cause_decision)\u001b[39m\n\u001b[32m    303\u001b[39m enc = to_dev(enc)  \u001b[38;5;66;03m# Ensure tensors are on the correct device\u001b[39;00m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     base = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m cls_logits_batch = base[\u001b[33m\"\u001b[39m\u001b[33mcls_logits\u001b[39m\u001b[33m\"\u001b[39m]              \u001b[38;5;66;03m# Sentence-level classification logits\u001b[39;00m\n\u001b[32m    309\u001b[39m bio_emissions_batch = base[\u001b[33m\"\u001b[39m\u001b[33mbio_emissions\u001b[39m\u001b[33m\"\u001b[39m]        \u001b[38;5;66;03m# BIO tagging emissions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\model.py:216\u001b[39m, in \u001b[36mJointCausalModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, bio_labels, pair_batch, cause_starts, cause_ends, effect_starts, effect_ends)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs a forward pass through the model.\u001b[39;00m\n\u001b[32m    195\u001b[39m \n\u001b[32m    196\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m \u001b[33;03m          relation extraction inputs provided).\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# Encode input\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Classification head\u001b[39;00m\n\u001b[32m    219\u001b[39m cls_logits = \u001b[38;5;28mself\u001b[39m.cls_head(hidden[:, \u001b[32m0\u001b[39m])  \u001b[38;5;66;03m# Use [CLS] token representation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\model.py:179\u001b[39m, in \u001b[36mJointCausalModel.encode\u001b[39m\u001b[34m(self, input_ids, attention_mask)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n\u001b[32m    169\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Encodes the input using the transformer model.\u001b[39;00m\n\u001b[32m    170\u001b[39m \n\u001b[32m    171\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m \u001b[33;03m        and layer normalization.\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m.last_hidden_state\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer_norm(\u001b[38;5;28mself\u001b[39m.dropout(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1144\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1137\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1139\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1142\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1156\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1157\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    684\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    685\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    686\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    692\u001b[39m         output_attentions,\n\u001b[32m    693\u001b[39m     )\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:627\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    624\u001b[39m     cross_attn_present_key_value = cross_attention_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    625\u001b[39m     present_key_value = present_key_value + cross_attn_present_key_value\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m layer_output = \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    632\u001b[39m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\pytorch_utils.py:253\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:639\u001b[39m, in \u001b[36mBertLayer.feed_forward_chunk\u001b[39m\u001b[34m(self, attention_output)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[32m--> \u001b[39m\u001b[32m639\u001b[39m     intermediate_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    640\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[39m, in \u001b[36mBertIntermediate.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.intermediate_act_fn(hidden_states)\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df = run_full_evaluation(\n",
    "    model_paths = models,\n",
    "    thresholds=thresholds,\n",
    "    classification_modes = causal_classification_modes,\n",
    "    batch_size=batch_size,\n",
    "    scenarios=scenarios,\n",
    "    eval_modes=eval_modes,\n",
    "    test_data_path = test_data_dir,\n",
    "    save_predictions_dir = save_dir,\n",
    "    save_report_dir = report_dir,\n",
    "    write_markdown=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8217c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
