{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645b23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the parent directory to the system path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8ec932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951f6c3ad3fc4acf8c1f2cd1ed08cbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd42d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── User-tunable parameters ────────────────────────────────────────────────────\n",
    "DATA_FILE: str = r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\datasets\\expert_multi_task_data\\test.csv\"            # path to CSV dataset\n",
    "PROMPT_FILE: str = r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\causal_pseudo_labeling\\prompt.txt\"                         # path to prompt template\n",
    "SAMPLE_SIZE: Optional[int] = None   # e.g. 100 -> sample 100 rows, None -> all\n",
    "USE_CHAT_TEMPLATE: bool = True       # toggle chat wrapping on/off\n",
    "BATCH_SIZE: int = 1000               # prompts per forward pass\n",
    "MAX_TOKENS: int = 512                # max new tokens to generate\n",
    "GPU_MEMORY_UTILISATION: float = 0.90 # fraction of GPU RAM vLLM may allocate\n",
    "RANDOM_SEED: int = 8642              # reproducible sampling\n",
    "SAVE_DIR: str = r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\predictions\"       # directory to save results\n",
    "SENTENCE_COLUMN: int = 0          # column index of the sentence to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "facbb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompt_template(path: str) -> str:\n",
    "    \"\"\"Read the prompt template containing the ``{{SENTENCE}}`` placeholder.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "    \n",
    "\n",
    "def load_sentences(path: str, column: int, sample_size: Optional[int]) -> List[str]:\n",
    "    \"\"\"Load sentences from a CSV file (optionally subsample).\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    sentences = df.iloc[:, column].astype(str)\n",
    "\n",
    "    if sample_size is not None:\n",
    "        sentences = sentences.sample(n=sample_size, random_state=RANDOM_SEED)\n",
    "\n",
    "    return sentences.tolist()\n",
    "\n",
    "\n",
    "def build_prompts(template: str, sentences: List[str], use_chat: bool,\n",
    "                  tokenizer: Optional[AutoTokenizer]) -> List[str]:\n",
    "    \"\"\"Return a list of formatted prompts ready for vLLM.\"\"\"\n",
    "    if use_chat and tokenizer is not None:\n",
    "        messages = [[{\"role\": \"user\", \"content\": template.replace(\"{{SENTENCE}}\", s)}]\n",
    "                    for s in sentences]\n",
    "        return [\n",
    "            tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True)\n",
    "            for m in messages\n",
    "        ]\n",
    "    # Fallback: plain string replacement (user supplies full template).\n",
    "    return [template.replace(\"{{SENTENCE}}\", s) for s in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f9ea75",
   "metadata": {},
   "source": [
    "# LLama3 8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da313ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prompt template …\n",
      "Loading sentences …\n",
      "Total sentences to annotate: 452\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading prompt template …\")\n",
    "prompt_template = load_prompt_template(PROMPT_FILE)\n",
    "\n",
    "print(\"Loading sentences …\")\n",
    "sentences = load_sentences(DATA_FILE, SENTENCE_COLUMN, SAMPLE_SIZE)\n",
    "print(f\"Total sentences to annotate: {len(sentences):,}\")\n",
    "\n",
    "# Prepare tokenizer only if we intend to use chat formatting.\n",
    "tokenizer = None\n",
    "if USE_CHAT_TEMPLATE:\n",
    "    print(\"Initialising tokenizer for chat template …\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "\n",
    "prompts = build_prompts(prompt_template, sentences, USE_CHAT_TEMPLATE, tokenizer)\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "print(\"Initialising vLLM … (this may take a moment)\")\n",
    "llm = LLM(\n",
    "        model=MODEL_NAME,\n",
    "        dtype=\"float16\",\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=GPU_MEMORY_UTILISATION,\n",
    "    )\n",
    "\n",
    "results: List[str] = []\n",
    "\n",
    "    # Get the len of the prompts list.\n",
    "\n",
    "num_prompts = len(prompts)\n",
    "print(f\"Total prompts to process: {num_prompts:,}\")\n",
    "print(f\"Batch size: {BATCH_SIZE:,}\")\n",
    "\n",
    "print(\"Running inference …\")\n",
    "\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "results.extend([output.outputs[0].text for output in outputs])\n",
    "\n",
    "print(\"Inference complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10333687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
