{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd85d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from config import DEVICE, SEED, MODEL_CONFIG, TRAINING_CONFIG, DATASET_CONFIG\n",
    "from model import JointCausalModel\n",
    "from utility import compute_class_weights, label_value_counts\n",
    "from dataset_collator import CausalDataset, CausalDatasetCollator\n",
    "from config import id2label_cls, id2label_bio, id2label_rel\n",
    "from evaluate_joint_causal_model import evaluate_model, print_eval_report\n",
    "from trainer import train_model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fec89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"C:\\\\Users\\\\norouzin\\\\Desktop\\\\JointLearning\\\\datasets\\\\expert_multi_task_data\\\\train.csv\"\n",
    "val_data_path = \"C:\\\\Users\\\\norouzin\\\\Desktop\\\\JointLearning\\\\datasets\\\\expert_multi_task_data\\\\val.csv\"\n",
    "test_data_path = \"C:\\\\Users\\\\norouzin\\\\Desktop\\\\JointLearning\\\\datasets\\\\expert_multi_task_data\\\\test.csv\"\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "val_df = pd.read_csv(val_data_path)\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caee3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CausalDataset(\n",
    "    train_df,\n",
    "    tokenizer_name=MODEL_CONFIG[\"encoder_name\"],\n",
    "    max_length=DATASET_CONFIG[\"max_length\"],\n",
    ")\n",
    "# %%\n",
    "val_dataset = CausalDataset(\n",
    "    val_df,\n",
    "    tokenizer_name=MODEL_CONFIG[\"encoder_name\"],\n",
    "    max_length=DATASET_CONFIG[\"max_length\"],\n",
    ")\n",
    "# %%\n",
    "test_dataset = CausalDataset(\n",
    "    test_df,\n",
    "    tokenizer_name=MODEL_CONFIG[\"encoder_name\"],\n",
    "    max_length=DATASET_CONFIG[\"max_length\"],\n",
    ")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dd2fc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_labels_value_counts:\n",
      " 0    1047\n",
      "1    1035\n",
      "Name: count, dtype: int64\n",
      "bio_labels_value_counts:\n",
      "  6      52764\n",
      " 3       8717\n",
      " 1       6948\n",
      "-100     4164\n",
      " 2       1320\n",
      " 0       1179\n",
      " 5        483\n",
      " 4         79\n",
      "Name: count, dtype: int64\n",
      "rel_labels_value_counts:\n",
      " 0    2887\n",
      "1    1494\n",
      "Name: count, dtype: int64\n",
      "CLS Weights: tensor([0.0015, 0.0016])\n",
      "BIO Weights: tensor([0.0014, 0.0010, 0.0014, 0.0010, 0.0132, 0.0026, 0.0010])\n",
      "REL Weights: tensor([0.0011, 0.0013])\n"
     ]
    }
   ],
   "source": [
    "labels_flat = label_value_counts(train_dataset)\n",
    "# %%\n",
    "cls_label_flat = labels_flat[\"cls_labels_flat\"]\n",
    "bio_label_flat = labels_flat[\"bio_labels_flat\"]\n",
    "rel_label_flat = labels_flat[\"rel_labels_flat\"]\n",
    "# %%\n",
    "# Calculate class weights\n",
    "cls_weights = compute_class_weights(labels_list=cls_label_flat, num_classes=MODEL_CONFIG[\"num_cls_labels\"], technique=\"ens\", ignore_index=-100)\n",
    "bio_weights = compute_class_weights(labels_list=bio_label_flat, num_classes=MODEL_CONFIG[\"num_bio_labels\"], technique=\"ens\", ignore_index=-100)\n",
    "rel_weights = compute_class_weights(labels_list=rel_label_flat, num_classes=MODEL_CONFIG[\"num_rel_labels\"], technique=\"ens\", ignore_index=-100)\n",
    "print(f\"CLS Weights: {cls_weights}\")\n",
    "print(f\"BIO Weights: {bio_weights}\")\n",
    "print(f\"REL Weights: {rel_weights}\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98d81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = CausalDatasetCollator(\n",
    "    tokenizer=train_dataset.tokenizer\n",
    ")\n",
    "# %%\n",
    "# take a 100 samples from train_dataset\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, random.sample(range(len(train_dataset)), 20))\n",
    "# val_dataset = torch.utils.data.Subset(val_dataset, random.sample(range(len(val_dataset)), 20))\n",
    "# # %%\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    collate_fn=collator,\n",
    "    shuffle=True\n",
    "\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=TRAINING_CONFIG[\"batch_size\"],\n",
    "    collate_fn=collator,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09250461",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = JointCausalModel(\n",
    "    encoder_name=MODEL_CONFIG[\"encoder_name\"],\n",
    "    num_cls_labels=MODEL_CONFIG[\"num_cls_labels\"],\n",
    "    num_bio_labels=MODEL_CONFIG[\"num_bio_labels\"],\n",
    "    num_rel_labels=MODEL_CONFIG[\"num_rel_labels\"],\n",
    "    dropout=MODEL_CONFIG[\"dropout\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5a8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=TRAINING_CONFIG[\"weight_decay\"]\n",
    ")\n",
    "# %%\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eaed35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = r\"C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\test_results\\expert_bert_softmax_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0df434d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Configuration ---\n",
      "Device: cpu\n",
      "Number of Epochs: 20\n",
      "Seed: 8642\n",
      "Optimizer: AdamW (LR: 5e-05, Weight Decay: 1.0)\n",
      "Scheduler: ReduceLROnPlateau\n",
      "Max Grad Norm: 1.0 (Mode: L2 norm if enabled)\n",
      "Early Stopping Patience: 20\n",
      "Model Save Path: C:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\test_results\\expert_bert_softmax_model.pt\n",
      "Mode: Silver Data Training (GCE)\n",
      "GCE q value: 0.7\n",
      "Using task loss weights: {'cls': 1.0, 'bio': 4.0, 'rel': 1.0}\n",
      "CLS Class Weights: Provided\n",
      "BIO Class Weights: Provided\n",
      "REL Class Weights: Provided\n",
      "----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Training]:   0%|          | 0/131 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trained_model, training_history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid2label_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid2label_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid2label_bio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid2label_bio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mid2label_rel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mid2label_rel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_class_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcls_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbio_class_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbio_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Only for softmax\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_class_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatience_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_epochs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgradient_clip_val\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_fn_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mevaluate_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass your evaluate_model function here\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprint_report_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_eval_report\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Pass your print_eval_report function here\u001b[39;49;00m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_silver_training\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask_loss_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\src\\jointlearning\\trainer.py:234\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_dataloader, val_dataloader, optimizer, num_epochs, device, id2label_cls, id2label_bio, id2label_rel, model_save_path, scheduler, cls_class_weights, bio_class_weights, rel_class_weights, patience_epochs, seed, max_grad_norm, eval_fn_metrics, print_report_fn, is_silver_training, gce_q_value, task_loss_weights)\u001b[39m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m \n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Backward pass: Compute gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[43mtotal_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Gradient Clipping: Prevent exploding gradients\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m max_grad_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\norouzin\\Desktop\\JointLearning\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trained_model, training_history = train_model(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "        device=DEVICE,\n",
    "        id2label_cls=id2label_cls,\n",
    "        id2label_bio=id2label_bio,\n",
    "        id2label_rel=id2label_rel,\n",
    "        model_save_path=model_save_path,\n",
    "        scheduler=scheduler,\n",
    "        cls_class_weights=cls_weights,\n",
    "        bio_class_weights=bio_weights, # Only for softmax\n",
    "        rel_class_weights=rel_weights,\n",
    "        patience_epochs=TRAINING_CONFIG[\"num_epochs\"],\n",
    "        seed=SEED,\n",
    "        max_grad_norm=TRAINING_CONFIG[\"gradient_clip_val\"],\n",
    "        eval_fn_metrics=evaluate_model, # Pass your evaluate_model function here\n",
    "        print_report_fn=print_eval_report, # Pass your print_eval_report function here\n",
    "        is_silver_training=True,\n",
    "        task_loss_weights={\"cls\": 1.0, \"bio\": 4.0, \"rel\": 1.0}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d65306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.save_pretrained(\n",
    "    r\"/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "add8aeca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax/tokenizer_config.json',\n",
       " '/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax/special_tokens_map.json',\n",
       " '/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax/vocab.txt',\n",
       " '/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax/added_tokens.json',\n",
       " '/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax/tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.tokenizer.save_pretrained(\n",
    "    r\"/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/hf_exper_bert_softmax\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35ef916",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Initialize tokenizer and model (model uses default bert-base-uncased here)\n",
    "    # In a real case, load your specific fine-tuned model and tokenizer\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG[\"encoder_name\"])\n",
    "        # Pass the config explicitly if not loading a full pretrained model\n",
    "        model = JointCausalModel(\n",
    "            encoder_name=MODEL_CONFIG[\"encoder_name\"],\n",
    "            num_cls_labels=MODEL_CONFIG[\"num_cls_labels\"],\n",
    "            num_bio_labels=MODEL_CONFIG[\"num_bio_labels\"],\n",
    "            num_rel_labels=MODEL_CONFIG[\"num_rel_labels\"],\n",
    "            dropout=MODEL_CONFIG[\"dropout\"]\n",
    "        )\n",
    "        model.to(device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing model/tokenizer: {e}\")\n",
    "        print(\"Please ensure 'bert-base-uncased' is available or replace with your model path.\")\n",
    "        exit()\n",
    "\n",
    "    for tc_idx, tc in enumerate(test_cases):\n",
    "        print(f\"\\n--- Running Test: {tc['name']} ---\")\n",
    "        print(f\"Text: '{tc['text']}'\")\n",
    "        print(f\"Settings: {tc['settings']}\")\n",
    "\n",
    "        texts_batch = [tc[\"text\"]]\n",
    "        tokenized_inputs = tokenizer(\n",
    "            texts_batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\", # Pad to max_length for consistent bio_token_ids length\n",
    "            max_length=32,      # A small max_length for testing\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        \n",
    "        # Apply the mock for the model's forward pass\n",
    "        # Store original forward and restore it later if needed, or make mock part of model for test mode\n",
    "        original_forward = model.forward\n",
    "        model.forward = get_mock_forward_fn(tc[\"mock_data\"], device=device)\n",
    "\n",
    "        try:\n",
    "            predictions = model.predict_batch(\n",
    "                texts_batch,\n",
    "                tokenized_inputs,\n",
    "                device=device,\n",
    "                **tc[\"settings\"]\n",
    "            )\n",
    "            result = predictions[0] # We are processing one sentence at a time\n",
    "\n",
    "            print(f\"  Predicted Output: {result}\")\n",
    "\n",
    "            # Basic Assertions (more detailed assertions can be added)\n",
    "            assert result[\"causal\"] == tc[\"expected_causal\"], \\\n",
    "                f\"Causal flag mismatch. Expected {tc['expected_causal']}, Got {result['causal']}\"\n",
    "            assert len(result[\"relations\"]) == tc[\"expected_relations_count\"], \\\n",
    "                f\"Relations count mismatch. Expected {tc['expected_relations_count']}, Got {len(result['relations'])}\"\n",
    "\n",
    "            if \"expected_relations_texts\" in tc:\n",
    "                extracted_rel_texts = sorted([(r[\"cause\"], r[\"effect\"]) for r in result[\"relations\"]])\n",
    "                expected_rel_texts = sorted(tc[\"expected_relations_texts\"])\n",
    "                assert extracted_rel_texts == expected_rel_texts, \\\n",
    "                    f\"Relation texts mismatch. Expected {expected_rel_texts}, Got {extracted_rel_texts}\"\n",
    "\n",
    "            print(f\"  Test PASSED!\")\n",
    "\n",
    "        except AssertionError as e:\n",
    "            print(f\"  Test FAILED: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Test ERRORED: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            model.forward = original_forward # Restore original forward method"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
