{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c8e7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rnorouzini/JointLearning/myenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from config import DEVICE, SEED, MODEL_CONFIG, TRAINING_CONFIG, DATASET_CONFIG\n",
    "from model import JointCausalModel\n",
    "from utility import compute_class_weights, label_value_counts\n",
    "from dataset_collator import CausalDataset, CausalDatasetCollator\n",
    "from config import id2label_cls, id2label_bio, id2label_rel\n",
    "from evaluate_joint_causal_model import evaluate_model, print_eval_report\n",
    "from trainer import train_model\n",
    "import random\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787267da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JointCausalModel(\n",
       "  (enc): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (cls_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       "  (bio_head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=384, out_features=7, bias=True)\n",
       "  )\n",
       "  (rel_head): Sequential(\n",
       "    (0): Linear(in_features=1536, out_features=768, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=768, out_features=384, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.2, inplace=False)\n",
       "    (6): Linear(in_features=384, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"/home/rnorouzini/JointLearning/src/jointlearning/expert_bert_softmax/expert_bert_softmax_model.pt\"\n",
    "model = JointCausalModel(**MODEL_CONFIG)\n",
    "model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97de1315",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CONFIG[\"encoder_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f262302",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"name\": \"Scenario 1: Simple Non-Causal\",\n",
    "        \"text\": \"The sky is blue.\",\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 0, # non-causal\n",
    "            \"bio_token_ids\": [6, 6, 6, 6, 6] # O O O O O (assuming 5 tokens after CLS/SEP)\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": False, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": False,\n",
    "        \"expected_relations_count\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 2: Simple Causal (C -> E) - Heuristic\",\n",
    "        \"text\": \"Heavy rain caused the flood.\", # Tokens: Heavy, rain, caused, the, flood, .\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 1, # causal\n",
    "            \"bio_token_ids\": [0, 1, 6, 6, 2, 6] # B-C, I-C, O, O, B-E, O\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": True, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": True,\n",
    "        \"expected_relations_count\": 1,\n",
    "        \"expected_relations_texts\": [(\"Heavy rain\", \"flood\")]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 3: Single CE Span - Heuristic (CE Self-Pair)\",\n",
    "        \"text\": \"The drought was the problem.\", # Tokens: The, drought, was, the, problem, .\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 1, # causal\n",
    "            \"bio_token_ids\": [4, 5, 6, 6, 6, 6] # B-CE, I-CE, O, O, O, O\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": True, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": True,\n",
    "        \"expected_relations_count\": 1,\n",
    "        \"expected_relations_texts\": [(\"The drought\", \"The drought\")]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 4: Single CE Span - Standard (No CE Self-Pair)\",\n",
    "        \"text\": \"The drought was the problem.\",\n",
    "        \"mock_data\": { # Same mock as Scenario 3\n",
    "            \"cls_id\": 1, \"bio_token_ids\": [4, 5, 6, 6, 6, 6]\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": False, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": False, # Becomes false because no relations are formed\n",
    "        \"expected_relations_count\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 5: CLS Override - Non-Causal to Causal\",\n",
    "        \"text\": \"Stress leads to burnout.\", # Tokens: Stress, leads, to, burnout, .\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 0, # Initially non-causal\n",
    "            \"bio_token_ids\": [0, 6, 6, 2, 6] # B-C, O, O, B-E, O\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": False, \"override_cls_if_spans_found\": True},\n",
    "        \"expected_causal\": True, # Should be overridden to True\n",
    "        \"expected_relations_count\": 1, # Assumes mock rel_head predicts this\n",
    "        \"expected_relations_texts\": [(\"Stress\", \"burnout\")]\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 6: CLS Override - Still Non-Causal (No Spans)\",\n",
    "        \"text\": \"A quiet day.\",\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 0, # non-causal\n",
    "            \"bio_token_ids\": [6, 6, 6] # O O O\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": False, \"override_cls_if_spans_found\": True},\n",
    "        \"expected_causal\": False,\n",
    "        \"expected_relations_count\": 0\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Scenario 7: Multiple Causes, One Effect - Heuristic\",\n",
    "        \"text\": \"Heat and lack of water caused crops to fail.\",\n",
    "        # Tokens: Heat, and, lack, of, water, caused, crops, to, fail, .\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 1, # causal\n",
    "            \"bio_token_ids\": [0, 6, 0, 6, 1, 6, 2, 6, 3, 6] # B-C(Heat), O, B-C(lack), O, I-C(water), O, B-E(crops), O, I-E(fail), O\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": True, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": True,\n",
    "        \"expected_relations_count\": 2, # (Heat, crops to fail), (lack of water, crops to fail)\n",
    "        \"expected_relations_texts\": [(\"Heat\", \"crops to fail\"), (\"lack of water\", \"crops to fail\")]\n",
    "    },\n",
    "     {\n",
    "        \"name\": \"Scenario 8: Invalid I-tag correction\",\n",
    "        \"text\": \"Bad food made sick.\", # Bad, food, made, sick, .\n",
    "        \"mock_data\": {\n",
    "            \"cls_id\": 1, # causal\n",
    "             # O, I-C (invalid), O, B-E, O\n",
    "            \"bio_token_ids\": [6, 1, 6, 2, 6]\n",
    "        },\n",
    "        \"settings\": {\"use_heuristic\": True, \"override_cls_if_spans_found\": False},\n",
    "        \"expected_causal\": True,\n",
    "        \"expected_relations_count\": 1,\n",
    "        \"expected_relations_texts\": [(\"food\", \"sick\")] # \"food\" becomes B-C after correction\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15d3394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from typing import Dict, Tuple, Optional, Any, List\n",
    "\n",
    "# --- Configuration (Simplified from config.py for this test script) ---\n",
    "id2label_bio = {\n",
    "    0: \"B-C\", 1: \"I-C\", 2: \"B-E\", 3: \"I-E\",\n",
    "    4: \"B-CE\", 5: \"I-CE\", 6: \"O\"\n",
    "}\n",
    "label2id_bio = {v: k for k, v in id2label_bio.items()}\n",
    "id2label_rel = {0: \"Rel_None\", 1: \"Rel_CE\"}\n",
    "label2id_rel = {v: k for k, v in id2label_rel.items()}\n",
    "id2label_cls = {0: \"non-causal\", 1: \"causal\"}\n",
    "label2id_cls = {v: k for k, v in id2label_cls.items()}\n",
    "\n",
    "MODEL_CONFIG = {\n",
    "    \"encoder_name\": \"bert-base-uncased\",\n",
    "    \"num_cls_labels\": 2,\n",
    "    \"num_bio_labels\": 7,\n",
    "    \"num_rel_labels\": 2,\n",
    "    \"dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6ec7ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mock_forward_fn(tokenized_inputs_for_mock: Dict[str, torch.Tensor], \n",
    "                        test_case_mock_data: Dict[str, Any], \n",
    "                        tokenizer_for_mock: AutoTokenizer,\n",
    "                        device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Returns a function that simulates the model's forward pass.\n",
    "    - tokenized_inputs_for_mock: Actual output from tokenizer for the current text (batch size 1).\n",
    "    - test_case_mock_data[\"bio_token_ids_for_words\"]: List of BIO IDs for *actual word tokens only*,\n",
    "      EXCLUDING CLS, SEP, and PAD.\n",
    "    \"\"\"\n",
    "    def mock_forward(input_ids_batch, attention_mask_batch, **kwargs):\n",
    "        batch_size, seq_len = input_ids_batch.shape\n",
    "        cls_id = test_case_mock_data[\"cls_id\"]\n",
    "        mock_cls_logits = torch.full((batch_size, MODEL_CONFIG[\"num_cls_labels\"]), -10.0, device=device)\n",
    "        mock_cls_logits[0, cls_id] = 10.0\n",
    "\n",
    "        mock_bio_emissions = torch.full((batch_size, seq_len, MODEL_CONFIG[\"num_bio_labels\"]), -10.0, device=device)\n",
    "        mock_bio_emissions[:, :, label2id_bio[\"O\"]] = 5.0 \n",
    "\n",
    "        word_bio_ids = test_case_mock_data[\"bio_token_ids_for_words\"]\n",
    "        current_word_bio_idx = 0\n",
    "        for token_pos in range(seq_len):\n",
    "            if attention_mask_batch[0, token_pos] == 0: continue\n",
    "            current_token_id_val = input_ids_batch[0, token_pos].item()\n",
    "            if current_token_id_val == tokenizer_for_mock.cls_token_id or \\\n",
    "               current_token_id_val == tokenizer_for_mock.sep_token_id:\n",
    "                continue\n",
    "            if current_word_bio_idx < len(word_bio_ids):\n",
    "                bio_id_for_this_token = word_bio_ids[current_word_bio_idx]\n",
    "                mock_bio_emissions[0, token_pos, bio_id_for_this_token] = 10.0\n",
    "                current_word_bio_idx += 1\n",
    "        \n",
    "        mock_hidden_states = torch.randn(batch_size, seq_len, 768, device=device)\n",
    "        return {\"cls_logits\": mock_cls_logits, \"bio_emissions\": mock_bio_emissions, \"hidden_states\": mock_hidden_states}\n",
    "    return mock_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d43d895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Test: Scenario 1: Simple Non-Causal ---\n",
      "Text: 'The sky is blue.'\n",
      "Settings: {'use_heuristic': False, 'override_cls_if_spans_found': False}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "get_mock_forward_fn() missing 2 required positional arguments: 'test_case_mock_data' and 'tokenizer_for_mock'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Apply the mock for the model's forward pass\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Store original forward and restore it later if needed, or make mock part of model for test mode\u001b[39;00m\n\u001b[1;32m     17\u001b[0m original_forward \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward\n\u001b[0;32m---> 18\u001b[0m model\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[43mget_mock_forward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmock_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_batch(\n\u001b[1;32m     22\u001b[0m         texts_batch,\n\u001b[1;32m     23\u001b[0m         tokenized_inputs,\n\u001b[1;32m     24\u001b[0m         device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msettings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: get_mock_forward_fn() missing 2 required positional arguments: 'test_case_mock_data' and 'tokenizer_for_mock'"
     ]
    }
   ],
   "source": [
    "for tc_idx, tc in enumerate(test_cases):\n",
    "        print(f\"\\n--- Running Test: {tc['name']} ---\")\n",
    "        print(f\"Text: '{tc['text']}'\")\n",
    "        print(f\"Settings: {tc['settings']}\")\n",
    "\n",
    "        texts_batch = [tc[\"text\"]]\n",
    "        tokenized_inputs = tokenizer(\n",
    "            texts_batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\", # Pad to max_length for consistent bio_token_ids length\n",
    "            max_length=32,      # A small max_length for testing\n",
    "            truncation=True,\n",
    "            return_offsets_mapping=True\n",
    "        )\n",
    "        # Apply the mock for the model's forward pass\n",
    "        # Store original forward and restore it later if needed, or make mock part of model for test mode\n",
    "        original_forward = model.forward\n",
    "        model.forward = get_mock_forward_fn(tc[\"mock_data\"], device=DEVICE)\n",
    "\n",
    "        try:\n",
    "            predictions = model.predict_batch(\n",
    "                texts_batch,\n",
    "                tokenized_inputs,\n",
    "                device=DEVICE,\n",
    "                **tc[\"settings\"]\n",
    "            )\n",
    "            result = predictions[0] # We are processing one sentence at a time\n",
    "\n",
    "            print(f\"  Predicted Output: {result}\")\n",
    "\n",
    "            # Basic Assertions (more detailed assertions can be added)\n",
    "            assert result[\"causal\"] == tc[\"expected_causal\"], \\\n",
    "                f\"Causal flag mismatch. Expected {tc['expected_causal']}, Got {result['causal']}\"\n",
    "            assert len(result[\"relations\"]) == tc[\"expected_relations_count\"], \\\n",
    "                f\"Relations count mismatch. Expected {tc['expected_relations_count']}, Got {len(result['relations'])}\"\n",
    "\n",
    "            if \"expected_relations_texts\" in tc:\n",
    "                extracted_rel_texts = sorted([(r[\"cause\"], r[\"effect\"]) for r in result[\"relations\"]])\n",
    "                expected_rel_texts = sorted(tc[\"expected_relations_texts\"])\n",
    "                assert extracted_rel_texts == expected_rel_texts, \\\n",
    "                    f\"Relation texts mismatch. Expected {expected_rel_texts}, Got {extracted_rel_texts}\"\n",
    "\n",
    "            print(f\"  Test PASSED!\")\n",
    "\n",
    "        except AssertionError as e:\n",
    "            print(f\"  Test FAILED: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Test ERRORED: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            model.forward = original_forward # Restore original forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42916138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
